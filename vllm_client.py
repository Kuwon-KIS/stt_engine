#!/usr/bin/env python3
"""
vLLM ì—°ë™ ëª¨ë“ˆ
STT ê²°ê³¼ë¥¼ vLLM ì„œë²„ë¡œ ì „ì†¡í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
from pathlib import Path
from typing import Optional, Dict
import requests
from pydantic import BaseModel
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class VLLMConfig(BaseModel):
    """vLLM ì„œë²„ ì„¤ì •"""
    api_url: str = os.getenv("VLLM_API_URL", "http://localhost:8000")
    model_name: str = os.getenv("VLLM_MODEL_NAME", "meta-llama/Llama-2-7b-hf")
    timeout: int = 60
    max_tokens: int = 512


class VLLMClient:
    """vLLM ì„œë²„ì™€ì˜ í†µì‹ ì„ ë‹´ë‹¹í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: VLLMConfig):
        """
        vLLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: vLLM ì„¤ì •
        """
        self.config = config
        self.completion_endpoint = f"{config.api_url}/v1/completions"
        
        print(f"ğŸ”— vLLM ì„œë²„ ì—°ê²° ì„¤ì •")
        print(f"   API URL: {config.api_url}")
        print(f"   ëª¨ë¸: {config.model_name}")
    
    def health_check(self) -> bool:
        """
        vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        
        Returns:
            ì„œë²„ ì •ìƒ ì—¬ë¶€
        """
        try:
            response = requests.get(
                f"{self.config.api_url}/health",
                timeout=5
            )
            is_healthy = response.status_code == 200
            status = "âœ… ì •ìƒ" if is_healthy else "âŒ ì˜¤ë¥˜"
            print(f"vLLM ì„œë²„ ìƒíƒœ: {status}")
            return is_healthy
        except Exception as e:
            print(f"âŒ vLLM ì„œë²„ ì—°ê²° ë¶ˆê°€: {e}")
            return False
    
    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> Optional[str]:
        """
        vLLM ì„œë²„ì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            temperature: ì˜¨ë„ ê°’ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )
            top_p: Top-p ìƒ˜í”Œë§ ê°’
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            payload = {
                "model": self.config.model_name,
                "prompt": prompt,
                "max_tokens": self.config.max_tokens,
                "temperature": temperature,
                "top_p": top_p
            }
            
            print(f"\nğŸ“¤ ìš”ì²­ ì „ì†¡ ì¤‘...")
            response = requests.post(
                self.completion_endpoint,
                json=payload,
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result["choices"][0]["text"]
                print(f"âœ… ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                return generated_text
            else:
                print(f"âŒ ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                print(f"   ì‘ë‹µ: {response.text}")
                return None
        
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return None
    
    def process_stt_with_vllm(
        self,
        transcribed_text: str,
        instruction: str = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:"
    ) -> Dict:
        """
        STT ê²°ê³¼ë¥¼ vLLMìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            transcribed_text: STT ê²°ê³¼ í…ìŠ¤íŠ¸
            instruction: ì²˜ë¦¬ ì§€ì‹œì‚¬í•­
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        prompt = f"{instruction}\n\n{transcribed_text}"
        
        result = self.generate(prompt)
        
        if result:
            return {
                "success": True,
                "original_text": transcribed_text,
                "processed_text": result,
                "instruction": instruction
            }
        else:
            return {
                "success": False,
                "original_text": transcribed_text,
                "error": "vLLM ì²˜ë¦¬ ì‹¤íŒ¨"
            }


def test_vllm_connection():
    """vLLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    config = VLLMConfig()
    client = VLLMClient(config)
    
    # ì„œë²„ ìƒíƒœ í™•ì¸
    if not client.health_check():
        print("âš ï¸  vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
        print("ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”: vllm serve <model_name>")
        return
    
    # ê°„ë‹¨í•œ ìš”ì²­ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ìš”ì²­ ì „ì†¡...")
    test_prompt = "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” STT ì—”ì§„ì…ë‹ˆë‹¤."
    result = client.generate(test_prompt)
    
    if result:
        print(f"\nğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸:")
        print(result)
    else:
        print("âŒ ìš”ì²­ ì‹¤íŒ¨")


if __name__ == "__main__":
    test_vllm_connection()
