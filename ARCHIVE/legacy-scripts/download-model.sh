#!/bin/bash
# Docker í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ“¥ Whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --no-cache-dir \
    transformers==4.37.2 \
    torch==2.1.2 \
    huggingface-hub==0.21.4

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python << 'PYTHON_SCRIPT'
import os
from pathlib import Path
from huggingface_hub import snapshot_download
from transformers import AutoProcessor

model_id = "openai/whisper-large-v3-turbo"
cache_dir = Path("/app/models")

print(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model_id}")
print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {cache_dir}")

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
model_path = snapshot_download(
    repo_id=model_id,
    cache_dir=str(cache_dir),
    resume_download=True,
    local_dir=str(cache_dir / model_id.replace("/", "_"))
)

# Processor ì €ì¥
processor = AutoProcessor.from_pretrained(model_id)
processor.save_pretrained(model_path)

print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {model_path}")
PYTHON_SCRIPT

echo "âœ¨ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
