#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (ì˜µì…˜ í¬í•¨)

ëª©ì :
  1. ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
  2. Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  3. CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±) - ì˜µì…˜
  4. ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz) - ì˜µì…˜
  5. ì„œë²„ ì „ì†¡ ì¤€ë¹„

ì‚¬ìš©:
  conda activate stt-py311
  python download_model_hf.py [ì˜µì…˜]

ì˜µì…˜:
  --no-convert        CTranslate2 ë³€í™˜ ìŠ¤í‚µ (PyTorch ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ)
  --skip-compress     ëª¨ë¸ íŒŒì¼ ì••ì¶• ìŠ¤í‚µ
  --skip-test         ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  --help              ë„ì›€ë§ ì¶œë ¥

ì˜ˆì‹œ:
  python download_model_hf.py                 # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’, ê¶Œì¥)
  python download_model_hf.py --no-convert    # CTranslate2 ë³€í™˜ ìŠ¤í‚µ
  python download_model_hf.py --skip-test     # í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  python download_model_hf.py --skip-compress # ì••ì¶• ìŠ¤í‚µ

âš ï¸  íŒ¨í‚¤ì§€ ë²„ì „ í˜¸í™˜ì„±:
  ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ë²„ì „ê³¼ í˜¸í™˜ì„±ì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤:
  - faster-whisper==1.2.1 (ì—”ì§„ê³¼ ë™ì¼, requirements.txt ì°¸ê³ )
  - ctranslate2==4.7.1 (ì—”ì§„ê³¼ ë™ì¼, requirements.txt ì°¸ê³ )
  - transformers>=4.30,<6 (ì—”ì§„ê³¼ ë™ì¼)
  
  ğŸ“ ì£¼ì˜: build-server-models.shì—ì„œ ìœ„ ë²„ì „ë“¤ì´ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤
  ë§Œì•½ ë‹¤ë¥¸ ë²„ì „ì„ ì‚¬ìš©í•˜ë©´ ì—”ì§„ ë¡œë”© ì‹œ í˜¸í™˜ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import ssl
import shutil
import subprocess
import tarfile
import argparse
import json
from pathlib import Path
from datetime import datetime

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬/ë°©í™”ë²½ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

# urllib3 ê²½ê³  ë¹„í™œì„±í™”
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

# ============================================================================
# ëª…ë ¹ì¤„ ì¸ìˆ˜ ì²˜ë¦¬
# ============================================================================

parser = argparse.ArgumentParser(
    description='STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog="""
ì˜ˆì‹œ:
  python download_model_hf.py                 # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’, ê¶Œì¥) - ì„œë²„ ì§ì ‘ ì‹¤í–‰
  python download_model_hf.py --local         # ë¡œì»¬ í™˜ê²½ (ë§¥ë¶) - conda ì‚¬ìš©
  python download_model_hf.py --no-convert    # CTranslate2 ë³€í™˜ ìŠ¤í‚µ
  python download_model_hf.py --skip-test     # ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  python download_model_hf.py --skip-compress # ì••ì¶• ìŠ¤í‚µ
  python download_model_hf.py --local --skip-compress # ë¡œì»¬ + ì••ì¶• ìŠ¤í‚µ
    """
)
parser.add_argument('--no-convert', action='store_true', 
                    help='CTranslate2 ë³€í™˜ ìŠ¤í‚µ (PyTorch ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ)')
parser.add_argument('--skip-compress', action='store_true',
                    help='ëª¨ë¸ íŒŒì¼ ì••ì¶• ìŠ¤í‚µ')
parser.add_argument('--skip-test', action='store_true',
                    help='ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ')
parser.add_argument('--local', action='store_true',
                    help='ë¡œì»¬ í™˜ê²½ (ë§¥ë¶)ì—ì„œ conda í™˜ê²½ìœ¼ë¡œ ì‹¤í–‰ (ê¸°ë³¸ê°’: ì„œë²„ ì§ì ‘ ì‹¤í–‰)')

args = parser.parse_args()

# ì˜µì…˜ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’)
should_convert = not args.no_convert
should_compress = not args.skip_compress
should_test = not args.skip_test
use_conda = args.local  # ë¡œì»¬ í™˜ê²½(--local)ì¼ ë•Œë§Œ conda ì‚¬ìš©

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def print_header(msg):
    print("\n" + "=" * 60)
    print(msg)
    print("=" * 60 + "\n")

def print_step(msg):
    print(f"\nğŸ“Œ {msg}")

def print_success(msg):
    print(f"âœ… {msg}")

def print_error(msg):
    print(f"âŒ {msg}")
    sys.exit(1)

def print_warn(msg):
    print(f"âš ï¸  {msg}")

def check_and_install_faster_whisper():
    """faster-whisper ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸, ì—†ìœ¼ë©´ ì„¤ì¹˜"""
    try:
        import faster_whisper
        return True
    except ImportError:
        print("âš ï¸  faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("ì„¤ì¹˜ ì¤‘...")
        
        try:
            import subprocess
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "-q", "faster-whisper"
            ])
            print("âœ… faster-whisper ì„¤ì¹˜ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ faster-whisper ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            print("ìˆ˜ë™ ì„¤ì¹˜: pip install faster-whisper")
            return False

# ============================================================================
# ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘
# ============================================================================

print_header("ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„ (ë‹¤ìš´ë¡œë“œ + ì˜µì…˜ ë³€í™˜ + ì••ì¶•)")

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
model_specific_dir = models_dir / "openai_whisper-large-v3-turbo"

print(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {BASE_DIR}")
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print(f"ğŸ“ ëª¨ë¸ íŠ¹ì • ê²½ë¡œ: {model_specific_dir}")
print()

# ============================================================================
# Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# ============================================================================

print_step("Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬")

if model_specific_dir.exists():
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {model_specific_dir}")
    print("ì‚­ì œ ì¤‘...")
    shutil.rmtree(model_specific_dir)
    print_success("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
else:
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ (ì‹ ê·œ ì„¤ì¹˜)")

models_dir.mkdir(parents=True, exist_ok=True)
model_specific_dir.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“ ì‹ ê·œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: {model_specific_dir}")

# ============================================================================
# Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
# ============================================================================

print_step("Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")

try:
    from huggingface_hub import snapshot_download
    
    MODEL_REPO = "openai/whisper-large-v3-turbo"
    MAX_RETRIES = 3
    RETRY_DELAY = 10  # seconds
    
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_REPO}")
    print(f"â³ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.5GB)...")
    print(f"   (ìµœëŒ€ {MAX_RETRIES}íšŒ ì¬ì‹œë„)")
    print()
    
    # ë‹¤ìš´ë¡œë“œ ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    model_path = None
    last_error = None
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            print(f"â³ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt}/{MAX_RETRIES}...")
            
            # snapshot_download ìµœì‹  API (deprecated ì¸ì ì œê±°)
            # - resume_download: deprecated (í•­ìƒ resume)
            # - local_dir_use_symlinks: deprecated (symlink ë¯¸ì‚¬ìš©)
            # - timeout: ì§€ì›í•˜ì§€ ì•ŠìŒ
            model_path = snapshot_download(
                repo_id=MODEL_REPO,
                cache_dir=None,
                local_dir=str(model_specific_dir)
            )
            
            print_success(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ì‹œë„ {attempt})")
            break
            
        except Exception as e:
            last_error = e
            error_msg = str(e)
            
            if attempt < MAX_RETRIES:
                print(f"âš ï¸  ì‹œë„ {attempt} ì‹¤íŒ¨: {error_msg[:100]}")
                print(f"   {RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                print()
                
                import time
                time.sleep(RETRY_DELAY)
            else:
                print(f"âŒ ì‹œë„ {attempt} ì‹¤íŒ¨")
    
    if model_path is None:
        print_error(f"ë‹¤ìš´ë¡œë“œ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {last_error}")
    
except ImportError:
    print_error("huggingface-hubì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install huggingface-hub")
except Exception as e:
    print_error(f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
# ============================================================================

print_step("Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦")

print(f"\nğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:")
if not any(model_specific_dir.iterdir()):
    print_error("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

REQUIRED_FILES = [
    "config.json",
    "model.safetensors",
    "generation_config.json",
    "preprocessor_config.json",
    "tokenizer.json",
]

all_found = True
total_size = 0

for req_file in REQUIRED_FILES:
    file_path = model_specific_dir / req_file
    if file_path.exists():
        size = file_path.stat().st_size / (1024**2)
        total_size += size
        print(f"  âœ“ {req_file} ({size:.2f}MB)")
    else:
        print(f"  âœ— {req_file} (MISSING)")
        all_found = False

if not all_found:
    print_error(f"ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”.\në‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\n  rm -rf {model_specific_dir}\n  python download_model_hf.py")

print(f"\nğŸ“ ì „ì²´ í¬ê¸°: {total_size:.2f}MB")

# ìµœì†Œ í¬ê¸° ê²€ì¦ (Whisper LargeëŠ” ì•½ 1.5GB ì´ìƒì´ì–´ì•¼ í•¨)
MIN_TOTAL_SIZE_MB = 1400  # ì•½ 1.4GB
if total_size < MIN_TOTAL_SIZE_MB:
    print_error(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {total_size:.2f}MB (ìµœì†Œ: {MIN_TOTAL_SIZE_MB}MB)\në‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n  rm -rf {model_specific_dir}\n  python download_model_hf.py")

# ê° íŒŒì¼ì˜ ì²´í¬ì„¬ ê²€ì¦ (ê°„ë‹¨í•œ ë¬´ê²°ì„± ê²€ì‚¬)
print()
print("âœ… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")

import json

for file_path in [model_specific_dir / f for f in REQUIRED_FILES]:
    if file_path.exists():
        try:
            # íŒŒì¼ì„ ì½ì–´ì„œ ê¸°ë³¸ì ì¸ ì†ìƒ ì—¬ë¶€ í™•ì¸
            if file_path.suffix in ['.json']:
                # JSON íŒŒì¼ì€ íŒŒì‹± ê°€ëŠ¥í•œì§€ í™•ì¸
                with open(file_path, 'r') as f:
                    data = json.load(f)
                file_size = file_path.stat().st_size
                if file_size < 100:  # 100ë°”ì´íŠ¸ ë¯¸ë§Œì´ë©´ ì†ìƒëœ ê²ƒìœ¼ë¡œ ì˜ì‹¬
                    print(f"   âš ï¸  {file_path.name} (íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ: {file_size} bytes - ì†ìƒ ê°€ëŠ¥ì„±)")
                    if file_path.name == "config.json":
                        print_error(f"âŒ PyTorch config.jsonì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n  rm -rf {model_specific_dir}\n  python download_model_hf.py")
                else:
                    print(f"   âœ“ {file_path.name} (JSON ê²€ì¦ OK, {file_size} bytes)")
            else:
                # ë‹¤ë¥¸ íŒŒì¼ì€ í¬ê¸°ë§Œ í™•ì¸
                size = file_path.stat().st_size
                if size == 0:
                    print(f"   âœ— {file_path.name} (í¬ê¸° 0ë°”ì´íŠ¸ - ì†ìƒë¨)")
                    raise ValueError(f"{file_path.name} is empty")
                print(f"   âœ“ {file_path.name} (ê²€ì¦ OK)")
        except Exception as e:
            print_error(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {file_path.name}\nì˜¤ë¥˜: {e}\në‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n  rm -rf {model_specific_dir}\n  python download_model_hf.py")

print_success("íŒŒì¼ ê²€ì¦ ì™„ë£Œ")

# ============================================================================
# Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±) - ì¡°ê±´ë¶€
# ============================================================================

# validation_passed ì´ˆê¸°í™” (ì´í›„ ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ì—…ë°ì´íŠ¸)
validation_passed = False

print_step("Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)")

if not should_convert:
    print("â­ï¸  CTranslate2 ë³€í™˜ ìŠ¤í‚µ (--no-convert ì˜µì…˜ ì‚¬ìš©)")
    print()
    print("âš ï¸  ì£¼ì˜: CTranslate2 ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤:")
    print("   â€¢ faster-whisper ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€")
    print("   â€¢ transformers ë˜ëŠ” OpenAI Whisper ë°±ì—”ë“œë¡œ í´ë°±")
    print()
    conversion_success = False
else:
    print("â³ PyTorch ëª¨ë¸ì„ CTranslate2 ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    print("   (ì´ ë‹¨ê³„ëŠ” 5-15ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()
    
    # ë³€í™˜ ì „ PyTorch ëª¨ë¸ ìƒíƒœ í™•ì¸
    print("ğŸ“ ë³€í™˜ ì „ PyTorch ëª¨ë¸ ìƒíƒœ:")
    pytorch_dir = model_specific_dir
    pytorch_files = list(pytorch_dir.glob("*.bin")) + list(pytorch_dir.glob("*.json"))
    for f in sorted(pytorch_files):
        size = f.stat().st_size / (1024**2) if f.stat().st_size > 1024*1024 else f.stat().st_size / 1024
        unit = "MB" if f.stat().st_size > 1024*1024 else "KB"
        print(f"   âœ“ {f.name} ({size:.2f}{unit})")
    print()
    
    # CTranslate2 í™˜ê²½ ì •ë³´ ì¶œë ¥
    print("ğŸ”§ CTranslate2 í™˜ê²½ ì •ë³´:")
    try:
        import ctranslate2
        print(f"   âœ“ ctranslate2 ë²„ì „: {ctranslate2.__version__}")
    except Exception as e:
        print(f"   âŒ ctranslate2 import ì‹¤íŒ¨: {e}")
    
    # ct2-transformers-converter ëª…ë ¹ì–´ ì¡´ì¬ í™•ì¸
    try:
        result = subprocess.run(["which", "ct2-transformers-converter"], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            converter_path = result.stdout.strip()
            print(f"   âœ“ ct2-transformers-converter ìœ„ì¹˜: {converter_path}")
        else:
            print(f"   âš ï¸  ct2-transformers-converterë¥¼ PATHì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print(f"      Python APIë¡œ ëŒ€ì‹  ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤")
    except Exception as e:
        print(f"   âš ï¸  ëª…ë ¹ì–´ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    print()

    output_dir = model_specific_dir / "ctranslate2_model"
    conversion_success = False
    MAX_CONVERSION_RETRIES = 2
    
    # CTranslate2 CLI ë„êµ¬ë¡œ ë³€í™˜ (ì¬ì‹œë„ ë¡œì§)
    for conv_attempt in range(1, MAX_CONVERSION_RETRIES + 1):
        try:
            print(f"â³ ct2-transformers-converter CLI ë„êµ¬ë¡œ ë³€í™˜ ì¤‘ (ì‹œë„ {conv_attempt}/{MAX_CONVERSION_RETRIES})...")
            print("   ëª¨ë¸: openai/whisper-large-v3-turbo")
            print(f"   ì¶œë ¥: {output_dir}")
            print()
            
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ëª…ë ¹ì–´ êµ¬ì„± (ë¡œì»¬ í™˜ê²½ì´ë©´ conda ì‚¬ìš©, ì„œë²„ëŠ” ì§ì ‘ ì‹¤í–‰)
            if use_conda:
                # ë¡œì»¬ í™˜ê²½: condaë¡œ ì‹¤í–‰
                cmd = [
                    "conda", "run", "-n", "stt-py311",
                    "ct2-transformers-converter",
                    "--model", "openai/whisper-large-v3-turbo",
                    "--output_dir", str(output_dir),
                    "--force",
                    "--quantization", "int8"
                ]
            else:
                # ì„œë²„ í™˜ê²½: ì§ì ‘ ì‹¤í–‰
                cmd = [
                    "ct2-transformers-converter",
                    "--model", "openai/whisper-large-v3-turbo",
                    "--output_dir", str(output_dir),
                    "--force",
                    "--quantization", "int8"
                ]
            
            print("   [ì‹¤í–‰ ì¤‘...]")
            result = subprocess.run(cmd, capture_output=False, text=True, timeout=900)
            
            print()
            print("=" * 80)
            print(f"ë³€í™˜ ëª…ë ¹ ì‹¤í–‰ ê²°ê³¼: ë°˜í™˜ì½”ë“œ {result.returncode}")
            print("=" * 80)
            print()
            
            if result.returncode == 0:
                # ë³€í™˜ í›„ ìƒì„±ëœ íŒŒì¼ í™•ì¸
                print("ğŸ“ ë³€í™˜ í›„ ìƒì„±ëœ íŒŒì¼ í™•ì¸ ì¤‘...")
                if output_dir.exists():
                    created_files = list(output_dir.rglob('*'))
                    print(f"   ìƒì„±ëœ í•­ëª© ìˆ˜: {len([f for f in created_files if f.is_file()])}")
                    for f in sorted(created_files):
                        if f.is_file():
                            size = f.stat().st_size
                            if size > 1024*1024:
                                print(f"     {f.relative_to(output_dir)}: {size/(1024*1024):.2f}MB")
                            else:
                                print(f"     {f.relative_to(output_dir)}: {size} bytes")
                
                # faster-whisperê°€ tokenizer/preprocessorë¥¼ ë¡œë“œí•  ë•Œ í•„ìš”í•œ íŒŒì¼ë“¤ì„ ëª¨ë‘ ë³µì‚¬
                # ë¶€ëª¨ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì„¤ì •/í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ctranslate2_model/ë¡œ ë³µì‚¬
                if output_dir.exists():
                    parent_dir = model_specific_dir
                    files_to_copy = [
                        "merges.txt",
                        "vocab.json",
                        "special_tokens_map.json",
                        "normalizer.json",
                        "tokenizer.json",
                        "tokenizer_config.json",
                        "preprocessor_config.json",
                        "added_tokens.json",
                        "generation_config.json"
                    ]
                    
                    copied_files = []
                    for filename in files_to_copy:
                        src = parent_dir / filename
                        dst = output_dir / filename
                        
                        if src.exists() and not dst.exists():
                            try:
                                shutil.copy2(str(src), str(dst))
                                copied_files.append(filename)
                            except Exception as copy_err:
                                print(f"âš ï¸  {filename} ë³µì‚¬ ì‹¤íŒ¨: {copy_err}")
                    
                    if copied_files:
                        print(f"ğŸ“‹ ë³µì‚¬ëœ ì„¤ì •/í† í¬ë‚˜ì´ì € íŒŒì¼: {', '.join(copied_files)}")
                
                print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
                conversion_success = True
                break
            else:
                print(f"âš ï¸  CLI ë„êµ¬ ì‹¤íŒ¨ (ë°˜í™˜ì½”ë“œ: {result.returncode})")
                print(f"âš ï¸  ì‹œë„ {conv_attempt}/{MAX_CONVERSION_RETRIES}")
                if conv_attempt < MAX_CONVERSION_RETRIES:
                    print(f"   5ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    import time
                    time.sleep(5)
                else:
                    print("   Python APIë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                print()
            
        except subprocess.TimeoutExpired:
            print(f"âš ï¸  ë³€í™˜ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {conv_attempt})")
            if conv_attempt < MAX_CONVERSION_RETRIES:
                print(f"   Python APIë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            print()
        except Exception as e:
            print(f"âš ï¸  CLI ë„êµ¬ ì˜¤ë¥˜: {e}")
            if conv_attempt < MAX_CONVERSION_RETRIES:
                print("   ì¬ì‹œë„í•©ë‹ˆë‹¤...")
            print()

    # íŒŒì´ì¬ API ì‚¬ìš© (HF ëª¨ë¸ ID) - CLI ì‹¤íŒ¨ ì‹œ ë˜ëŠ” ì¬ì‹œë„
    if not conversion_success:
        for py_attempt in range(1, MAX_CONVERSION_RETRIES + 1):
            try:
                from ctranslate2.converters.transformers import TransformersConverter
                
                print(f"â³ Python API(TransformersConverter)ë¡œ ë³€í™˜ ì¤‘ (ì‹œë„ {py_attempt}/{MAX_CONVERSION_RETRIES})...")
                print("   ëª¨ë¸: openai/whisper-large-v3-turbo (Hugging Face)")
                print("   [ë³€í™˜ ì§„í–‰ ì¤‘...]")
                print()
                
                converter = TransformersConverter("openai/whisper-large-v3-turbo")
                
                converter.convert(
                    output_dir=str(output_dir),
                    force=True,
                )
                
                # ë³€í™˜ í›„ ìƒì„±ëœ íŒŒì¼ í™•ì¸
                print()
                print("ğŸ“ ë³€í™˜ í›„ ìƒì„±ëœ íŒŒì¼ í™•ì¸ ì¤‘...")
                if output_dir.exists():
                    created_files = list(output_dir.rglob('*'))
                    print(f"   ìƒì„±ëœ í•­ëª© ìˆ˜: {len([f for f in created_files if f.is_file()])}")
                    for f in sorted(created_files):
                        if f.is_file():
                            size = f.stat().st_size
                            if size > 1024*1024:
                                print(f"     {f.relative_to(output_dir)}: {size/(1024*1024):.2f}MB")
                            else:
                                print(f"     {f.relative_to(output_dir)}: {size} bytes")
                
                print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
                
                # faster-whisperê°€ tokenizer/preprocessorë¥¼ ë¡œë“œí•  ë•Œ í•„ìš”í•œ íŒŒì¼ë“¤ì„ ëª¨ë‘ ë³µì‚¬
                # ë¶€ëª¨ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì„¤ì •/í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ctranslate2_model/ë¡œ ë³µì‚¬
                if output_dir.exists():
                    parent_dir = model_specific_dir
                    files_to_copy = [
                        "merges.txt",
                        "vocab.json",
                        "special_tokens_map.json",
                        "normalizer.json",
                        "tokenizer.json",
                        "tokenizer_config.json",
                        "preprocessor_config.json",
                        "added_tokens.json",
                        "generation_config.json"
                    ]
                    
                    copied_files = []
                    for filename in files_to_copy:
                        src = parent_dir / filename
                        dst = output_dir / filename
                        
                        if src.exists() and not dst.exists():
                            try:
                                shutil.copy2(str(src), str(dst))
                                copied_files.append(filename)
                            except Exception as copy_err:
                                print(f"âš ï¸  {filename} ë³µì‚¬ ì‹¤íŒ¨: {copy_err}")
                    
                    if copied_files:
                        print(f"ğŸ“‹ ë³µì‚¬ëœ ì„¤ì •/í† í¬ë‚˜ì´ì € íŒŒì¼: {', '.join(copied_files)}")
                
                conversion_success = True
                break
                
            except Exception as e:
                error_msg = str(e)
                if len(error_msg) > 300:
                    error_msg = error_msg[:300] + "..."
                print(f"âš ï¸  ë³€í™˜ ì‹¤íŒ¨ (ì‹œë„ {py_attempt}): {error_msg}")
                
                if py_attempt < MAX_CONVERSION_RETRIES:
                    print(f"   5ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    import time
                    time.sleep(5)
                    print()
                else:
                    print()
    
    # ë³€í™˜ ê²°ê³¼ í™•ì¸

    # ë³€í™˜ ê²°ê³¼ í™•ì¸
    print()
    print("="*80)
    print("ë³€í™˜ ê²°ê³¼ ë¶„ì„")
    print("="*80)
    print()
    
    if conversion_success and output_dir.exists():
        print(f"âœ… CTranslate2 ë³€í™˜ ì„±ê³µ - ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        print()
        
        # ëª¨ë“  íŒŒì¼ ë‚˜ì—´
        print("ğŸ“ ë³€í™˜ëœ ëª¨ë“  íŒŒì¼:")
        all_files = []
        for f in sorted(output_dir.rglob('*')):
            if f.is_file():
                all_files.append(f)
                size = f.stat().st_size
                if size > 1024*1024:
                    print(f"   âœ“ {f.relative_to(output_dir)} ({size/(1024*1024):.2f}MB)")
                elif size > 1024:
                    print(f"   âœ“ {f.relative_to(output_dir)} ({size/1024:.1f}KB)")
                else:
                    print(f"   âœ“ {f.relative_to(output_dir)} ({size} bytes)")
        
        if not all_files:
            print("   âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ë³€í™˜ì´ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        print()
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = {
            'model.bin': 'ë°”ì´ë„ˆë¦¬ ëª¨ë¸ íŒŒì¼',
            'config.json': 'ëª¨ë¸ ì„¤ì •',
            'vocabulary.json': 'ì–´íœ˜ ì‚¬ì „'
        }
        
        print("âœ… í•„ìˆ˜ íŒŒì¼ ê²€ì¦:")
        for filename, desc in required_files.items():
            filepath = output_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                if size > 1024*1024:
                    print(f"   âœ“ {filename} ({desc}) - {size/(1024*1024):.2f}MB")
                elif size > 1024:
                    print(f"   âœ“ {filename} ({desc}) - {size/1024:.1f}KB")
                else:
                    print(f"   âœ“ {filename} ({desc}) - {size} bytes")
            else:
                print(f"   âŒ {filename} ({desc}) - ëˆ„ë½ë¨!")
        
        print()
        
        # ê° íŒŒì¼ì˜ ìƒì„¸ ê²€ì¦
        print("ğŸ“‹ íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦:")
        
        # config.json ê²€ì¦ ë° num_mel_bins ì¶”ê°€
        config_json = output_dir / "config.json"
        if config_json.exists():
            try:
                with open(config_json, 'r') as f:
                    config_data = json.load(f)
                config_size = config_json.stat().st_size
                print(f"   âœ“ config.json ìœ íš¨ ({config_size} bytes)")
                
                # PyTorch ëª¨ë¸ì˜ config.jsonì—ì„œ num_mel_bins ì¶”ì¶œí•˜ì—¬ CTranslate2 configì— ì¶”ê°€
                pytorch_config_json = model_specific_dir / "config.json"
                if pytorch_config_json.exists():
                    try:
                        with open(pytorch_config_json, 'r') as f:
                            pytorch_config = json.load(f)
                        
                        if 'num_mel_bins' in pytorch_config:
                            num_mel_bins = pytorch_config['num_mel_bins']
                            
                            # CTranslate2 config.jsonì— num_mel_bins ì¶”ê°€
                            if 'num_mel_bins' not in config_data:
                                config_data['num_mel_bins'] = num_mel_bins
                                
                                with open(config_json, 'w') as f:
                                    json.dump(config_data, f, indent=2)
                                
                                print(f"   âœ“ num_mel_bins ì¶”ê°€ë¨: {num_mel_bins}")
                            else:
                                print(f"   â„¹ï¸  num_mel_bins ì´ë¯¸ ì¡´ì¬: {config_data['num_mel_bins']}")
                        else:
                            print(f"   âš ï¸  PyTorch config.jsonì— num_mel_binsê°€ ì—†ìŠµë‹ˆë‹¤")
                    except Exception as e:
                        print(f"   âš ï¸  PyTorch config ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        
            except Exception as e:
                print(f"   âŒ config.json ì˜¤ë¥˜: {e}")
        else:
            print(f"   âŒ config.json ëˆ„ë½!")
        
        # vocabulary.json ê²€ì¦
        vocab_json = output_dir / "vocabulary.json"
        if vocab_json.exists():
            try:
                with open(vocab_json, 'r') as f:
                    vocab_data = json.load(f)
                vocab_size = vocab_json.stat().st_size
                # â˜… ìˆ˜ì •: dictì™€ list ëª¨ë‘ ì§€ì›
                vocab_count = len(vocab_data) if isinstance(vocab_data, (dict, list)) else 0
                
                # â˜… í•µì‹¬: vocabulary.jsonì´ ë¹„ì–´ìˆìœ¼ë©´ ì•ˆ ë¨!
                if vocab_count == 0:
                    print(f"   âŒ vocabulary.jsonì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ({vocab_size} bytes, 0 tokens)")
                    print()
                    print("   [vocabulary.json íŒŒì¼ ë‚´ìš©]")
                    with open(vocab_json, 'r') as f:
                        content = f.read()[:500]  # ì²˜ìŒ 500ì
                        print(f"   {content}")
                    print()
                    print_error(f"âŒ CTranslate2 ë³€í™˜ ì‹¤íŒ¨: vocabulary.jsonì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                    print()
                    print("   ì›ì¸ ë¶„ì„:")
                    print("   1. CTranslate2 ë³€í™˜ì´ ë¶ˆì™„ì „í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                    print("   2. Whisper ëª¨ë¸ì˜ íŠ¹ì • ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ")
                    print("   3. í† í¬ë‚˜ì´ì € ì²˜ë¦¬ ì˜¤ë¥˜")
                    print()
                    print("   í•´ê²°ì±…:")
                    print(f"   1. CTranslate2 ë²„ì „ í™•ì¸: conda list | grep ctranslate2")
                    print(f"   2. ëª¨ë¸ ì¬ìƒì„±: rm -rf {output_dir} && python download_model_hf.py")
                    print()
                    sys.exit(1)
                else:
                    print(f"   âœ“ vocabulary.json ìœ íš¨ ({vocab_size} bytes, {vocab_count} tokens)")
            except Exception as e:
                print(f"   âŒ vocabulary.json ì˜¤ë¥˜: {e}")
        else:
            print(f"   âŒ vocabulary.json ëˆ„ë½!")
        
        # model.bin ê²€ì¦
        model_bin = output_dir / "model.bin"
        if model_bin.exists():
            try:
                size = model_bin.stat().st_size
                print(f"   âœ“ model.bin ìœ íš¨ ({size/(1024*1024):.2f}MB)")
            except Exception as e:
                print(f"   âŒ model.bin ì˜¤ë¥˜: {e}")
        else:
            print(f"   âŒ model.bin ëˆ„ë½!")
    
    elif not conversion_success:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨!")
        if output_dir.exists():
            existing = list(output_dir.rglob('*'))
            print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬ì— {len([f for f in existing if f.is_file()])}ê°œ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤ (ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ)")
        print()
        print("   ì¡°ì¹˜ ë°©ë²•:")
        print(f"   1. ë””ë ‰í† ë¦¬ ì‚­ì œ: rm -rf {output_dir}")
        print("   2. ë‹¤ì‹œ ì‹¤í–‰: python download_model_hf.py")
        sys.exit(1)
    
    # model.bin ì¤€ë¹„ (ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ë˜ëŠ” ì¹´í”¼)
    # ì¤‘ìš”: ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ Docker(/app/models)ì™€ ìš´ì˜ ì„œë²„(/data/models) ëª¨ë‘ í˜¸í™˜
    print()
    print("â³ model.bin íŒŒì¼ ì¤€ë¹„ ì¤‘...")
    
    model_bin_link = model_specific_dir / "model.bin"
    model_bin_created = False
    
    bin_files = list(output_dir.glob("*.bin"))
    
    if bin_files:
        model_bin_src = bin_files[0]
        
        # ê¸°ì¡´ íŒŒì¼ ì •ë¦¬
        if model_bin_link.exists() or model_bin_link.is_symlink():
            try:
                model_bin_link.unlink()
            except Exception as e:
                print(f"âš ï¸  ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ìƒì„± ì‹œë„
        try:
            # ìƒëŒ€ ê²½ë¡œ: ctranslate2_model ë””ë ‰í† ë¦¬ ì•ˆì˜ bin íŒŒì¼ì„ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ ì°¸ì¡°
            relative_path = model_bin_src.relative_to(model_specific_dir)
            model_bin_link.symlink_to(relative_path)
            print_success("âœ… model.bin ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ìƒì„± ì™„ë£Œ")
            print(f"   ì†ŒìŠ¤: {relative_path}")
            print(f"   ëŒ€ìƒ: model.bin")
            print(f"   (Docker: /app/models â†’ ìš´ì˜: /data/modelsì—ì„œë„ ì‘ë™)")
            model_bin_created = True
        except Exception as e:
            # ì‹¬ë§í¬ ì‹¤íŒ¨ ì‹œ íŒŒì¼ ë³µì‚¬ (Windows/ê¶Œí•œ ë¬¸ì œ í•´ê²°)
            print(f"âš ï¸  ì‹¬ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            print("   íŒŒì¼ ë³µì‚¬ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            try:
                import shutil
                shutil.copy2(model_bin_src, model_bin_link)
                print_success("âœ… model.bin íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")
                print(f"   ì†ŒìŠ¤: {model_bin_src.name}")
                print(f"   ëŒ€ìƒ: model.bin")
                model_bin_created = True
            except Exception as copy_e:
                print_error(f"âŒ model.bin íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {copy_e}\në‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n  rm -rf {model_specific_dir}\n  python download_model_hf.py")
    else:
        print_error("âŒ ë³€í™˜ëœ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # model.bin ìƒì„± í™•ì¸
    if not model_bin_created:
        print_error(f"âŒ model.bin íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    if not model_bin_link.exists():
        print_error(f"âŒ model.bin íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {model_bin_link}")

# ============================================================================
# Step 6: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (faster-whisper) - ì¡°ê±´ë¶€
# ============================================================================

print_step("Step 6: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")

if not should_test:
    print("â­ï¸  ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ (--skip-test ì˜µì…˜ ì‚¬ìš©)")
    print()
else:
    print("â³ faster-whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ì¤‘...")
    print("   (ì´ ë‹¨ê³„ëŠ” 1-2ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()
    
    # vocabulary.json í˜•ì‹ ê²€ì¦ ë° ë³€í™˜
    ct2_model_dir = model_specific_dir / "ctranslate2_model"
    vocab_json = ct2_model_dir / "vocabulary.json"
    
    if vocab_json.exists():
        print(f"ğŸ“‹ vocabulary.json í˜•ì‹ ê²€ì¦ ì¤‘...")
        try:
            with open(vocab_json, 'r') as f:
                vocab_data = json.load(f)
            
            # í˜•ì‹ ì²´í¬: dict â†’ list ë³€í™˜ì´ í•„ìš”í–ˆëŠ”ì§€ í™•ì¸
            if isinstance(vocab_data, dict):
                print(f"   âš ï¸  vocabulary.jsonì´ dict í˜•ì‹ì…ë‹ˆë‹¤. ë°°ì—´ë¡œ ë³µì›í•©ë‹ˆë‹¤...")
                # dict â†’ list ë³€í™˜: keyë¥¼ indexë¡œ ì •ë ¬í•´ì„œ ë°°ì—´ë¡œ
                vocab_list = [None] * len(vocab_data)
                for token, idx in vocab_data.items():
                    vocab_list[idx] = token
                
                # íŒŒì¼ ë®ì–´ì“°ê¸°
                with open(vocab_json, 'w') as f:
                    json.dump(vocab_list, f, ensure_ascii=False)
                
                print(f"   âœ“ ë³µì› ì™„ë£Œ: {len(vocab_list)} tokens (ë°°ì—´ í˜•ì‹)")
                vocab_data = vocab_list
            elif isinstance(vocab_data, list):
                print(f"   âœ“ vocabulary.jsonì´ ë°°ì—´ í˜•ì‹ì…ë‹ˆë‹¤ (OK)")
            else:
                print_error(f"âŒ vocabulary.jsonì˜ í˜•ì‹ì´ ì˜ˆìƒí•˜ì§€ ëª»í•œ í˜•ì‹ì…ë‹ˆë‹¤: {type(vocab_data)}")
                sys.exit(1)
        except Exception as e:
            print_error(f"âŒ vocabulary.json í˜•ì‹ ê²€ì¦ ì‹¤íŒ¨: {e}")
            sys.exit(1)
        print()
        
        # Hugging Face tokenizer íŒŒì¼ì„ ctranslate2_model/ë¡œ ë³µì‚¬ (faster-whisperê°€ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
        print(f"ğŸ“‹ Hugging Face í† í¬ë‚˜ì´ì € íŒŒì¼ ë³µì‚¬ ì¤‘...")
        tokenizer_files = [
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "normalizer.json"
        ]
        
        for tokenizer_file in tokenizer_files:
            src_file = model_specific_dir / tokenizer_file
            dst_file = ct2_model_dir / tokenizer_file
            
            if src_file.exists() and not dst_file.exists():
                try:
                    shutil.copy2(src_file, dst_file)
                    print(f"   âœ“ {tokenizer_file} ë³µì‚¬ë¨")
                except Exception as e:
                    print(f"   âš ï¸  {tokenizer_file} ë³µì‚¬ ì‹¤íŒ¨: {e}")
        print()
    
    try:
        from faster_whisper import WhisperModel
        
        # CTranslate2 ëª¨ë¸ì´ ìˆëŠ” ì„œë¸Œë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
        # faster-whisperê°€ tokenizer íŒŒì¼ì„ ì´ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ìŒ
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
        print(f"ğŸ” ë¡œë“œ ì¤‘...")
        
        model = WhisperModel(str(ct2_model_dir), device="cpu")
        
        print_success("âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        print(f"   âœ“ Model: WhisperModel (CTranslate2)")
        print(f"   âœ“ Device: CPU")
        print(f"   âœ“ vocabulary.json: 51,866 tokens ë¡œë“œë¨")
        print()
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        print("ğŸ“‹ ëª¨ë¸ ì •ë³´:")
        print(f"   âœ“ Model language: multilingual")
        print(f"   âœ“ Model size: large-v3-turbo")
        print()
        
        # âœ… ê²€ì¦ ì„±ê³µ í‘œì‹œ
        validation_passed = True
        
    except Exception as e:
        error_msg = str(e)
        print_error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
        print()
        print("   ì›ì¸ ë¶„ì„:")
        print("   1. CTranslate2 ëª¨ë¸ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("   2. vocabulary.json ë˜ëŠ” model.bin íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("   3. faster-whisperì™€ ctranslate2 ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ")
        print()
        print("   í•´ê²°ì±…:")
        print(f"   1. ëª¨ë¸ ì¬ìƒì„±: rm -rf {model_specific_dir}")
        print("   2. ë‹¤ì‹œ ì‹¤í–‰: python download_model_hf.py")
        sys.exit(1)

# ============================================================================
# Step 7: ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz) - ì¡°ê±´ë¶€
# ============================================================================

print_step("Step 7: ëª¨ë¸ íŒŒì¼ ì••ì¶•")

compressed_path = None

if not should_compress:
    print("â­ï¸  ëª¨ë¸ ì••ì¶• ìŠ¤í‚µ (--skip-compress ì˜µì…˜ ì‚¬ìš©)")
    print()
else:
    print("â³ ëª¨ë¸ íŒŒì¼ì„ tar.gzë¡œ ì••ì¶• ì¤‘...")
    print("   (ì´ ë‹¨ê³„ëŠ” ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    compressed_filename = f"whisper-large-v3-turbo_models_{timestamp}.tar.gz"
    compressed_path = BASE_DIR / "build" / "output" / compressed_filename

    compressed_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        print("ğŸ“¦ tar.gz ìƒì„± ì¤‘...")
        with tarfile.open(compressed_path, "w:gz") as tar:
            tar.add(model_specific_dir, arcname="models/openai_whisper-large-v3-turbo")
        
        print_success("ëª¨ë¸ ì••ì¶• ì™„ë£Œ")
        
        comp_size = compressed_path.stat().st_size / (1024**3)
        print(f"ğŸ“¦ ì••ì¶• íŒŒì¼: {compressed_path}")
        print(f"ğŸ“ í¬ê¸°: {comp_size:.2f}GB")
        
        original_size = sum(f.stat().st_size for f in model_specific_dir.rglob("*") if f.is_file()) / (1024**3)
        compression_ratio = (1 - comp_size / original_size) * 100
        print(f"ğŸ“Š ì••ì¶•ë¥ : {compression_ratio:.1f}%")
        print()
        
        # ì••ì¶• íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
        print("âœ… ì••ì¶• íŒŒì¼ ê²€ì¦ ì¤‘...")
        try:
            with tarfile.open(compressed_path, "r:gz") as tar:
                members = tar.getmembers()
                print(f"   âœ“ ì••ì¶• íŒŒì¼ ê²€ì¦ ì„±ê³µ ({len(members)} members)")
        except Exception as e:
            print_error(f"ì••ì¶• íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
        
    except Exception as e:
        print_error(f"ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 8: MD5 ì²´í¬ì„¬ ìƒì„± - ì••ì¶•ì´ ì„±ê³µí•œ ê²½ìš°ë§Œ
# ============================================================================

print_step("Step 8: ë¬´ê²°ì„± ê²€ì¦ íŒŒì¼ ìƒì„±")

if compressed_path is None:
    print("â­ï¸  MD5 ì²´í¬ì„¬ ìƒì„± ìŠ¤í‚µ (ì••ì¶•ì´ ìŠ¤í‚µë¨)")
    print()
else:
    import hashlib

    def calculate_md5(file_path):
        md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                md5.update(chunk)
        return md5.hexdigest()

    md5_value = calculate_md5(compressed_path)
    md5_path = compressed_path.parent / f"{compressed_path.name}.md5"

    with open(md5_path, 'w') as f:
        f.write(f"{md5_value}  {compressed_path.name}\n")

    print_success(f"MD5 ì²´í¬ì„¬: {md5_value}")
    print(f"íŒŒì¼: {md5_path}")
    print()

# ============================================================================
# Step 9: ìµœì¢… ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„
# ============================================================================

print_header("ğŸ“‹ ìµœì¢… ìš”ì•½")

print("âœ… ì™„ë£Œëœ ë‹¨ê³„:")
print("  1. âœ“ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
print("  2. âœ“ íŒŒì¼ ê²€ì¦")
if should_convert:
    print("  3. âœ“ CTranslate2 í¬ë§· ë³€í™˜")
else:
    print("  3. â­ï¸  CTranslate2 í¬ë§· ë³€í™˜ (ìŠ¤í‚µ)")
    
if validation_passed:
    print("  4. âœ“ ëª¨ë¸ ê²€ì¦")
else:
    print("  4. âš ï¸  ëª¨ë¸ ê²€ì¦ (ì‹¤íŒ¨ ë˜ëŠ” ìŠ¤í‚µ)")
    
if compressed_path is not None:
    print("  5. âœ“ ëª¨ë¸ ì••ì¶•")
else:
    print("  5. â­ï¸  ëª¨ë¸ ì••ì¶• (ìŠ¤í‚µ)")

print()
print("ğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
print(f"  â€¢ ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")

if compressed_path is not None:
    print(f"  â€¢ ì••ì¶• íŒŒì¼: {compressed_path}")
    print(f"  â€¢ MD5 ì²´í¬ì„: {compressed_path.parent / f'{compressed_path.name}.md5'}")

print()
print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")

if not validation_passed:
    print("  âš ï¸  ëª¨ë¸ ê²€ì¦ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    print()
    print("  1. ëª¨ë¸ íŒŒì¼ í™•ì¸:")
    print(f"     ls -lh {model_specific_dir}/")
    print()
    print("  2. ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ:")
    print("     rm -rf models/openai_whisper-large-v3-turbo build/output/*")
    print("     python download_model_hf.py --skip-compress")
    print()
else:
    print("  âœ… ëª¨ë¸ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print()
    if compressed_path is not None:
        print("  1. ì••ì¶• íŒŒì¼ ë°°í¬:")
        print(f"     scp {compressed_path} user@server:/path/to/deployment/")
        print()
        print("  2. ì„œë²„ì—ì„œ ì••ì¶• í•´ì œ:")
        print(f"     tar -xzf {compressed_path.name}")
        print()
    
    print("  3. Docker ì‹¤í–‰:")
    print("     docker run -it -p 8003:8003 \\")
    print("       -v $(pwd)/models:/app/models \\")
    print("       -v $(pwd)/logs:/app/logs \\")
    print("       stt-engine:cuda129-rhel89-v1.4")
    print()
    
    print("  4. API í…ŒìŠ¤íŠ¸:")
    print("     curl -X POST http://localhost:8003/health")
    print()

print("âœ¨ ì¤€ë¹„ ì™„ë£Œ!")
print()

# ============================================================================
# Step 7: ìµœì¢… ìš”ì•½
# ============================================================================

print_header("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

print("ğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
print(f"  1. ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")
print()

# ëª¨ë¸ ê²€ì¦: ì••ì¶• ì „ì— ë°˜ë“œì‹œ ê²€ì¦ í•„ìˆ˜
print_step("ëª¨ë¸ ê²€ì¦ - ì••ì¶• ì „ ë¬´ê²°ì„± í™•ì¸")

validation_passed = True

if not should_test:
    print("â­ï¸  ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
    print("âš ï¸  ì••ì¶• ì „ì— ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤:")
    print("   python download_model_hf.py --skip-compress")
    print()
    validation_passed = False
else:
    print("âœ… ëª¨ë¸ ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    validation_passed = True

print()

# ============================================================================
# Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz) - ê²€ì¦ í›„ì—ë§Œ ìˆ˜í–‰
# ============================================================================

print_step("Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶•")

compressed_path = None

if not validation_passed:
    print("âš ï¸  ê²€ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì••ì¶•ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
    print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ê²€ì¦ì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("   python download_model_hf.py")
    print()
    print_warn("ì••ì¶• íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print()
elif not should_compress:
    print("â­ï¸  ëª¨ë¸ ì••ì¶• ìŠ¤í‚µ (--skip-compress ì˜µì…˜ ì‚¬ìš©)")
    print()
    print()
    print("=" * 60)
    print("ğŸ” ëª¨ë¸ ê²€ì¦ (faster-whisper ë¡œë“œ í…ŒìŠ¤íŠ¸)")
    print("=" * 60)
    print()

    if not check_and_install_faster_whisper():
        print_error("faster-whisper ì„¤ì¹˜ í•„ìˆ˜")

    try:
        from faster_whisper import WhisperModel
        import numpy as np
        
        ct2_model_dir = model_specific_dir / "ctranslate2_model"
        model_bin_path = ct2_model_dir / "model.bin"
        
        print("ğŸ“ ëª¨ë¸ êµ¬ì¡° í™•ì¸:")
        print(f"   ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")
        print(f"   CTranslate2 ê²½ë¡œ: {ct2_model_dir}")
        print(f"   model.bin ìœ„ì¹˜: {model_bin_path}")
        print()
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬/íŒŒì¼ í™•ì¸
        if not model_specific_dir.exists():
            print_error(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {model_specific_dir}\në‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”:\n  python download_model_hf.py")
        
        if not ct2_model_dir.exists():
            print_error(f"âŒ CTranslate2 ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {ct2_model_dir}\nCTranslate2 ë³€í™˜ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”:\n  python download_model_hf.py")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„¸ í™•ì¸
        print(f"   ğŸ“‚ {model_specific_dir.name}/ ë‚´ìš©:")
        for item in sorted(model_specific_dir.iterdir()):
            if item.is_file():
                size_mb = item.stat().st_size / (1024**2)
                print(f"      - {item.name} ({size_mb:.2f}MB)")
            elif item.is_dir():
                file_count = len(list(item.glob("*")))
                print(f"      ğŸ“ {item.name}/ ({file_count} items)")
                if item.name == "ctranslate2_model":
                    for sub in sorted(item.glob("*"))[:5]:
                        if sub.is_file():
                            size_mb = sub.stat().st_size / (1024**2)
                            print(f"         - {sub.name} ({size_mb:.2f}MB)")
        print()
        
        # model.bin í™•ì¸ (ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ)
        model_bin_parent = model_specific_dir / "model.bin"
        if not model_bin_path.exists() and not model_bin_parent.exists():
            print_error(f"âŒ model.bin íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\nCTranslate2 ë³€í™˜ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”:\n  python download_model_hf.py")
            print()
            
            # ëŒ€ì²´ ê²½ë¡œ í™•ì¸
            alt_bins = list(ct2_model_dir.glob("*.bin")) if ct2_model_dir.exists() else []
            if alt_bins:
                print(f"âš ï¸  {len(alt_bins)}ê°œì˜ .bin íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
                for alt_bin in alt_bins:
                    size_mb = alt_bin.stat().st_size / (1024**2)
                    print(f"   - {alt_bin.name} ({size_mb:.2f}MB)")
                print()
                print("ğŸ’¡ first-whisperëŠ” model.binì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.")
                print("   model.bin ì‹¬ë§í¬/ë³µì‚¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                
                # ìë™ìœ¼ë¡œ model.bin ìƒì„±
                try:
                    first_bin = sorted(alt_bins)[0]
                    shutil.copy2(first_bin, model_bin_path)
                    print(f"âœ… model.bin ìƒì„± ì™„ë£Œ: {first_bin.name} â†’ model.bin")
                except Exception as copy_e:
                    print(f"âŒ model.bin ìƒì„± ì‹¤íŒ¨: {copy_e}")
                    raise
            else:
                print_warn("ë³€í™˜ì„ ìŠ¤í‚µí–ˆê±°ë‚˜ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                print()
                print("ğŸ’¡ ì˜µì…˜ ì—†ì´ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ë³€í™˜í•˜ì„¸ìš”:")
                print("   python download_model_hf.py")
                print()
                raise RuntimeError("CTranslate2 ëª¨ë¸ ë³€í™˜ í•„ìš”")
        else:
            print_success("âœ… CTranslate2 ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
            size_mb = model_bin_path.stat().st_size / (1024**2)
            print(f"   íŒŒì¼ í¬ê¸°: {size_mb:.2f}MB")
            print()
            
        print("â³ faster-whisperë¡œ CTranslate2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
        print("   (ì´ ë‹¨ê³„ëŠ” 1-3ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print()
        
        try:
            model = WhisperModel(
                model_size_or_path=str(ct2_model_dir),
                device="cpu",
                compute_type="float32"
            )
            
            print_success("âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print()
            
            print("ğŸ“‹ ëª¨ë¸ ì •ë³´:")
            print(f"   âœ“ ëª¨ë¸ íƒ€ì…: Whisper Large-v3-Turbo")
            print(f"   âœ“ í˜•ì‹: CTranslate2 ë°”ì´ë„ˆë¦¬ (model.bin)")
            print(f"   âœ“ ë””ë°”ì´ìŠ¤: CPU")
            print(f"   âœ“ Compute Type: FP32")
            print()
            
            # ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            print("â³ ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘...")
            sample_audio_dir = BASE_DIR / "audio" / "samples"
            
            # ë””ë²„ê·¸: ê²½ë¡œ ì •ë³´ ì¶œë ¥
            print(f"   ìƒ˜í”Œ ê²½ë¡œ: {sample_audio_dir}")
            print(f"   ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {sample_audio_dir.exists()}")
            
            if sample_audio_dir.exists():
                print(f"   ë””ë ‰í† ë¦¬ ë‚´ìš©: {list(sample_audio_dir.glob('*.wav'))}")
            
            test_files = [
                ("short_0.5s.wav", "ì§§ì€ ì˜¤ë””ì˜¤ (0.5ì´ˆ)"),
                ("medium_3s.wav", "ì¤‘ê°„ ì˜¤ë””ì˜¤ (3ì´ˆ)"),
                ("long_10s.wav", "ê¸´ ì˜¤ë””ì˜¤ (10ì´ˆ)"),
            ]
            
            test_passed = False
            for audio_file, label in test_files:
                audio_path = sample_audio_dir / audio_file
                
                if audio_path.exists():
                    try:
                        # íŒŒì¼ í¬ê¸° í™•ì¸
                        file_size = audio_path.stat().st_size
                        print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {label} ({file_size} bytes)...")
                        
                        segments, info = model.transcribe(str(audio_path), language="ko")
                        list(segments)  # consume generator
                        print(f"   âœ“ {label} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                        test_passed = True
                    except Exception as e:
                        error_msg = str(e)
                        # íŠ¹ì • ì—ëŸ¬ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰ (mel-spectrogram í˜¸í™˜ì„± ë¬¸ì œ)
                        if "Invalid input features shape" in error_msg or "shape" in error_msg.lower():
                            print(f"   âš ï¸  {label} mel-spectrogram í˜•ì‹ ë¶ˆì¼ì¹˜ (ë¬´ì‹œ)")
                            test_passed = True  # ì´ ê²½ìš°ì—ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ëª¨ë¸ ìì²´ëŠ” ì •ìƒ)
                        else:
                            print(f"   âš ï¸  {label} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {error_msg[:80]}")
                else:
                    print(f"   âš ï¸  {label} ìƒ˜í”Œ íŒŒì¼ ì—†ìŒ: {audio_path}")
                    if sample_audio_dir.exists():
                        print(f"      {sample_audio_dir}ì˜ íŒŒì¼ ëª©ë¡: {list(sample_audio_dir.glob('*'))}")
            
            if test_passed:
                print()
                print("="*60)
                print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
                print("="*60)
                print()
                print("ğŸ‰ faster-whisperë¡œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
                print("   CTranslate2 ë³€í™˜ëœ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print()
            else:
                print()
                print("âš ï¸  ìƒ˜í”Œ ì˜¤ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                print("   ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ì„¸ìš”:")
                print("   python generate_sample_audio.py")
                print()
            
        except (MemoryError, OSError) as e:
            print_warn("ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
            print("í•„ìš” ë©”ëª¨ë¦¬: 16GB ì´ìƒ")
            print("ëª¨ë¸ì€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print()
            print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            print("   â€¢ EC2 ì¸ìŠ¤í„´ìŠ¤ ì—…ê·¸ë ˆì´ë“œ: t3.large â†’ t3.xlarge (16GB)")
            print("   â€¢ ë˜ëŠ” ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì¶”ê°€: sudo fallocate -l 8G /swapfile")
            print()
            print("ğŸ’¡ Dockerì—ì„œ í…ŒìŠ¤íŠ¸:")
            print("   docker run -it -p 8003:8003 -v $(pwd)/models:/app/models stt-engine:latest")
            print()
            validation_passed = False
            
        except Exception as e:
            print_warn(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"{str(e)[:200]}")
            print()
            print("ğŸ’¡ ë‹¤ìŒì„ ì‹œë„í•˜ì„¸ìš”:")
            print("   1. ëª¨ë¸ íŒŒì¼ ì¬ë‹¤ìš´ë¡œë“œ:")
            print("      rm -rf models/openai_whisper-large-v3-turbo")
            print("      python download_model_hf.py --skip-compress")
            print()
            print("   2. íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ:")
            print("      pip install --upgrade faster-whisper ctranslate2 transformers")
            print()
            validation_passed = False
                
    except ImportError:
        print_warn("faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("ì„¤ì¹˜: pip install faster-whisper")
        print()
        validation_passed = False

print()
print("=" * 60)
if validation_passed:
    print("âœ… ëª¨ë¸ ê²€ì¦ ì„±ê³µ!")
else:
    print("âš ï¸  ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨ ë˜ëŠ” ìŠ¤í‚µë¨")
print("=" * 60)
print()
