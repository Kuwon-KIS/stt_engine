#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (Complete)

ëª©ì :
  1. ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
  2. Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  3. CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)
  4. ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz)
  5. ì„œë²„ ì „ì†¡ ì¤€ë¹„

ì‚¬ìš©:
  conda activate stt-py311
  python download_model_hf.py
"""

import os
import sys
import ssl
import shutil
import subprocess
import gzip
import tarfile
from pathlib import Path
from datetime import datetime

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

def print_header(msg):
    print("\n" + "=" * 60)
    print(msg)
    print("=" * 60 + "\n")

def print_step(msg):
    print(f"\nğŸ“Œ {msg}")

def print_success(msg):
    print(f"âœ… {msg}")

def print_error(msg):
    print(f"âŒ {msg}")
    sys.exit(1)

print_header("ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„ (ë‹¤ìš´ë¡œë“œ + ë³€í™˜ + ì••ì¶•)")

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
model_specific_dir = models_dir / "openai_whisper-large-v3-turbo"

print(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {BASE_DIR}")
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print(f"ğŸ“ ëª¨ë¸ íŠ¹ì • ê²½ë¡œ: {model_specific_dir}")

# ============================================================================
# Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# ============================================================================

print_step("Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬")

if model_specific_dir.exists():
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {model_specific_dir}")
    print("ì‚­ì œ ì¤‘...")
    shutil.rmtree(model_specific_dir)
    print_success("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
else:
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ (ì‹ ê·œ ì„¤ì¹˜)")

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
models_dir.mkdir(parents=True, exist_ok=True)
model_specific_dir.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“ ì‹ ê·œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: {model_specific_dir}")

# ============================================================================
# Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ============================================================================

print_step("Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")

try:
    from huggingface_hub import snapshot_download
    
    MODEL_REPO = "openai/whisper-large-v3-turbo"
    
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_REPO}")
    print(f"â³ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.5GB)...")
    print()
    
    # snapshot_downloadë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥
    model_path = snapshot_download(
        repo_id=MODEL_REPO,
        cache_dir=None,
        local_dir=str(model_specific_dir),
        local_dir_use_symlinks=False,  # ğŸ”‘ ì‹¬ë§í¬ ì‚¬ìš© ì•ˆ í•¨
        resume_download=True,           # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
        force_download=False            # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    )
    
    print_success("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    
except ImportError:
    print_error("huggingface-hubì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install huggingface-hub")
    
except Exception as e:
    print_error(f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
# ============================================================================

print_step("Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦")

print(f"\nğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:")
if not any(model_specific_dir.iterdir()):
    print_error("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# í•„ìˆ˜ íŒŒì¼ í™•ì¸
REQUIRED_FILES = [
    "config.json",
    "model.safetensors",
    "generation_config.json",
    "preprocessor_config.json",
    "tokenizer.json",
]

all_found = True
total_size = 0

for req_file in REQUIRED_FILES:
    file_path = model_specific_dir / req_file
    if file_path.exists():
        size = file_path.stat().st_size / (1024**2)
        total_size += size
        print(f"  âœ“ {req_file} ({size:.2f}MB)")
    else:
        print(f"  âœ— {req_file} (MISSING)")
        all_found = False

if not all_found:
    print_error("ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")

print(f"\nğŸ“ ì „ì²´ í¬ê¸°: {total_size:.2f}MB")
print_success("íŒŒì¼ ê²€ì¦ ì™„ë£Œ")

# ============================================================================
# Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)
# ============================================================================

print_step("Step 4: CTranslate2 í¬ë§· ë³€í™˜")

print("â³ PyTorch ëª¨ë¸ì„ CTranslate2 ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
print("   (ì´ ë‹¨ê³„ëŠ” ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
print()

try:
    import torch
    import torch.nn as nn
    from ctranslate2.converters import TransformersConverter
    
    # PyTorch ëª¨ë¸ ê²½ë¡œ
    pytorch_model_path = model_specific_dir
    
    # CTranslate2 ë³€í™˜ê¸° ìƒì„±
    converter = TransformersConverter(
        model_name_or_path=str(pytorch_model_path),
        quantization=None,  # ì •ë°€ë„ ìœ ì§€ (no quantization)
        trust_remote_code=True
    )
    
    # ë³€í™˜ ì‹¤í–‰
    output_dir = model_specific_dir / "ctranslate2_model"
    converter.convert(str(output_dir), force=True)
    
    print_success("CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ")
    
    # ë³€í™˜ëœ íŒŒì¼ í™•ì¸
    print(f"\nğŸ“ ë³€í™˜ëœ íŒŒì¼:")
    for f in output_dir.glob("*"):
        if f.is_file():
            size = f.stat().st_size / (1024**2)
            print(f"  âœ“ {f.name} ({size:.2f}MB)")
    
    # model.bin ë³µì‚¬/ë§í¬ ìƒì„± (í˜¸í™˜ì„±)
    print(f"\nâ³ model.bin ë³µì‚¬ ì¤‘...")
    model_bin_src = output_dir / "model.bin"
    model_bin_dst = model_specific_dir / "model.bin"
    
    if model_bin_src.exists():
        # ë°”ì´ë„ˆë¦¬ íŒŒì¼ ë³µì‚¬
        shutil.copy2(model_bin_src, model_bin_dst)
        print_success(f"model.bin ìƒì„± ì™„ë£Œ: {model_bin_dst}")
    else:
        # ì‹¬ë§í¬ ìƒì„± (ê³µê°„ ì ˆì•½)
        if model_bin_dst.exists() or model_bin_dst.is_symlink():
            model_bin_dst.unlink()
        model_bin_dst.symlink_to(model_bin_src)
        print_success(f"model.bin ì‹¬ë§í¬ ìƒì„±: {model_bin_dst} -> {model_bin_src}")
    
except ImportError as e:
    print(f"âŒ CTranslate2 ë³€í™˜ ì‹¤íŒ¨: {e}")
    print("   ì„¤ì¹˜: pip install ctranslate2 torch")
    print("   âš ï¸  CTranslate2 ë³€í™˜ì€ ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. (openai-whisperë¡œ í´ë°± ê°€ëŠ¥)")
    print()
except Exception as e:
    print(f"âŒ CTranslate2 ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
    print("   âš ï¸  CTranslate2 ë³€í™˜ì€ ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. (openai-whisperë¡œ í´ë°± ê°€ëŠ¥)")
    print()

# ============================================================================
# Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz)
# ============================================================================

print_step("Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶•")

print("â³ ëª¨ë¸ íŒŒì¼ì„ tar.gzë¡œ ì••ì¶• ì¤‘...")
print("   (ì´ ë‹¨ê³„ëŠ” ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
print()

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
compressed_filename = f"whisper-large-v3-turbo_models_{timestamp}.tar.gz"
compressed_path = BASE_DIR / "build" / "output" / compressed_filename

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
compressed_path.parent.mkdir(parents=True, exist_ok=True)

try:
    # tar.gz ìƒì„±
    with tarfile.open(compressed_path, "w:gz") as tar:
        tar.add(model_specific_dir, arcname="models/openai_whisper-large-v3-turbo")
    
    print_success("ëª¨ë¸ ì••ì¶• ì™„ë£Œ")
    
    # ì••ì¶• íŒŒì¼ í¬ê¸°
    comp_size = compressed_path.stat().st_size / (1024**3)
    print(f"ğŸ“¦ ì••ì¶• íŒŒì¼: {compressed_path}")
    print(f"ğŸ“ í¬ê¸°: {comp_size:.2f}GB")
    
    # ì••ì¶•ë¥ 
    original_size = sum(f.stat().st_size for f in model_specific_dir.rglob("*") if f.is_file()) / (1024**3)
    compression_ratio = (1 - comp_size / original_size) * 100
    print(f"ğŸ“Š ì••ì¶•ë¥ : {compression_ratio:.1f}%")
    
except Exception as e:
    print_error(f"ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 6: MD5 ì²´í¬ì„¬ ìƒì„±
# ============================================================================

print_step("Step 6: ë¬´ê²°ì„± ê²€ì¦ íŒŒì¼ ìƒì„±")

import hashlib

def calculate_md5(file_path):
    md5 = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            md5.update(chunk)
    return md5.hexdigest()

md5_value = calculate_md5(compressed_path)
md5_path = compressed_path.with_suffix('.tar.gz.md5')

with open(md5_path, 'w') as f:
    f.write(f"{md5_value}  {compressed_path.name}\n")

print_success(f"MD5 ì²´í¬ì„¬: {md5_value}")
print(f"íŒŒì¼: {md5_path}")

# ============================================================================
# Step 7: ìµœì¢… ìš”ì•½
# ============================================================================

print_header("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

print("ğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
print(f"  1. ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")
print(f"  2. ì••ì¶• íŒŒì¼: {compressed_path}")
print(f"  3. MD5 ì²´í¬ì„¬: {md5_path}")
print()

print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print("  1. ì••ì¶• íŒŒì¼ì„ ìš´ì˜ ì„œë²„ë¡œ ì „ì†¡:")
print(f"     scp {compressed_path} deploy-user@server:/tmp/")
print()
print("  2. ìš´ì˜ ì„œë²„ì—ì„œ ì••ì¶• í•´ì œ:")
print(f"     cd /path/to/deployment")
print(f"     tar -xzf {compressed_filename}")
print()
print("  3. Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸:")
print(f"     docker run -v /path/to/models:/app/models stt-engine:cuda129-v1.2")
print()

print("âœ¨ ëª¨ë“  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")