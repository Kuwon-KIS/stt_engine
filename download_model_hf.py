#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (huggingface-hub ì§ì ‘ ì‚¬ìš©)

ëª©ì : 
  - Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  - ì‹¬ë§í¬ ì—†ì´ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥ (ì˜¤í”„ë¼ì¸ ë°°í¬ìš©)
  - ì•ˆì •ì ì¸ ë‹¤ìš´ë¡œë“œ (ì¬ì‹œë„ ì§€ì›)

ì‚¬ìš©:
  conda activate stt-py311
  python download_model_hf.py
"""

import os
import sys
import ssl
from pathlib import Path

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

print("ğŸ”„ Whisper Large-V3-Turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
print("=" * 60)

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
models_dir.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print()

try:
    from huggingface_hub import snapshot_download
    
    MODEL_REPO = "openai/whisper-large-v3-turbo"
    
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_REPO}")
    print(f"â³ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.3GB)...")
    print()
    
    # snapshot_downloadë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥
    # local_dir_use_symlinks=False: ì‹¬ë§í¬ ëŒ€ì‹  ì‹¤ì œ íŒŒì¼ ë³µì‚¬
    model_path = snapshot_download(
        repo_id=MODEL_REPO,
        cache_dir=str(models_dir),
        local_dir=str(models_dir / "model"),  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ
        local_dir_use_symlinks=False,         # ğŸ”‘ ì‹¬ë§í¬ ì‚¬ìš© ì•ˆ í•¨
        resume_download=True,                 # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
        force_download=False                  # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    )
    
    print()
    print("=" * 60)
    print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print("=" * 60)
    print()
    
    # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
    import subprocess
    
    # ëª¨ë¸ í´ë” êµ¬ì¡° í™•ì¸
    print("ğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:")
    result = subprocess.run(
        f"find {models_dir / 'model'} -type f ! -name '.DS_Store' -exec ls -lh {{}} \\; | awk '{{print $5, $9}}'",
        shell=True, capture_output=True, text=True
    )
    
    files_list = result.stdout.strip().split('\n')
    files_list = [f for f in files_list if f]
    
    print(f"\nâœ“ íŒŒì¼ ëª©ë¡:")
    for line in files_list:
        print(f"  {line}")
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    print(f"\nâœ“ í•„ìˆ˜ íŒŒì¼ í™•ì¸:")
    REQUIRED_FILES = [
        "config.json",
        "model.safetensors",
        "generation_config.json",
        "preprocessor_config.json",
        "tokenizer.json",
    ]
    
    all_found = True
    for req_file in REQUIRED_FILES:
        file_path = models_dir / "model" / req_file
        if file_path.exists():
            size = file_path.stat().st_size / (1024**2)
            print(f"  âœ“ {req_file} ({size:.2f}MB)")
        else:
            print(f"  âœ— {req_file} (MISSING)")
            all_found = False
    
    if not all_found:
        print("\nâš ï¸  ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        sys.exit(1)
    
    # ì „ì²´ í¬ê¸° í™•ì¸
    result = subprocess.run(
        f"du -sh {models_dir / 'model'}",
        shell=True, capture_output=True, text=True
    )
    print(f"\nğŸ“ ì „ì²´ í¬ê¸°: {result.stdout.strip()}")
    
    print("\nâœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ!")
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ëª¨ë¸ êµ¬ì¡° í™•ì¸: python validate_model.py")
    print("  2. ëª¨ë¸ ì••ì¶•: python compress_model.py")
    print("  3. ì„œë²„ë¡œ ì „ì†¡: bash scripts/transfer-to-server.sh")
    
except ImportError:
    print("âŒ huggingface-hubì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
    print("ì„¤ì¹˜: pip install huggingface-hub")
    sys.exit(1)
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc()
    sys.exit(1)
