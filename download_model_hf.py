#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (Complete)

ëª©ì :
  1. ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
  2. Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  3. CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)
  4. ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz)
  5. ì„œë²„ ì „ì†¡ ì¤€ë¹„

ì‚¬ìš©:
  conda activate stt-py311
  python download_model_hf.py
"""

import os
import sys
import ssl
import shutil
import subprocess
import gzip
import tarfile
from pathlib import Path
from datetime import datetime

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬/ë°©í™”ë²½ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

# urllib3 ê²½ê³  ë¹„í™œì„±í™”
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

def print_header(msg):
    print("\n" + "=" * 60)
    print(msg)
    print("=" * 60 + "\n")

def print_step(msg):
    print(f"\nğŸ“Œ {msg}")

def print_success(msg):
    print(f"âœ… {msg}")

def print_error(msg):
    print(f"âŒ {msg}")
    sys.exit(1)

print_header("ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„ (ë‹¤ìš´ë¡œë“œ + ë³€í™˜ + ì••ì¶•)")

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
model_specific_dir = models_dir / "openai_whisper-large-v3-turbo"

print(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {BASE_DIR}")
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print(f"ğŸ“ ëª¨ë¸ íŠ¹ì • ê²½ë¡œ: {model_specific_dir}")

# ============================================================================
# Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# ============================================================================

print_step("Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬")

if model_specific_dir.exists():
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {model_specific_dir}")
    print("ì‚­ì œ ì¤‘...")
    shutil.rmtree(model_specific_dir)
    print_success("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
else:
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ (ì‹ ê·œ ì„¤ì¹˜)")

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
models_dir.mkdir(parents=True, exist_ok=True)
model_specific_dir.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“ ì‹ ê·œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: {model_specific_dir}")

# ============================================================================
# Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ============================================================================

print_step("Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")

try:
    from huggingface_hub import snapshot_download
    
    # ========== SSL ê²€ì¦ ì™„ì „ ë¹„í™œì„±í™” ==========
    import requests
    from requests.packages.urllib3.exceptions import InsecureRequestWarning
    
    # urllib3 ê²½ê³  ë¬´ì‹œ
    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œë„ ë¹„í™œì„±í™”
    os.environ['REQUESTS_CA_BUNDLE'] = ''
    os.environ['CURL_CA_BUNDLE'] = ''
    
    MODEL_REPO = "openai/whisper-large-v3-turbo"
    
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_REPO}")
    print(f"â³ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.5GB)...")
    print()
    
    # snapshot_downloadë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥
    model_path = snapshot_download(
        repo_id=MODEL_REPO,
        cache_dir=None,
        local_dir=str(model_specific_dir),
        local_dir_use_symlinks=False,  # ğŸ”‘ ì‹¬ë§í¬ ì‚¬ìš© ì•ˆ í•¨
        resume_download=True,           # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
        force_download=False            # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
    )
    
    print_success("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    
except ImportError:
    print_error("huggingface-hubì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install huggingface-hub")
    
except Exception as e:
    print_error(f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
# ============================================================================

print_step("Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦")

print(f"\nğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:")
if not any(model_specific_dir.iterdir()):
    print_error("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# í•„ìˆ˜ íŒŒì¼ í™•ì¸
REQUIRED_FILES = [
    "config.json",
    "model.safetensors",
    "generation_config.json",
    "preprocessor_config.json",
    "tokenizer.json",
]

all_found = True
total_size = 0

for req_file in REQUIRED_FILES:
    file_path = model_specific_dir / req_file
    if file_path.exists():
        size = file_path.stat().st_size / (1024**2)
        total_size += size
        print(f"  âœ“ {req_file} ({size:.2f}MB)")
    else:
        print(f"  âœ— {req_file} (MISSING)")
        all_found = False

if not all_found:
    print_error("ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")

print(f"\nğŸ“ ì „ì²´ í¬ê¸°: {total_size:.2f}MB")
print_success("íŒŒì¼ ê²€ì¦ ì™„ë£Œ")

# ============================================================================
# Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)
# ============================================================================

print_step("Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)")

print("â³ PyTorch ëª¨ë¸ì„ CTranslate2 ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
print("   (ì´ ë‹¨ê³„ëŠ” 5-15ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
print()

output_dir = model_specific_dir / "ctranslate2_model"
conversion_success = False

# CTranslate2 CLI ë„êµ¬ë¡œ ë³€í™˜ (Hugging Face ëª¨ë¸ ID ì‚¬ìš©)
try:
    print("â³ ct2-transformers-converter CLI ë„êµ¬ë¡œ ë³€í™˜ ì¤‘...")
    print("   ëª¨ë¸: openai/whisper-large-v3-turbo")
    print(f"   ì¶œë ¥: {output_dir}")
    print()
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # CLI ë„êµ¬ ì‹¤í–‰
    cmd = [
        "conda", "run", "-n", "stt-py311",
        "ct2-transformers-converter",
        "--model", "openai/whisper-large-v3-turbo",
        "--output_dir", str(output_dir),
        "--force",
        "--quantization", "int8"
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=900)
    
    if result.returncode == 0:
        print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
        conversion_success = True
    else:
        # CLI ë„êµ¬ ì‹¤íŒ¨ ì‹œ Python APIë¡œ ì¬ì‹œë„
        print(f"âš ï¸  CLI ë„êµ¬ ì‹¤íŒ¨, Python APIë¡œ ì¬ì‹œë„...")
        print()
        
except Exception as e:
    print(f"âš ï¸  CLI ë„êµ¬ ì˜¤ë¥˜: {e}")
    print("   Python APIë¡œ ì¬ì‹œë„...")
    print()

# íŒŒì´ì¬ API ì‚¬ìš© (HF ëª¨ë¸ ID)
if not conversion_success:
    try:
        from ctranslate2.converters.transformers import TransformersConverter
        
        print("â³ Python API(TransformersConverter)ë¡œ ë³€í™˜ ì¤‘...")
        print("   ëª¨ë¸: openai/whisper-large-v3-turbo (Hugging Face)")
        print()
        
        # HF ëª¨ë¸ IDë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜
        converter = TransformersConverter("openai/whisper-large-v3-turbo")
        
        converter.convert(
            output_dir=str(output_dir),
            quantization="int8",
            force=True
        )
        
        print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
        conversion_success = True
        
    except Exception as e:
        error_msg = str(e)
        if len(error_msg) > 300:
            error_msg = error_msg[:300] + "..."
        print(f"âš ï¸  ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {error_msg}")
        print()

# ë³€í™˜ ê²°ê³¼ í™•ì¸
print()
if conversion_success and output_dir.exists():
    bin_files = list(output_dir.glob("*.bin"))
    
    if bin_files:
        print("âœ… ë³€í™˜ëœ CTranslate2 ëª¨ë¸ íŒŒì¼:")
        total_size = 0
        
        for bin_file in sorted(bin_files):
            size = bin_file.stat().st_size / (1024**2)
            total_size += size
            print(f"   âœ“ {bin_file.name} ({size:.2f}MB)")
        
        print(f"\n   ğŸ“ í•©ê³„: {total_size:.2f}MB")
        
        # model.bin ì‹¬ë§í¬ ìƒì„± (faster-whisper í˜¸í™˜ì„±)
        print()
        print("â³ ì‹¬ë§í¬ ìƒì„± ì¤‘...")
        
        model_bin_src = bin_files[0]
        model_bin_link = model_specific_dir / "model.bin"
        
        if model_bin_link.exists() or model_bin_link.is_symlink():
            model_bin_link.unlink()
        
        model_bin_link.symlink_to(model_bin_src)
        print_success("âœ… model.bin ì‹¬ë§í¬ ìƒì„± ì™„ë£Œ")
        print(f"   ì†ŒìŠ¤: {model_bin_src.name}")
        print(f"   ëŒ€ìƒ: model.bin")
        
    else:
        print("âš ï¸  ë³€í™˜ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print()
        print("ğŸ’¡ ìˆ˜ë™ ë³€í™˜ ì‹œë„:")
        print(f"   conda activate stt-py311")
        print(f"   ct2-transformers-converter --model openai/whisper-large-v3-turbo \\")
        print(f"     --output_dir {output_dir} --force --quantization int8")
else:
    print("âš ï¸  CTranslate2 ë³€í™˜ ì‹¤íŒ¨")
    print()
    print("ğŸ’¡ ìˆ˜ë™ ë³€í™˜ ì‹œë„:")
    print(f"   conda activate stt-py311")
    print(f"   ct2-transformers-converter --model openai/whisper-large-v3-turbo \\")
    print(f"     --output_dir {output_dir} --force --quantization int8")

# ============================================================================
# Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz)
# ============================================================================

print_step("Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶•")

print("â³ ëª¨ë¸ íŒŒì¼ì„ tar.gzë¡œ ì••ì¶• ì¤‘...")
print("   (ì´ ë‹¨ê³„ëŠ” ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
print()

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
compressed_filename = f"whisper-large-v3-turbo_models_{timestamp}.tar.gz"
compressed_path = BASE_DIR / "build" / "output" / compressed_filename

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
compressed_path.parent.mkdir(parents=True, exist_ok=True)

try:
    # tar.gz ìƒì„±
    with tarfile.open(compressed_path, "w:gz") as tar:
        tar.add(model_specific_dir, arcname="models/openai_whisper-large-v3-turbo")
    
    print_success("ëª¨ë¸ ì••ì¶• ì™„ë£Œ")
    
    # ì••ì¶• íŒŒì¼ í¬ê¸°
    comp_size = compressed_path.stat().st_size / (1024**3)
    print(f"ğŸ“¦ ì••ì¶• íŒŒì¼: {compressed_path}")
    print(f"ğŸ“ í¬ê¸°: {comp_size:.2f}GB")
    
    # ì••ì¶•ë¥ 
    original_size = sum(f.stat().st_size for f in model_specific_dir.rglob("*") if f.is_file()) / (1024**3)
    compression_ratio = (1 - comp_size / original_size) * 100
    print(f"ğŸ“Š ì••ì¶•ë¥ : {compression_ratio:.1f}%")
    
except Exception as e:
    print_error(f"ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 6: MD5 ì²´í¬ì„¬ ìƒì„±
# ============================================================================

print_step("Step 6: ë¬´ê²°ì„± ê²€ì¦ íŒŒì¼ ìƒì„±")

import hashlib

def calculate_md5(file_path):
    md5 = hashlib.md5()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            md5.update(chunk)
    return md5.hexdigest()

md5_value = calculate_md5(compressed_path)
md5_path = compressed_path.with_suffix('.tar.gz.md5')

with open(md5_path, 'w') as f:
    f.write(f"{md5_value}  {compressed_path.name}\n")

print_success(f"MD5 ì²´í¬ì„¬: {md5_value}")
print(f"íŒŒì¼: {md5_path}")

# ============================================================================
# Step 7: ìµœì¢… ìš”ì•½
# ============================================================================

print_header("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

print("ğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
print(f"  1. ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")
print(f"  2. ì••ì¶• íŒŒì¼: {compressed_path}")
print(f"  3. MD5 ì²´í¬ì„¬: {md5_path}")
print()

print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print("  1. ì••ì¶• íŒŒì¼ì„ ìš´ì˜ ì„œë²„ë¡œ ì „ì†¡:")
print(f"     scp {compressed_path} deploy-user@server:/tmp/")
print()
print("  2. ìš´ì˜ ì„œë²„ì—ì„œ ì••ì¶• í•´ì œ:")
print(f"     cd /path/to/deployment")
print(f"     tar -xzf {compressed_filename}")
print()
print("  3. Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸:")
print(f"     docker run -v /path/to/models:/app/models stt-engine:cuda129-v1.2")
print()

print("âœ¨ ëª¨ë“  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

# ============================================================================
# Step 7: faster-whisper ê²€ì¦ (CTranslate2 ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸)
# ============================================================================

print()
print("=" * 60)
print("ğŸ” ëª¨ë¸ ê²€ì¦ (faster-whisper ë¡œë“œ í…ŒìŠ¤íŠ¸)")
print("=" * 60)
print()

try:
    from faster_whisper import WhisperModel
    import numpy as np
    
    # CTranslate2ë¡œ ë³€í™˜ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸
    ct2_model_dir = model_specific_dir / "ctranslate2_model"
    model_bin_path = ct2_model_dir / "model.bin"
    
    if not model_bin_path.exists():
        raise FileNotFoundError(f"CTranslate2 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_bin_path}")
    
    print("â³ faster-whisperë¡œ CTranslate2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"   ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
    print("   (ì´ ë‹¨ê³„ëŠ” 1-3ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()
    
    # CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë¡œë“œ
    model = WhisperModel(
        model_size_or_path=str(ct2_model_dir),
        device="cpu",
        compute_type="int8"
    )
    
    print_success("âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    print()
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print("ğŸ“‹ ëª¨ë¸ ì •ë³´:")
    print(f"   âœ“ ëª¨ë¸ íƒ€ì…: Whisper Large-v3-Turbo")
    print(f"   âœ“ í˜•ì‹: CTranslate2 ë°”ì´ë„ˆë¦¬ (model.bin)")
    print(f"   âœ“ ë””ë°”ì´ìŠ¤: CPU")
    print(f"   âœ“ ì–‘ìí™”: INT8 (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )")
    print()
    
    # ë”ë¯¸ ì˜¤ë””ì˜¤ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("â³ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘ (ë”ë¯¸ ì˜¤ë””ì˜¤)...")
    
    # 1ì´ˆì˜ ë”ë¯¸ ì˜¤ë””ì˜¤ ìƒì„± (16kHz, ëª¨ë…¸)
    dummy_audio = np.zeros((16000,), dtype=np.float32)
    
    # ì¶”ë¡  ì‹¤í–‰
    segments, info = model.transcribe(dummy_audio, language="ko")
    
    print_success("âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print()
    
    print("ğŸ“Š ì¶”ë¡  ê²°ê³¼:")
    print(f"   âœ“ ê°ì§€ëœ ì–¸ì–´: {info.language}")
    print(f"   âœ“ ì–¸ì–´ ì‹ ë¢°ë„: {info.language_probability:.2%}")
    print(f"   âœ“ ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ì‹œê°„: {info.duration:.2f}ì´ˆ")
    
    segment_list = list(segments)
    print(f"   âœ“ ê°ì§€ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(segment_list)}ê°œ")
    print()
    
    print("="*60)
    print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
    print("="*60)
    print()
    print("ğŸ‰ faster-whisperë¡œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    print("   CTranslate2 ë³€í™˜ëœ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print()
    
except FileNotFoundError as e:
    print(f"âš ï¸  íŒŒì¼ ì˜¤ë¥˜: {e}")
    print()
    print("ğŸ’¡ í•´ê²° ë°©ë²•:")
    print("   CTranslate2 ë³€í™˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    print("   ë§Œì•½ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ìˆ˜ë™ ë³€í™˜:")
    print()
    print("   conda activate stt-py311")
    print(f"   ct2-transformers-converter --model openai/whisper-large-v3-turbo \\")
    print(f"     --output_dir {ct2_model_dir} --force --quantization int8")
    print()
    
except ImportError:
    print("âš ï¸  faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
    print()
    print("ì„¤ì¹˜: pip install faster-whisper")
    print()
    
except Exception as e:
    print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    print()
    print("ğŸ“ ë””ë²„ê¹…:")
    print("   1. CTranslate2 ë³€í™˜ ìƒíƒœ í™•ì¸:")
    print(f"      ls -lh {model_specific_dir}/ctranslate2_model/")
    print()
    print("   2. íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸:")
    print("      pip list | grep -E 'faster-whisper|ctranslate2'")
    print()
    print("   3. íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ:")
    print("      pip install --upgrade faster-whisper ctranslate2 torch")
    print()