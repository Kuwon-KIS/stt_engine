#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (ì˜µì…˜ í¬í•¨)

ëª©ì :
  1. ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
  2. Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  3. CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±) - ì˜µì…˜
  4. ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz) - ì˜µì…˜
  5. ì„œë²„ ì „ì†¡ ì¤€ë¹„

ì‚¬ìš©:
  conda activate stt-py311
  python download_model_hf.py [ì˜µì…˜]

ì˜µì…˜:
  --no-convert        CTranslate2 ë³€í™˜ ìŠ¤í‚µ (PyTorch ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ)
  --skip-compress     ëª¨ë¸ íŒŒì¼ ì••ì¶• ìŠ¤í‚µ
  --skip-test         ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  --help              ë„ì›€ë§ ì¶œë ¥

ì˜ˆì‹œ:
  python download_model_hf.py                 # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’, ê¶Œì¥)
  python download_model_hf.py --no-convert    # CTranslate2 ë³€í™˜ ìŠ¤í‚µ
  python download_model_hf.py --skip-test     # í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  python download_model_hf.py --skip-compress # ì••ì¶• ìŠ¤í‚µ

âš ï¸  íŒ¨í‚¤ì§€ ë²„ì „ í˜¸í™˜ì„±:
  ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ë²„ì „ê³¼ í˜¸í™˜ì„±ì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤:
  - faster-whisper==1.2.1 (ì—”ì§„ê³¼ ë™ì¼, requirements.txt ì°¸ê³ )
  - ctranslate2==4.7.1 (ì—”ì§„ê³¼ ë™ì¼, requirements.txt ì°¸ê³ )
  - transformers>=4.30,<6 (ì—”ì§„ê³¼ ë™ì¼)
  
  ğŸ“ ì£¼ì˜: build-server-models.shì—ì„œ ìœ„ ë²„ì „ë“¤ì´ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤
  ë§Œì•½ ë‹¤ë¥¸ ë²„ì „ì„ ì‚¬ìš©í•˜ë©´ ì—”ì§„ ë¡œë”© ì‹œ í˜¸í™˜ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import ssl
import shutil
import subprocess
import tarfile
import argparse
from pathlib import Path
from datetime import datetime

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬/ë°©í™”ë²½ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

# urllib3 ê²½ê³  ë¹„í™œì„±í™”
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'

# ============================================================================
# ëª…ë ¹ì¤„ ì¸ìˆ˜ ì²˜ë¦¬
# ============================================================================

parser = argparse.ArgumentParser(
    description='STT Engine ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸',
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog="""
ì˜ˆì‹œ:
  python download_model_hf.py                 # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’, ê¶Œì¥)
  python download_model_hf.py --no-convert    # CTranslate2 ë³€í™˜ ìŠ¤í‚µ
  python download_model_hf.py --skip-test     # ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
  python download_model_hf.py --skip-compress # ì••ì¶• ìŠ¤í‚µ
    """
)
parser.add_argument('--no-convert', action='store_true', 
                    help='CTranslate2 ë³€í™˜ ìŠ¤í‚µ (PyTorch ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ)')
parser.add_argument('--skip-compress', action='store_true',
                    help='ëª¨ë¸ íŒŒì¼ ì••ì¶• ìŠ¤í‚µ')
parser.add_argument('--skip-test', action='store_true',
                    help='ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ')

args = parser.parse_args()

# ì˜µì…˜ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’)
should_convert = not args.no_convert
should_compress = not args.skip_compress
should_test = not args.skip_test

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def print_header(msg):
    print("\n" + "=" * 60)
    print(msg)
    print("=" * 60 + "\n")

def print_step(msg):
    print(f"\nğŸ“Œ {msg}")

def print_success(msg):
    print(f"âœ… {msg}")

def print_error(msg):
    print(f"âŒ {msg}")
    sys.exit(1)

def print_warn(msg):
    print(f"âš ï¸  {msg}")

def check_and_install_faster_whisper():
    """faster-whisper ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸, ì—†ìœ¼ë©´ ì„¤ì¹˜"""
    try:
        import faster_whisper
        return True
    except ImportError:
        print("âš ï¸  faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("ì„¤ì¹˜ ì¤‘...")
        
        try:
            import subprocess
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "-q", "faster-whisper"
            ])
            print("âœ… faster-whisper ì„¤ì¹˜ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ faster-whisper ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
            print("ìˆ˜ë™ ì„¤ì¹˜: pip install faster-whisper")
            return False

# ============================================================================
# ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘
# ============================================================================

print_header("ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„ (ë‹¤ìš´ë¡œë“œ + ì˜µì…˜ ë³€í™˜ + ì••ì¶•)")

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
model_specific_dir = models_dir / "openai_whisper-large-v3-turbo"

print(f"ğŸ“ ê¸°ë³¸ ê²½ë¡œ: {BASE_DIR}")
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print(f"ğŸ“ ëª¨ë¸ íŠ¹ì • ê²½ë¡œ: {model_specific_dir}")
print()

# ============================================================================
# Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# ============================================================================

print_step("Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬")

if model_specific_dir.exists():
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {model_specific_dir}")
    print("ì‚­ì œ ì¤‘...")
    shutil.rmtree(model_specific_dir)
    print_success("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
else:
    print(f"ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ (ì‹ ê·œ ì„¤ì¹˜)")

models_dir.mkdir(parents=True, exist_ok=True)
model_specific_dir.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“ ì‹ ê·œ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±: {model_specific_dir}")

# ============================================================================
# Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# ============================================================================

print_step("Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")

try:
    from huggingface_hub import snapshot_download
    
    MODEL_REPO = "openai/whisper-large-v3-turbo"
    
    print(f"ğŸ“¦ ëª¨ë¸: {MODEL_REPO}")
    print(f"â³ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.5GB)...")
    print()
    
    # snapshot_downloadë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥
    model_path = snapshot_download(
        repo_id=MODEL_REPO,
        cache_dir=None,
        local_dir=str(model_specific_dir),
        local_dir_use_symlinks=False,
        resume_download=True,
        force_download=False
    )
    
    print_success("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    
except ImportError:
    print_error("huggingface-hubì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install huggingface-hub")
except Exception as e:
    print_error(f"ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
# ============================================================================

print_step("Step 3: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦")

print(f"\nğŸ“ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼:")
if not any(model_specific_dir.iterdir()):
    print_error("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

REQUIRED_FILES = [
    "config.json",
    "model.safetensors",
    "generation_config.json",
    "preprocessor_config.json",
    "tokenizer.json",
]

all_found = True
total_size = 0

for req_file in REQUIRED_FILES:
    file_path = model_specific_dir / req_file
    if file_path.exists():
        size = file_path.stat().st_size / (1024**2)
        total_size += size
        print(f"  âœ“ {req_file} ({size:.2f}MB)")
    else:
        print(f"  âœ— {req_file} (MISSING)")
        all_found = False

if not all_found:
    print_error("ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")

print(f"\nğŸ“ ì „ì²´ í¬ê¸°: {total_size:.2f}MB")
print_success("íŒŒì¼ ê²€ì¦ ì™„ë£Œ")

# ============================================================================
# Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±) - ì¡°ê±´ë¶€
# ============================================================================

print_step("Step 4: CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)")

if not should_convert:
    print("â­ï¸  CTranslate2 ë³€í™˜ ìŠ¤í‚µ (--no-convert ì˜µì…˜ ì‚¬ìš©)")
    print()
    print("âš ï¸  ì£¼ì˜: CTranslate2 ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤:")
    print("   â€¢ faster-whisper ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€")
    print("   â€¢ transformers ë˜ëŠ” OpenAI Whisper ë°±ì—”ë“œë¡œ í´ë°±")
    print()
    conversion_success = False
else:
    print("â³ PyTorch ëª¨ë¸ì„ CTranslate2 ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    print("   (ì´ ë‹¨ê³„ëŠ” 5-15ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    output_dir = model_specific_dir / "ctranslate2_model"
    conversion_success = False

    # CTranslate2 CLI ë„êµ¬ë¡œ ë³€í™˜
    try:
        print("â³ ct2-transformers-converter CLI ë„êµ¬ë¡œ ë³€í™˜ ì¤‘...")
        print("   ëª¨ë¸: openai/whisper-large-v3-turbo")
        print(f"   ì¶œë ¥: {output_dir}")
        print()
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        cmd = [
            "conda", "run", "-n", "stt-py311",
            "ct2-transformers-converter",
            "--model", "openai/whisper-large-v3-turbo",
            "--output_dir", str(output_dir),
            "--force",
            "--quantization", "int8"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=900)
        
        if result.returncode == 0:
            print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
            conversion_success = True
        else:
            print(f"âš ï¸  CLI ë„êµ¬ ì‹¤íŒ¨, Python APIë¡œ ì¬ì‹œë„...")
            print()
            
    except Exception as e:
        print(f"âš ï¸  CLI ë„êµ¬ ì˜¤ë¥˜: {e}")
        print("   Python APIë¡œ ì¬ì‹œë„...")
        print()

    # íŒŒì´ì¬ API ì‚¬ìš© (HF ëª¨ë¸ ID)
    if not conversion_success:
        try:
            from ctranslate2.converters.transformers import TransformersConverter
            
            print("â³ Python API(TransformersConverter)ë¡œ ë³€í™˜ ì¤‘...")
            print("   ëª¨ë¸: openai/whisper-large-v3-turbo (Hugging Face)")
            print()
            
            converter = TransformersConverter("openai/whisper-large-v3-turbo")
            
            converter.convert(
                output_dir=str(output_dir),
                force=True,
            )
            
            print_success("âœ… CTranslate2 ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
            conversion_success = True
            
        except Exception as e:
            error_msg = str(e)
            if len(error_msg) > 300:
                error_msg = error_msg[:300] + "..."
            print(f"âš ï¸  ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {error_msg}")
            print()

    # ë³€í™˜ ê²°ê³¼ í™•ì¸
    print()
    if conversion_success and output_dir.exists():
        bin_files = list(output_dir.glob("*.bin"))
        config_files = list(output_dir.glob("*.json"))
        
        print("âœ… ë³€í™˜ëœ CTranslate2 ëª¨ë¸ íŒŒì¼:")
        
        if bin_files:
            total_size = 0
            for bin_file in sorted(bin_files):
                size = bin_file.stat().st_size / (1024**2)
                total_size += size
                print(f"   âœ“ {bin_file.name} ({size:.2f}MB)")
            print(f"\n   ğŸ“ í•©ê³„: {total_size:.2f}MB")
        
        if config_files:
            print("\n   âœ“ ì„¤ì • íŒŒì¼:")
            for cfg_file in sorted(config_files):
                print(f"     - {cfg_file.name}")
        
        # model.bin ì¤€ë¹„ (ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ë˜ëŠ” ì¹´í”¼)
        # ì¤‘ìš”: ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ Docker(/app/models)ì™€ ìš´ì˜ ì„œë²„(/data/models) ëª¨ë‘ í˜¸í™˜
        print()
        print("â³ model.bin íŒŒì¼ ì¤€ë¹„ ì¤‘...")
        
        if bin_files:
            model_bin_src = bin_files[0]
            model_bin_link = model_specific_dir / "model.bin"
            
            # ê¸°ì¡´ íŒŒì¼ ì •ë¦¬
            if model_bin_link.exists() or model_bin_link.is_symlink():
                try:
                    model_bin_link.unlink()
                except Exception as e:
                    print(f"âš ï¸  ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            # ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ìƒì„± ì‹œë„
            try:
                # ìƒëŒ€ ê²½ë¡œ: ctranslate2_model ë””ë ‰í† ë¦¬ ì•ˆì˜ bin íŒŒì¼ì„ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ ì°¸ì¡°
                relative_path = model_bin_src.relative_to(model_specific_dir)
                model_bin_link.symlink_to(relative_path)
                print_success("âœ… model.bin ìƒëŒ€ ê²½ë¡œ ì‹¬ë§í¬ ìƒì„± ì™„ë£Œ")
                print(f"   ì†ŒìŠ¤: {relative_path}")
                print(f"   ëŒ€ìƒ: model.bin")
                print(f"   (Docker: /app/models â†’ ìš´ì˜: /data/modelsì—ì„œë„ ì‘ë™)")
            except Exception as e:
                # ì‹¬ë§í¬ ì‹¤íŒ¨ ì‹œ íŒŒì¼ ë³µì‚¬ (Windows/ê¶Œí•œ ë¬¸ì œ í•´ê²°)
                print(f"âš ï¸  ì‹¬ë§í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                print("   íŒŒì¼ ë³µì‚¬ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
                try:
                    import shutil
                    shutil.copy2(model_bin_src, model_bin_link)
                    print_success("âœ… model.bin íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")
                    print(f"   ì†ŒìŠ¤: {model_bin_src.name}")
                    print(f"   ëŒ€ìƒ: model.bin")
                except Exception as copy_e:
                    print(f"âŒ íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {copy_e}")
        else:
            print("âš ï¸  ë³€í™˜ëœ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    else:
        print("âš ï¸  CTranslate2 ë³€í™˜ ì‹¤íŒ¨")
        print()
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸:")
        print("      pip list | grep -E 'ctranslate2|faster-whisper'")
        print()
        print("   2. íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ:")
        print("      pip install --upgrade ctranslate2 faster-whisper transformers")
        print()
        print("   3. ìˆ˜ë™ ë³€í™˜ ì‹œë„:")
        print(f"      ct2-transformers-converter --model openai/whisper-large-v3-turbo \\")
        print(f"        --output_dir {output_dir} --force")

# ============================================================================
# Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶• (tar.gz) - ì¡°ê±´ë¶€
# ============================================================================

print_step("Step 5: ëª¨ë¸ íŒŒì¼ ì••ì¶•")

compressed_path = None

if not should_compress:
    print("â­ï¸  ëª¨ë¸ ì••ì¶• ìŠ¤í‚µ (--skip-compress ì˜µì…˜ ì‚¬ìš©)")
    print()
else:
    print("â³ ëª¨ë¸ íŒŒì¼ì„ tar.gzë¡œ ì••ì¶• ì¤‘...")
    print("   (ì´ ë‹¨ê³„ëŠ” ëª‡ ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    compressed_filename = f"whisper-large-v3-turbo_models_{timestamp}.tar.gz"
    compressed_path = BASE_DIR / "build" / "output" / compressed_filename

    compressed_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        with tarfile.open(compressed_path, "w:gz") as tar:
            tar.add(model_specific_dir, arcname="models/openai_whisper-large-v3-turbo")
        
        print_success("ëª¨ë¸ ì••ì¶• ì™„ë£Œ")
        
        comp_size = compressed_path.stat().st_size / (1024**3)
        print(f"ğŸ“¦ ì••ì¶• íŒŒì¼: {compressed_path}")
        print(f"ğŸ“ í¬ê¸°: {comp_size:.2f}GB")
        
        original_size = sum(f.stat().st_size for f in model_specific_dir.rglob("*") if f.is_file()) / (1024**3)
        compression_ratio = (1 - comp_size / original_size) * 100
        print(f"ğŸ“Š ì••ì¶•ë¥ : {compression_ratio:.1f}%")
        
    except Exception as e:
        print_error(f"ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")

# ============================================================================
# Step 6: MD5 ì²´í¬ì„¬ ìƒì„± - ì¡°ê±´ë¶€
# ============================================================================

print_step("Step 6: ë¬´ê²°ì„± ê²€ì¦ íŒŒì¼ ìƒì„±")

if compressed_path is None:
    print("â­ï¸  MD5 ì²´í¬ì„¬ ìƒì„± ìŠ¤í‚µ (ì••ì¶•ì´ ìŠ¤í‚µë¨)")
    print()
else:
    import hashlib

    def calculate_md5(file_path):
        md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                md5.update(chunk)
        return md5.hexdigest()

    md5_value = calculate_md5(compressed_path)
    md5_path = compressed_path.parent / f"{compressed_path.name}.md5"

    with open(md5_path, 'w') as f:
        f.write(f"{md5_value}  {compressed_path.name}\n")

    print_success(f"MD5 ì²´í¬ì„¬: {md5_value}")
    print(f"íŒŒì¼: {md5_path}")

# ============================================================================
# Step 7: ìµœì¢… ìš”ì•½
# ============================================================================

print_header("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

print("ğŸ“¦ ìƒì„±ëœ íŒŒì¼:")
print(f"  1. ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")

if compressed_path is not None:
    print(f"  2. ì••ì¶• íŒŒì¼: {compressed_path}")
    print(f"  3. MD5 ì²´í¬ì„¬: {compressed_path.parent / f'{compressed_path.name}.md5'}")
else:
    print("  2. ì••ì¶• íŒŒì¼: (ìŠ¤í‚µë¨)")
    print("  3. MD5 ì²´í¬ì„¬: (ìŠ¤í‚µë¨)")
print()

print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
if compressed_path is not None:
    compressed_filename = f"{compressed_path.name}"
    print("  1. ì••ì¶• íŒŒì¼ì„ ìš´ì˜ ì„œë²„ë¡œ ì „ì†¡:")
    print(f"     scp {compressed_path} deploy-user@server:/tmp/")
    print()
    print("  2. ìš´ì˜ ì„œë²„ì—ì„œ ì••ì¶• í•´ì œ:")
    print(f"     cd /path/to/deployment")
    print(f"     tar -xzf {compressed_filename}")
else:
    print("  1. ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì§ì ‘ ì „ì†¡:")
    print(f"     scp -r {model_specific_dir} deploy-user@server:/path/to/models/")
    print()

print("  3. Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸:")
print(f"     docker run -v /path/to/models:/app/models stt-engine:cuda129-v1.2")
print()

print("âœ¨ ëª¨ë“  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

# ============================================================================
# Step 8: ëª¨ë¸ ê²€ì¦ (ì¡°ê±´ë¶€)
# ============================================================================

if not should_test:
    print()
    print("=" * 60)
    print("ğŸ” ëª¨ë¸ ê²€ì¦ (ìŠ¤í‚µë¨ - --skip-test ì˜µì…˜ ì‚¬ìš©)")
    print("=" * 60)
    print()
    print("â­ï¸  ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
    print()
    print("ğŸ’¡ ë‚˜ì¤‘ì— ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print("   python download_model_hf.py")
    print()
    print("ë˜ëŠ” Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸:")
    print("   docker build -t stt-engine:latest -f docker/Dockerfile.engine.rhel89 .")
    print("   docker run -it -p 8003:8003 -v $(pwd)/models:/app/models stt-engine:latest")
    print()
else:
    print()
    print("=" * 60)
    print("ğŸ” ëª¨ë¸ ê²€ì¦ (faster-whisper ë¡œë“œ í…ŒìŠ¤íŠ¸)")
    print("=" * 60)
    print()

    if not check_and_install_faster_whisper():
        print_error("faster-whisper ì„¤ì¹˜ í•„ìˆ˜")

    try:
        from faster_whisper import WhisperModel
        import numpy as np
        
        ct2_model_dir = model_specific_dir / "ctranslate2_model"
        model_bin_path = ct2_model_dir / "model.bin"
        
        print("ğŸ“ ëª¨ë¸ êµ¬ì¡° í™•ì¸:")
        print(f"   ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_specific_dir}")
        print(f"   CTranslate2 ê²½ë¡œ: {ct2_model_dir}")
        print(f"   model.bin ìœ„ì¹˜: {model_bin_path}")
        print()
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„¸ í™•ì¸
        if model_specific_dir.exists():
            print(f"   ğŸ“‚ {model_specific_dir.name}/ ë‚´ìš©:")
            for item in sorted(model_specific_dir.iterdir()):
                if item.is_file():
                    size_mb = item.stat().st_size / (1024**2)
                    print(f"      - {item.name} ({size_mb:.2f}MB)")
                elif item.is_dir():
                    file_count = len(list(item.glob("*")))
                    print(f"      ğŸ“ {item.name}/ ({file_count} items)")
                    if item.name == "ctranslate2_model":
                        for sub in sorted(item.glob("*"))[:5]:
                            if sub.is_file():
                                size_mb = sub.stat().st_size / (1024**2)
                                print(f"         - {sub.name} ({size_mb:.2f}MB)")
        print()
        
        if not model_bin_path.exists():
            print("âŒ CTranslate2 ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print()
            
            # ëŒ€ì²´ ê²½ë¡œ í™•ì¸
            alt_bins = list(ct2_model_dir.glob("*.bin")) if ct2_model_dir.exists() else []
            if alt_bins:
                print(f"âš ï¸  {len(alt_bins)}ê°œì˜ .bin íŒŒì¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
                for alt_bin in alt_bins:
                    size_mb = alt_bin.stat().st_size / (1024**2)
                    print(f"   - {alt_bin.name} ({size_mb:.2f}MB)")
                print()
                print("ğŸ’¡ first-whisperëŠ” model.binì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.")
                print("   model.bin ì‹¬ë§í¬/ë³µì‚¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                
                # ìë™ìœ¼ë¡œ model.bin ìƒì„±
                try:
                    first_bin = sorted(alt_bins)[0]
                    shutil.copy2(first_bin, model_bin_path)
                    print(f"âœ… model.bin ìƒì„± ì™„ë£Œ: {first_bin.name} â†’ model.bin")
                except Exception as copy_e:
                    print(f"âŒ model.bin ìƒì„± ì‹¤íŒ¨: {copy_e}")
                    raise
            else:
                print_warn("ë³€í™˜ì„ ìŠ¤í‚µí–ˆê±°ë‚˜ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                print()
                print("ğŸ’¡ ì˜µì…˜ ì—†ì´ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ë³€í™˜í•˜ì„¸ìš”:")
                print("   python download_model_hf.py")
                print()
                raise RuntimeError("CTranslate2 ëª¨ë¸ ë³€í™˜ í•„ìš”")
        else:
            print_success("âœ… CTranslate2 ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
            size_mb = model_bin_path.stat().st_size / (1024**2)
            print(f"   íŒŒì¼ í¬ê¸°: {size_mb:.2f}MB")
            print()
            
        print("â³ faster-whisperë¡œ CTranslate2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
        print("   (ì´ ë‹¨ê³„ëŠ” 1-3ë¶„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print()
        
        try:
            model = WhisperModel(
                model_size_or_path=str(ct2_model_dir),
                device="cpu",
                compute_type="float32"
            )
            
            print_success("âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print()
            
            print("ğŸ“‹ ëª¨ë¸ ì •ë³´:")
            print(f"   âœ“ ëª¨ë¸ íƒ€ì…: Whisper Large-v3-Turbo")
            print(f"   âœ“ í˜•ì‹: CTranslate2 ë°”ì´ë„ˆë¦¬ (model.bin)")
            print(f"   âœ“ ë””ë°”ì´ìŠ¤: CPU")
            print(f"   âœ“ Compute Type: FP32")
            print()
            
            # ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            print("â³ ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘...")
            sample_audio_dir = BASE_DIR / "audio" / "samples"
            
            # ë””ë²„ê·¸: ê²½ë¡œ ì •ë³´ ì¶œë ¥
            print(f"   ìƒ˜í”Œ ê²½ë¡œ: {sample_audio_dir}")
            print(f"   ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {sample_audio_dir.exists()}")
            
            if sample_audio_dir.exists():
                print(f"   ë””ë ‰í† ë¦¬ ë‚´ìš©: {list(sample_audio_dir.glob('*.wav'))}")
            
            test_files = [
                ("short_0.5s.wav", "ì§§ì€ ì˜¤ë””ì˜¤ (0.5ì´ˆ)"),
                ("medium_3s.wav", "ì¤‘ê°„ ì˜¤ë””ì˜¤ (3ì´ˆ)"),
                ("long_10s.wav", "ê¸´ ì˜¤ë””ì˜¤ (10ì´ˆ)"),
            ]
            
            test_passed = False
            for audio_file, label in test_files:
                audio_path = sample_audio_dir / audio_file
                
                if audio_path.exists():
                    try:
                        # íŒŒì¼ í¬ê¸° í™•ì¸
                        file_size = audio_path.stat().st_size
                        print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {label} ({file_size} bytes)...")
                        
                        segments, info = model.transcribe(str(audio_path), language="ko")
                        list(segments)  # consume generator
                        print(f"   âœ“ {label} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                        test_passed = True
                    except Exception as e:
                        error_msg = str(e)
                        # íŠ¹ì • ì—ëŸ¬ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰ (mel-spectrogram í˜¸í™˜ì„± ë¬¸ì œ)
                        if "Invalid input features shape" in error_msg or "shape" in error_msg.lower():
                            print(f"   âš ï¸  {label} mel-spectrogram í˜•ì‹ ë¶ˆì¼ì¹˜ (ë¬´ì‹œ)")
                            test_passed = True  # ì´ ê²½ìš°ì—ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ëª¨ë¸ ìì²´ëŠ” ì •ìƒ)
                        else:
                            print(f"   âš ï¸  {label} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {error_msg[:80]}")
                else:
                    print(f"   âš ï¸  {label} ìƒ˜í”Œ íŒŒì¼ ì—†ìŒ: {audio_path}")
                    if sample_audio_dir.exists():
                        print(f"      {sample_audio_dir}ì˜ íŒŒì¼ ëª©ë¡: {list(sample_audio_dir.glob('*'))}")
            
            if test_passed:
                print()
                print("="*60)
                print("âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
                print("="*60)
                print()
                print("ğŸ‰ faster-whisperë¡œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
                print("   CTranslate2 ë³€í™˜ëœ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print()
            else:
                print()
                print("âš ï¸  ìƒ˜í”Œ ì˜¤ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                print("   ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ì„¸ìš”:")
                print("   python generate_sample_audio.py")
                print()
            
        except (MemoryError, OSError) as e:
            print_warn("ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
            print("í•„ìš” ë©”ëª¨ë¦¬: 16GB ì´ìƒ")
            print("ëª¨ë¸ì€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print()
            print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            print("   â€¢ EC2 ì¸ìŠ¤í„´ìŠ¤ ì—…ê·¸ë ˆì´ë“œ: t3.large â†’ t3.xlarge (16GB)")
            print("   â€¢ ë˜ëŠ” ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ì¶”ê°€: sudo fallocate -l 8G /swapfile")
            print()
            print("ğŸ’¡ Dockerì—ì„œ í…ŒìŠ¤íŠ¸:")
            print("   docker run -it -p 8003:8003 -v $(pwd)/models:/app/models stt-engine:latest")
            print()
            
        except Exception as e:
            print_warn(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"{str(e)[:200]}")
            print()
            print("ğŸ’¡ Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:")
            print("   docker run -it -p 8003:8003 -v $(pwd)/models:/app/models stt-engine:latest")
            print()
                
    except ImportError:
        print_warn("faster-whisperê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("ì„¤ì¹˜: pip install faster-whisper")
        print()
