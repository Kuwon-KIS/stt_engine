# STT Engine - Whisper ëª¨ë¸ ì¤€ë¹„ ê°€ì´ë“œ

## ğŸ“‹ ëª©í‘œ

- ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬ (ì†ìƒëœ íŒŒì¼ ì œê±°)
- Huggingfaceì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- CTranslate2 í¬ë§· ë³€í™˜ (faster-whisper í˜¸í™˜)
- ì••ì¶•í•˜ì—¬ ìš´ì˜ ì„œë²„ë¡œ ì „ì†¡

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì˜µì…˜ 1: Python ê°„ë‹¨í•œ ë²„ì „ (ê¶Œì¥)

```bash
# Macì—ì„œ ì‹¤í–‰
cd /Users/a113211/workspace/stt_engine
python3 prepare_model_simple.py

# ë˜ëŠ” ë³€í™˜ ê±´ë„ˆë›°ê¸° (PyTorch í¬ë§·ë§Œ)
python3 prepare_model_simple.py --no-convert
```

**ì†Œìš” ì‹œê°„**: 5-10ë¶„
**ê²°ê³¼**: `models/whisper-large-v3-turbo-model.tar.gz`

---

### ì˜µì…˜ 2: Bash ìŠ¤í¬ë¦½íŠ¸

```bash
cd /Users/a113211/workspace/stt_engine
bash prepare_model.sh
```

**ì†Œìš” ì‹œê°„**: 5-10ë¶„
**ê²°ê³¼**: `models/whisper-large-v3-turbo-model.tar.gz`

---

### ì˜µì…˜ 3: Python ìƒì„¸ ë²„ì „

```bash
python3 prepare_model.py
```

---

## ğŸ“Š ê° ë‹¨ê³„ ì„¤ëª…

### Step 1: ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ (~30ì´ˆ)
- ê¸°ì¡´ `models/openai_whisper-large-v3-turbo/` ë°±ì—… ìƒì„±
- ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ
- ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±

**ê²°ê³¼**: ê¹¨ë—í•œ ìƒíƒœì—ì„œ ë‹¤ì‹œ ì‹œì‘

### Step 2: Huggingface ë‹¤ìš´ë¡œë“œ (2-5ë¶„)
- `openai/whisper-large-v3-turbo` ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- í•„ìˆ˜ íŒŒì¼:
  - `pytorch_model.bin` (1.3GB)
  - `config.json`
  - `preprocessor_config.json`
  - `tokenizer.json`
  - ê¸°íƒ€ ë©”íƒ€ë°ì´í„° íŒŒì¼

**ê²°ê³¼**: ëª¨ë“  ëª¨ë¸ íŒŒì¼ì´ ë¡œì»¬ì— ì €ì¥

### Step 3: CTranslate2 ë³€í™˜ (3-10ë¶„, ì„ íƒì‚¬í•­)
- PyTorch ëª¨ë¸ì„ CTranslate2 ë°”ì´ë„ˆë¦¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜
- ê²°ê³¼: `model.bin` ìƒì„±
- ìš©ë„: faster-whisper ìµœì í™” ë°±ì—”ë“œ

**ê²°ê³¼**: `model.bin` (1.5GB, CTranslate2 í¬ë§·)

**ì°¸ê³ **:
- ë³€í™˜ì´ ì‹¤íŒ¨í•´ë„ `pytorch_model.bin`ì´ ìˆìœ¼ë©´ openai-whisper ì‚¬ìš© ê°€ëŠ¥
- `--no-convert` ì˜µì…˜ìœ¼ë¡œ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ

### Step 4: ê²€ì¦ (~10ì´ˆ)
- í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
- íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦

### Step 5: ì••ì¶• (1-3ë¶„)
- ëª¨ë“  ëª¨ë¸ íŒŒì¼ì„ `tar.gz`ìœ¼ë¡œ ì••ì¶•
- ê²°ê³¼: `whisper-large-v3-turbo-model.tar.gz` (~500MB)

---

## ğŸ“ ìµœì¢… êµ¬ì¡°

```
models/
â”œâ”€â”€ openai_whisper-large-v3-turbo/
â”‚   â”œâ”€â”€ pytorch_model.bin           (1.3GB)
â”‚   â”œâ”€â”€ model.bin                   (1.5GB, CTranslate2 - ì„ íƒì‚¬í•­)
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ preprocessor_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ whisper-large-v3-turbo-model.tar.gz  (500MB)
â””â”€â”€ .backup/
    â””â”€â”€ backup_20260205_1234567/    (ì´ì „ ë²„ì „ ë°±ì—…)
```

---

## ğŸš€ ìš´ì˜ ì„œë²„ë¡œ ë°°í¬

### 1ë‹¨ê³„: Macì—ì„œ ì „ì†¡
```bash
# ì¤€ë¹„ëœ tar íŒŒì¼ ë‹¤ìš´ë¡œë“œ í™•ì¸
ls -lh models/whisper-large-v3-turbo-model.tar.gz

# RHEL ì„œë²„ë¡œ ì „ì†¡
scp models/whisper-large-v3-turbo-model.tar.gz user@rhel_server:/tmp/
```

### 2ë‹¨ê³„: RHEL ì„œë²„ì—ì„œ ì„¤ì¹˜
```bash
# ì ‘ì†
ssh user@rhel_server

# ì••ì¶• í•´ì œ
cd /tmp
tar -xzf whisper-large-v3-turbo-model.tar.gz

# ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
mv openai_whisper-large-v3-turbo /path/to/stt_engine/models/

# í™•ì¸
ls -la /path/to/stt_engine/models/openai_whisper-large-v3-turbo/
```

### 3ë‹¨ê³„: Dockerì— ë§ˆìš´íŠ¸
```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ Docker ì»¨í…Œì´ë„ˆì— ë§ˆìš´íŠ¸
docker run -d \
  --name stt-api \
  -v /path/to/models/openai_whisper-large-v3-turbo:/app/models/openai_whisper-large-v3-turbo \
  -p 8003:8003 \
  stt-engine:cuda129-v1.2
```

---

## âœ… ê²€ì¦

### ë‹¤ìš´ë¡œë“œ í›„ í™•ì¸
```bash
# íŒŒì¼ ëª©ë¡ í™•ì¸
ls -la /path/to/models/openai_whisper-large-v3-turbo/

# íŒŒì¼ í¬ê¸° í™•ì¸
du -sh /path/to/models/openai_whisper-large-v3-turbo/

# í•„ìˆ˜ íŒŒì¼ í™•ì¸
ls -1 /path/to/models/openai_whisper-large-v3-turbo/ | grep -E "pytorch_model|config|tokenizer"
```

### Dockerì—ì„œ í…ŒìŠ¤íŠ¸
```bash
# ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
docker run --rm \
  -v /path/to/models/openai_whisper-large-v3-turbo:/app/models/openai_whisper-large-v3-turbo \
  stt-engine:cuda129-v1.2 \
  python3.11 -c "
import whisper
print('âœ… PyTorch ëª¨ë¸ ë¡œë“œ ì„±ê³µ')
"

# faster-whisper í…ŒìŠ¤íŠ¸
docker run --rm \
  -v /path/to/models/openai_whisper-large-v3-turbo:/app/models/openai_whisper-large-v3-turbo \
  stt-engine:cuda129-v1.2 \
  python3.11 -c "
from faster_whisper import WhisperModel
print('âœ… faster-whisper ë¡œë“œ ì„±ê³µ')
"
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### "Huggingface ë¡œê·¸ì¸ í•„ìš”" ì˜¤ë¥˜
```bash
# í† í° ì„¤ì •
huggingface-cli login
# ë˜ëŠ”
export HF_TOKEN="your_token_here"
```

### "ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨ë¨" ì˜¤ë¥˜
```bash
# ì¬ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì´ì–´ì„œ ë‹¤ìš´ë¡œë“œ
python3 prepare_model_simple.py
```

### "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±" ì˜¤ë¥˜
```bash
# í•„ìš” ê³µê°„: ~5GB (ë‹¤ìš´ë¡œë“œ ì¤‘), ~3GB (ìµœì¢…)
df -h

# ë°±ì—… ì •ë¦¬ (ì„ íƒ)
rm -rf models/.backup/*
```

### "CTranslate2 ë³€í™˜ ì‹¤íŒ¨" ì˜¤ë¥˜
```bash
# ë³€í™˜ ê±´ë„ˆë›°ê³  PyTorchë§Œ ì‚¬ìš©
python3 prepare_model_simple.py --no-convert

# ë˜ëŠ” ë‚˜ì¤‘ì— ìˆ˜ë™ ë³€í™˜
cd models/openai_whisper-large-v3-turbo
ct2-transformers-converter --model_name_or_path . --output_dir . --quantization float32
```

---

## ğŸ“Š ì˜ˆìƒ ì‹œê°„ ë° í¬ê¸°

| í•­ëª© | ì†Œìš” ì‹œê°„ | í¬ê¸° |
|------|---------|------|
| ì •ë¦¬ | 30ì´ˆ | - |
| ë‹¤ìš´ë¡œë“œ | 2-5ë¶„ | 1.3GB |
| ë³€í™˜ | 3-10ë¶„ | +1.5GB |
| ê²€ì¦ | 10ì´ˆ | - |
| ì••ì¶• | 1-3ë¶„ | 500MB |
| **ì´í•©** | **7-20ë¶„** | **5GB (ì„ì‹œ) â†’ 500MB (ìµœì¢…)** |

---

## ğŸ’¡ íŒ

### ë„¤íŠ¸ì›Œí¬ê°€ ëŠë¦° ê²½ìš°
- ì•¼ê°„ì— ì‹¤í–‰í•˜ê¸°
- ë¡œì»¬ ì™€ì´íŒŒì´ ì‚¬ìš©
- `--no-convert` ì˜µì…˜ìœ¼ë¡œ ë³€í™˜ ê±´ë„ˆë›°ê¸°

### ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•œ ê²½ìš°
```bash
# ì‘ì€ ëª¨ë¸ ë¨¼ì € ë‹¤ìš´ë¡œë“œ í›„ í™•ì¸
python3 prepare_model_simple.py --no-convert

# ë‚˜ì¤‘ì— í•„ìš”ì‹œ ë³€í™˜
cd models/openai_whisper-large-v3-turbo
python3 -c "from faster_whisper import WhisperModel; WhisperModel('.')"
```

### ë°±ì—… ë³µì›
```bash
# ì´ì „ ëª¨ë¸ë¡œ ë¡¤ë°±
rm -rf models/openai_whisper-large-v3-turbo
cp -r models/.backup/backup_20260205_123456 models/openai_whisper-large-v3-turbo
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ** âœ…
   ```bash
   python3 prepare_model_simple.py
   ```

2. **EC2ì—ì„œ Docker ì´ë¯¸ì§€ ë¹Œë“œ** (ë³„ë„)
   ```bash
   bash scripts/build-stt-engine-ec2.sh
   ```

3. **ëª¨ë¸ + ì´ë¯¸ì§€ í•¨ê»˜ RHEL ì„œë²„ë¡œ ë°°í¬**
   ```bash
   scp models/whisper-large-v3-turbo-model.tar.gz user@rhel_server:/tmp/
   scp build/output/stt-engine-cuda129-v1.2.tar.gz user@rhel_server:/tmp/
   ```

---

## ğŸ“ ì§€ì›

- ë¬¸ì œ ë°œìƒ ì‹œ: ë¡œê·¸ í™•ì¸ ë° ë‹¤ì‹œ ì‹¤í–‰
- `--help` ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš© ë°©ë²• í™•ì¸ (ì¼ë¶€)
- Python ìŠ¤í¬ë¦½íŠ¸ëŠ” ìì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ

