# ğŸ“Œ vLLM ì—°ë™ ì„¤ì • - í•µì‹¬ ìš”ì•½

ë‹¹ì‹ ì˜ ì§ˆë¬¸: "vllmì„ dockerë¡œ ë„ì› ëŠ”ë°, endpointëŠ” ì–´ë–»ê²Œ ì„¤ì •í•˜ê³ , ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´?"

---

## â­ í•µì‹¬ ë‹µë³€ (3ì¤„)

1. **Endpoint ì„¤ì •**: í™˜ê²½ë³€ìˆ˜ `VLLM_API_URL="http://localhost:8000"` ì„¤ì •
2. **ìë™ ì—°ê²°**: `.env` íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ì½ê³  vLLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—°ê²°
3. **ìŒì„± ì²˜ë¦¬**: HTTP POSTë¡œ `/transcribe-and-process` ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ

---

## ğŸ¯ ì„¸ ê°€ì§€ ìƒí™©ë³„ ì„¤ì •

### ìƒí™© 1: ë¡œì»¬ ê°œë°œ (macOS)
```bash
# 1ë‹¨ê³„: vLLM Docker ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# 2ë‹¨ê³„: STT Engineì—ì„œ endpoint ì„¤ì •
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# 3ë‹¨ê³„: ìŒì„± íŒŒì¼ ì²˜ë¦¬
python test_vllm_integration.py --test-vllm audio.mp3
```

---

### ìƒí™© 2: Docker Compose (STT + vLLM í•¨ê»˜)
```bash
# 1ë‹¨ê³„: ì‹œì‘ (ìë™ìœ¼ë¡œ ëª¨ë“  ì„¤ì • ì™„ë£Œ)
docker-compose -f docker-compose.vllm.yml up -d

# 2ë‹¨ê³„: ìŒì„± íŒŒì¼ ì²˜ë¦¬
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio.mp3" \
  -F "language=ko"

# ì¥ì : VLLM_API_URLì´ ìë™ìœ¼ë¡œ http://vllm:8000 ì„¤ì •ë¨
```

---

### ìƒí™© 3: ì›ê²© GPU ì„œë²„
```bash
# GPU ì„œë²„ì—ì„œ vLLM ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# ë¡œì»¬ì—ì„œ endpoint ì„¤ì •
export VLLM_API_URL="http://192.168.1.100:8000"
python api_server.py
```

---

## ğŸ”Œ Endpoint ì„¤ì • 3ê°€ì§€ ë°©ë²•

### ë°©ë²• 1: í™˜ê²½ ë³€ìˆ˜ (ê°„ë‹¨)
```bash
export VLLM_API_URL="http://localhost:8000"
python api_server.py
```

### ë°©ë²• 2: .env íŒŒì¼ (ê¶Œì¥)
```env
VLLM_API_URL="http://localhost:8000"
```

### ë°©ë²• 3: Docker Compose (ìë™)
```yaml
environment:
  - VLLM_API_URL=http://vllm:8000
```

---

## ğŸ™ï¸ ìŒì„± íŒŒì¼ ì²˜ë¦¬ ë°©ì‹

### API í˜¸ì¶œ (ê°€ì¥ ê°„ë‹¨)

**Curl ì˜ˆì œ**:
```bash
# ìŒì„± â†’ í…ìŠ¤íŠ¸ â†’ vLLM ì²˜ë¦¬ (í•œ ë²ˆì—)
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio.mp3" \
  -F "language=ko" \
  -F "instruction=ì´ ìŒì„±ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:"
```

**ì‘ë‹µ**:
```json
{
  "success": true,
  "stt_result": {
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤."
  },
  "vllm_result": {
    "summary": "ì‚¬ìš©ìì˜ ì¸ì‚¬ë§"
  }
}
```

---

### Python ì˜ˆì œ

```python
import requests

with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "language": "ko",
        "instruction": "ê°ì • ë¶„ì„"
    }
    
    response = requests.post(
        "http://localhost:8001/transcribe-and-process",
        files=files,
        data=data
    )
    
    result = response.json()
    print(f"ìŒì„± ì¸ì‹: {result['stt_result']['text']}")
    print(f"vLLM ê²°ê³¼: {result['vllm_result']}")
```

---

## ğŸ“Š ì²˜ë¦¬ íë¦„

```
ìŒì„± íŒŒì¼ (audio.mp3)
       â†“
STT Engine (Port 8001)
  â”œâ”€ Whisper ëª¨ë¸ë¡œ ìŒì„± ì¸ì‹
  â”œâ”€ ê²°ê³¼: "ì•ˆë…•í•˜ì„¸ìš”..."
       â†“
vLLM Server (Port 8000) â† VLLM_API_URLë¡œ ìë™ ì—°ê²°
  â”œâ”€ LLM ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
  â”œâ”€ ê²°ê³¼: "ìš”ì•½/ë¶„ì„/ê°ì •ë¶„ì„..."
       â†“
ìµœì¢… ì‘ë‹µ (JSON)
```

---

## ğŸ“ ìƒì„±ëœ íŒŒì¼

| íŒŒì¼ | ëª©ì  |
|------|------|
| **VLLM_ANSWER.md** | â† ë‹¹ì‹ ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ |
| **VLLM_QUICKSTART.md** | 5ë¶„ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ |
| **VLLM_SETUP.md** | ì™„ë²½í•œ ì„¤ì • ë§¤ë‰´ì–¼ |
| **docker-compose.vllm.yml** | STT + vLLM Docker Compose ì„¤ì • |
| **test_vllm_integration.py** | í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ |
| **.env** | í™˜ê²½ ë³€ìˆ˜ ì„¤ì • |

---

## âš¡ ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰ (30ì´ˆ)

```bash
# 1. Docker Composeë¡œ ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d

# 2. 30ì´ˆ ëŒ€ê¸°
sleep 30

# 3. í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio_samples/test.mp3" \
  -F "language=ko"

# ë! ğŸ‰
```

---

## ğŸ” endpoint í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] vLLM Docker ì‹¤í–‰ ì¤‘? â†’ `curl http://localhost:8000/health`
- [ ] STT Engine ì‹¤í–‰ ì¤‘? â†’ `curl http://localhost:8001/health`
- [ ] VLLM_API_URL ì„¤ì •ë¨? â†’ `echo $VLLM_API_URL`
- [ ] ìŒì„± íŒŒì¼ ì¡´ì¬? â†’ `ls audio_samples/`
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ? â†’ `python test_vllm_integration.py --test-vllm`

---

## ğŸ’¡ ì£¼ìš” í¬ì¸íŠ¸

1. **Port 8000**: vLLM ì„œë²„
2. **Port 8001**: STT Engine
3. **VLLM_API_URL**: ì—°ê²° í†µë¡œ (ë¡œì»¬: localhost, Docker: vllm, ì›ê²©: IP)
4. **ìë™ ì—°ê²°**: í™˜ê²½ë³€ìˆ˜ë§Œ ì„¤ì •í•˜ë©´ ëª¨ë“  í†µì‹  ìë™
5. **ìŒì„± ì²˜ë¦¬**: HTTP POSTë¡œ ìŒì„± íŒŒì¼ ì „ì†¡

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. âœ… VLLM_QUICKSTART.md ë”°ë¼ì„œ ë¡œì»¬ í…ŒìŠ¤íŠ¸
2. âœ… docker-compose.vllm.ymlë¡œ Docker Compose í…ŒìŠ¤íŠ¸
3. âœ… test_vllm_integration.pyë¡œ í†µí•© ê²€ì¦
4. âœ… GPU ì„œë²„ë¡œ ë°°í¬

**ëª¨ë“  ìƒì„¸ ë‚´ìš©ì€ VLLM_ANSWER.mdë¥¼ ì°¸ê³ í•˜ì„¸ìš”!**
