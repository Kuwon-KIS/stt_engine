# vLLM ì—°ë™ ë¹ ë¥¸ ì‹œì‘ (5ë¶„)

## ğŸš€ 30ì´ˆ ìš”ì•½

```bash
# 1. vLLM ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# 2. STT Engine ì‹¤í–‰ (ë‹¤ë¥¸ í„°ë¯¸ë„)
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# 3. í…ŒìŠ¤íŠ¸
python test_vllm_integration.py --test-vllm audio_samples/test.mp3
```

---

## ğŸ› ï¸ ë¡œì»¬ í™˜ê²½ ì„¤ì • (macOS)

### 1ë‹¨ê³„: vLLM Docker ì´ë¯¸ì§€ ì¤€ë¹„
```bash
# GPUê°€ ì—†ëŠ” ê²½ìš° (CPU)
docker run -p 8000:8000 vllm/vllm-openai:latest \
  --model mistralai/Mistral-7B-v0.1

# GPUê°€ ìˆëŠ” ê²½ìš° (ê¶Œì¥)
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf --dtype float16
```

âœ… í™•ì¸: `curl http://localhost:8000/health`

### 2ë‹¨ê³„: STT Engine ì„¤ì •
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env

# í•„ìš”ì‹œ ìˆ˜ì •
vim .env
# VLLM_API_URL="http://localhost:8000"
```

### 3ë‹¨ê³„: STT Engine ì‹¤í–‰
```bash
# í„°ë¯¸ë„ 2ì—ì„œ
python api_server.py
```

âœ… í™•ì¸: `curl http://localhost:8001/health`

### 4ë‹¨ê³„: í†µí•© í…ŒìŠ¤íŠ¸
```bash
# í„°ë¯¸ë„ 3ì—ì„œ
python test_vllm_integration.py --check-health

# STTë§Œ í…ŒìŠ¤íŠ¸
python test_vllm_integration.py --test-stt audio_samples/test.mp3

# STT + vLLM í…ŒìŠ¤íŠ¸
python test_vllm_integration.py --test-vllm audio_samples/test.mp3 \
  --instruction "ì´ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:"
```

---

## ğŸ³ Docker Composeë¡œ í•œ ë²ˆì— (ê¶Œì¥)

### 1ë‹¨ê³„: ì„¤ì • íŒŒì¼ í™•ì¸
```bash
# docker-compose.vllm.yml íŒŒì¼ í™•ì¸
ls -la docker-compose.vllm.yml
```

### 2ë‹¨ê³„: ëª¨ë¸ ìºì‹œ ì¤€ë¹„ (ì„ íƒ)
```bash
# Hugging Face ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­)
huggingface-cli download meta-llama/Llama-2-7b-hf
```

### 3ë‹¨ê³„: ì‹œì‘
```bash
# STT + vLLM í•¨ê»˜ ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d

# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.vllm.yml logs -f

# ì •ìƒ ì‹œì‘ í™•ì¸ (30-40ì´ˆ ëŒ€ê¸°)
sleep 40
curl http://localhost:8000/health  # vLLM
curl http://localhost:8001/health  # STT
```

### 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í…ŒìŠ¤íŠ¸
docker-compose -f docker-compose.vllm.yml exec whisper-api \
  python test_vllm_integration.py --check-health

# ë˜ëŠ” í˜¸ìŠ¤íŠ¸ì—ì„œ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8001/transcribe" \
  -F "file=@audio_samples/test.mp3" \
  -F "language=ko"
```

### 5ë‹¨ê³„: ì¤‘ì§€
```bash
docker-compose -f docker-compose.vllm.yml down

# ì»¨í…Œì´ë„ˆ ì‚­ì œ
docker-compose -f docker-compose.vllm.yml down -v
```

---

## ğŸ”Œ Endpoint ì„¤ì • (í™˜ê²½ë³„)

### ë¡œì»¬ ê°œë°œ (macOS)
```env
VLLM_API_URL="http://localhost:8000"
```

```bash
# vLLM ì‹¤í–‰
docker run -p 8000:8000 vllm/vllm-openai:latest --model mistralai/Mistral-7B-v0.1
```

### Docker Compose (ë¡œì»¬)
```env
VLLM_API_URL="http://vllm:8000"
```

```bash
# ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d
```

### ì›ê²© GPU ì„œë²„
```env
VLLM_API_URL="http://192.168.1.100:8000"
```

```bash
# ì„œë²„ì—ì„œ vLLM ì‹¤í–‰
ssh gpu-server
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf
```

---

## ğŸ“ ìŒì„± íŒŒì¼ ì²˜ë¦¬ ë°©ì‹

### API ì‚¬ìš© (ê¶Œì¥)

**1. í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ**
```bash
curl -X POST "http://localhost:8001/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=ko" | jq
```

**ì‘ë‹µ**:
```json
{
  "success": true,
  "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤.",
  "language": "ko",
  "duration": 5.2
}
```

**2. í…ìŠ¤íŠ¸ + vLLM ì²˜ë¦¬ (ê¶Œì¥)**
```bash
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio.mp3" \
  -F "language=ko" \
  -F "instruction=ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:" | jq
```

**ì‘ë‹µ**:
```json
{
  "success": true,
  "stt_result": {
    "success": true,
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤.",
    "language": "ko"
  },
  "vllm_result": {
    "summary": "ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ì¸ì‚¬ë§"
  }
}
```

### Python ì‚¬ìš©
```python
import requests

# STT + vLLM
with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "language": "ko",
        "instruction": "ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:"
    }
    response = requests.post(
        "http://localhost:8001/transcribe-and-process",
        files=files,
        data=data
    )
    result = response.json()
    print(f"ì¸ì‹: {result['stt_result']['text']}")
    print(f"ë¶„ì„: {result['vllm_result']}")
```

---

## âš™ï¸ vLLM ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | í’ˆì§ˆ | ì¶”ì²œ í™˜ê²½ |
|------|------|------|------|----------|
| Mistral 7B | 15GB | âš¡âš¡âš¡ | â­â­â­ | ë¡œì»¬ / ì„œë²„ |
| Llama 2 7B | 16GB | âš¡âš¡ | â­â­â­â­ | ì„œë²„ |
| Llama 2 13B | 26GB | âš¡ | â­â­â­â­â­ | ê³ ì„±ëŠ¥ ì„œë²„ |
| Phi 2 | 6GB | âš¡âš¡âš¡âš¡ | â­â­â­ | ë¡œì»¬ |

### ëª¨ë¸ ë³€ê²½ ë°©ë²•

```bash
# 1. .env íŒŒì¼ ìˆ˜ì •
VLLM_MODEL_NAME="mistralai/Mistral-7B-v0.1"

# 2. vLLM ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker-compose -f docker-compose.vllm.yml restart vllm
```

---

## ğŸ” ë¬¸ì œ í•´ê²°

### vLLM ì„œë²„ ì—°ê²° ë¶ˆê°€
```bash
# 1. vLLM ì‹¤í–‰ í™•ì¸
docker ps | grep vllm

# 2. í¬íŠ¸ í™•ì¸
lsof -i :8000

# 3. í—¬ìŠ¤ ì²´í¬
curl -v http://localhost:8000/health

# 4. ë¡œê·¸ í™•ì¸
docker logs vllm-server
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë°©ë²• 1: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model mistralai/Mistral-7B-v0.1

# ë°©ë²• 2: ë©”ëª¨ë¦¬ ì œí•œ ì¡°ì •
--gpu-memory-utilization 0.7  # ê¸°ë³¸ê°’ 0.9 â†’ 0.7ë¡œ ê°ì†Œ

# ë°©ë²• 3: í† í° ê¸¸ì´ ì œí•œ
--max-model-len 2048  # ê¸°ë³¸ê°’ 4096 â†’ 2048ë¡œ ê°ì†Œ
```

### Hugging Face ì¸ì¦ í•„ìš”
```bash
# í† í° ì„¤ì • (Llama 2ì˜ ê²½ìš°)
huggingface-cli login
# í† í° ì…ë ¥: hf_xxxxxxxxxxxxx

# ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜
export HF_TOKEN=hf_xxxxxxxxxxxxx
```

### Docker Compose ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ
```bash
# ì»¨í…Œì´ë„ˆ ê°„ ì—°ê²° í…ŒìŠ¤íŠ¸
docker-compose -f docker-compose.vllm.yml exec whisper-api \
  curl http://vllm:8000/health

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker network ls
docker network inspect stt_engine_stt_network
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### GPU ì„¤ì •
```yaml
# docker-compose.vllm.ymlì—ì„œ
command: >
  --model meta-llama/Llama-2-7b-hf
  --dtype float16
  --gpu-memory-utilization 0.9
  --max-num-seqs 256
```

### ë™ì‹œ ìš”ì²­ ì²˜ë¦¬
```python
# ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬
import asyncio
import aiohttp

async def process_multiple(audio_files):
    async with aiohttp.ClientSession() as session:
        tasks = [
            post_request(session, f)
            for f in audio_files
        ]
        return await asyncio.gather(*tasks)
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] vLLM Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
- [ ] `VLLM_API_URL` í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
- [ ] STT Engine ì‹¤í–‰ í™•ì¸
- [ ] vLLM í—¬ìŠ¤ ì²´í¬ (`curl http://localhost:8000/health`)
- [ ] STT í—¬ìŠ¤ ì²´í¬ (`curl http://localhost:8001/health`)
- [ ] í…ŒìŠ¤íŠ¸ ìŒì„± íŒŒì¼ ì¤€ë¹„
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (`test_vllm_integration.py`)
- [ ] ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (ì„ íƒ)

---

## ğŸ“š ê´€ë ¨ íŒŒì¼

- [VLLM_SETUP.md](VLLM_SETUP.md) - ìƒì„¸ ì„¤ì • ê°€ì´ë“œ
- [docker-compose.vllm.yml](docker-compose.vllm.yml) - Docker Compose ì„¤ì •
- [test_vllm_integration.py](test_vllm_integration.py) - í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- [.env](`.env`) - í™˜ê²½ ë³€ìˆ˜

---

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ë¡œì»¬ì—ì„œ ì„±ê³µì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
2. âœ… GPU ì„œë²„ë¡œ ëª¨ë¸ ì´ì „
3. âœ… ì›ê²© vLLMê³¼ ì—°ë™
4. âœ… í”„ë¡œë•ì…˜ ë°°í¬

VLLM_SETUP.mdì—ì„œ ë” ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”!
