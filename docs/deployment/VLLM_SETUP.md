# vLLM Docker ì—°ë™ ê°€ì´ë“œ

## ğŸ¯ ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ë¡œì»¬ (macOS) ë˜ëŠ” GPU ì„œë²„ (Linux)                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   STT Engine         â”‚          â”‚   vLLM Server        â”‚   â”‚
â”‚  â”‚ (Port 8001)          â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (Port 8000)          â”‚   â”‚
â”‚  â”‚                      â”‚          â”‚                      â”‚   â”‚
â”‚  â”‚ â€¢ Whisper Model      â”‚  ìŒì„±    â”‚ â€¢ Llama/Mistral      â”‚   â”‚
â”‚  â”‚ â€¢ FastAPI Server     â”‚  íŒŒì¼    â”‚ â€¢ OpenAI API í˜¸í™˜    â”‚   â”‚
â”‚  â”‚ â€¢ Audio Processing   â”‚ â”€â”€â”€â”€â”€â”€â”€â–º â”‚ â€¢ í…ìŠ¤íŠ¸ ì²˜ë¦¬/ìš”ì•½    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  í”Œë¡œìš°:                                                         â”‚
â”‚  ìŒì„±íŒŒì¼ â”€â”€(HTTP POST)â”€â”€â–º STT Engine â”€â”€(ìŒì„± ì²˜ë¦¬)â”€â”€â–º          â”‚
â”‚     â–²                                                            â”‚
â”‚     â”‚                                                   â–¼        â”‚
â”‚     â”‚                                          í…ìŠ¤íŠ¸ ì¶”ì¶œ       â”‚
â”‚     â”‚                                                   â–¼        â”‚
â”‚     â”‚â—„â”€(ê²°ê³¼)â”€â”€â”€â”€â”€â”€â”€â”€â”€(HTTP POST)â”€â”€(vLLM) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ vLLM Docker í™˜ê²½ ì„¤ì •

### 1.1 vLLM ì»¨í…Œì´ë„ˆ ì‹œì‘

#### Option A: ë‹¨ì¼ ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰
```bash
# Llama 2 ëª¨ë¸ ì‚¬ìš©
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16 \
  --max-model-len 4096

# ë˜ëŠ” Mistral ëª¨ë¸ ì‚¬ìš© (ë” ë¹ ë¦„)
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model mistralai/Mistral-7B-v0.1 \
  --dtype float16
```

#### Option B: docker-compose ì‚¬ìš©
```yaml
# docker-compose.yml ì¶”ê°€
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0  # GPU ì„ íƒ
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface  # ëª¨ë¸ ìºì‹œ
    command: >
      --model meta-llama/Llama-2-7b-hf
      --dtype float16
      --max-model-len 4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### 1.2 vLLM ì •ìƒ ì‹¤í–‰ í™•ì¸
```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# ì‘ë‹µ ì˜ˆì‹œ
# {"model_name":"meta-llama/Llama-2-7b-hf"}
```

---

## 2ï¸âƒ£ STT Engineì—ì„œ vLLM Endpoint ì„¤ì •

### 2.1 í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (`.env` íŒŒì¼)

```dotenv
# vLLM ì„¤ì •
VLLM_API_URL="http://localhost:8000"      # ë¡œì»¬ í…ŒìŠ¤íŠ¸
# VLLM_API_URL="http://vllm:8000"         # Docker Compose ë„¤íŠ¸ì›Œí¬
# VLLM_API_URL="http://gpu-server:8000"   # ì›ê²© GPU ì„œë²„

VLLM_MODEL_NAME="meta-llama/Llama-2-7b-hf"
VLLM_TIMEOUT=60
```

### 2.2 Docker í™˜ê²½ì—ì„œì˜ Endpoint ì„¤ì •

#### ê²½ìš° 1: ë¡œì»¬ í…ŒìŠ¤íŠ¸ (macOS)
```bash
# í„°ë¯¸ë„ 1: vLLM ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# í„°ë¯¸ë„ 2: STT Engine ì‹¤í–‰
export VLLM_API_URL="http://localhost:8000"
python api_server.py
```

#### ê²½ìš° 2: Docker Compose (STT + vLLM í•¨ê»˜)
```yaml
version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model meta-llama/Llama-2-7b-hf
      --dtype float16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  whisper-api:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    ports:
      - "8001:8001"
    environment:
      - VLLM_API_URL=http://vllm:8000  # â† Docker ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
      - WHISPER_DEVICE=cuda
    volumes:
      - ./models:/app/models
      - ./audio_samples:/app/audio_samples
    depends_on:
      - vllm
    command: python api_server.py
```

#### ê²½ìš° 3: ì›ê²© GPU ì„œë²„
```bash
# GPU ì„œë²„ì—ì„œ vLLM ì‹¤í–‰ (ì˜ˆ: 192.168.1.100)
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# ë¡œì»¬ì—ì„œ STT Engine ì„¤ì •
export VLLM_API_URL="http://192.168.1.100:8000"
python api_server.py
```

---

## 3ï¸âƒ£ ìŒì„± íŒŒì¼ ì²˜ë¦¬ í”Œë¡œìš°

### 3.1 API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©

#### ì˜µì…˜ A: STTë§Œ ì²˜ë¦¬
```bash
# ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œë§Œ ë³€í™˜
curl -X POST "http://localhost:8001/transcribe" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@audio.mp3" \
  -F "language=ko"

# ì‘ë‹µ
{
  "success": true,
  "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤.",
  "language": "ko",
  "duration": 5.2
}
```

#### ì˜µì…˜ B: STT + vLLM ì²˜ë¦¬ (ê¶Œì¥)
```bash
# ìŒì„± â†’ í…ìŠ¤íŠ¸ â†’ vLLM ì²˜ë¦¬
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@audio.mp3" \
  -F "language=ko" \
  -F "instruction=ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:"

# ì‘ë‹µ
{
  "success": true,
  "stt_result": {
    "success": true,
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤.",
    "language": "ko"
  },
  "vllm_result": {
    "summary": "ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ ìŒì„±ìœ¼ë¡œ ì¸ì‚¬ë¥¼ í•©ë‹ˆë‹¤."
  }
}
```

### 3.2 Python í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

#### ê¸°ë³¸ í…ŒìŠ¤íŠ¸
```python
import requests

# í—¬ìŠ¤ ì²´í¬
response = requests.get("http://localhost:8001/health")
print(response.json())
# {'status': 'healthy', 'device': 'cuda', 'models_loaded': True}

# STTë§Œ ì²˜ë¦¬
with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {"language": "ko"}
    response = requests.post(
        "http://localhost:8001/transcribe",
        files=files,
        data=data
    )
    print(response.json())

# STT + vLLM ì²˜ë¦¬
with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "language": "ko",
        "instruction": "ì´ ë¬¸ì¥ì˜ í•µì‹¬ì„ 3ë‹¨ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:"
    }
    response = requests.post(
        "http://localhost:8001/transcribe-and-process",
        files=files,
        data=data
    )
    result = response.json()
    print(f"ìŒì„± ì¸ì‹: {result['stt_result']['text']}")
    print(f"vLLM ì²˜ë¦¬: {result['vllm_result']}")
```

#### ë°°ì¹˜ ì²˜ë¦¬
```python
import os
from pathlib import Path
import requests

# audio_samples í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
audio_dir = Path("audio_samples")
for audio_file in audio_dir.glob("*.mp3"):
    with open(audio_file, "rb") as f:
        files = {"file": f}
        data = {
            "language": "ko",
            "instruction": "ì´ ìŒì„±ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:"
        }
        response = requests.post(
            "http://localhost:8001/transcribe-and-process",
            files=files,
            data=data
        )
        result = response.json()
        
        print(f"\níŒŒì¼: {audio_file.name}")
        print(f"ì¸ì‹: {result['stt_result']['text']}")
        print(f"ë¶„ì„: {result['vllm_result']}")
```

---

## 4ï¸âƒ£ ì½”ë“œì—ì„œ Endpoint êµ¬ì„± ë°©ì‹

### 4.1 `vllm_client.py` - í˜„ì¬ êµ¬ì¡°

```python
class VLLMConfig(BaseModel):
    """vLLM ì„œë²„ ì„¤ì •"""
    api_url: str = os.getenv("VLLM_API_URL", "http://localhost:8000")
    model_name: str = os.getenv("VLLM_MODEL_NAME", "meta-llama/Llama-2-7b-hf")
    timeout: int = 60
    max_tokens: int = 512

class VLLMClient:
    def __init__(self, config: VLLMConfig):
        self.config = config
        self.completion_endpoint = f"{config.api_url}/v1/completions"
        # â†‘ ìë™ìœ¼ë¡œ êµ¬ì„±ë¨: "http://localhost:8000/v1/completions"
```

### 4.2 `api_server.py` - ì„œë²„ ì´ˆê¸°í™”

```python
# ìë™ ë¡œë“œ
vllm_client = VLLMClient(VLLMConfig())  # .env íŒŒì¼ì—ì„œ ìë™ ì½ìŒ

# ë˜ëŠ” ëª…ì‹œì  ì„¤ì •
from vllm_client import VLLMConfig, VLLMClient

config = VLLMConfig(
    api_url="http://gpu-server:8000",
    model_name="mistralai/Mistral-7B-v0.1"
)
vllm_client = VLLMClient(config)
```

---

## 5ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ê°€ì´ë“œ

### 5.1 ë¡œì»¬ ê°œë°œ (macOS)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STT Port â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚vLLM Port â”‚
â”‚  8001    â”‚         â”‚  8000    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 localhost           localhost
```

**ì„¤ì •**:
```bash
export VLLM_API_URL="http://localhost:8000"
```

### 5.2 Docker Compose (ë¡œì»¬)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Docker Network (vllm_net)       â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STT        â”‚      â”‚ vLLM       â”‚ â”‚
â”‚  â”‚ Port 8001  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ Port 8000  â”‚ â”‚
â”‚  â”‚ Host: stt  â”‚      â”‚ Host: vllm â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„¤ì •**:
```yaml
environment:
  - VLLM_API_URL=http://vllm:8000  # Docker DNS ì‚¬ìš©
```

### 5.3 ì›ê²© GPU ì„œë²„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë¡œì»¬ (macOS) â”‚                â”‚GPU ì„œë²„(Linux)â”‚
â”‚              â”‚                â”‚              â”‚
â”‚STT 8001 â”€â”€â”€â”€â”€â”€â”€â”€(HTTP)â”€â”€â”€â”€â”€â”€â”€â–º vLLM 8000    â”‚
â”‚              â”‚   192.168.1.100â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„¤ì •**:
```bash
export VLLM_API_URL="http://192.168.1.100:8000"
```

**GPU ì„œë²„ì—ì„œ vLLM ì‹¤í–‰**:
```bash
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf
```

---

## 6ï¸âƒ£ ë¬¸ì œ í•´ê²°

### Issue 1: "vLLM ì„œë²„ ì—°ê²° ë¶ˆê°€"
```bash
# 1ë‹¨ê³„: vLLM ì‹¤í–‰ í™•ì¸
docker ps | grep vllm

# 2ë‹¨ê³„: í¬íŠ¸ í™•ì¸
lsof -i :8000

# 3ë‹¨ê³„: í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# 4ë‹¨ê³„: ë°©í™”ë²½ í™•ì¸ (ì›ê²© ì„œë²„ì˜ ê²½ìš°)
telnet 192.168.1.100 8000
```

### Issue 2: Docker Composeì—ì„œ ì—°ê²° ë¶ˆê°€
```bash
# ì›ì¸: ë„¤íŠ¸ì›Œí¬ ì´ë¦„ í™•ì¸
docker network ls

# STT ì»¨í…Œì´ë„ˆì—ì„œ vLLM ì ‘ê·¼ í…ŒìŠ¤íŠ¸
docker exec whisper-api curl http://vllm:8000/health

# ë˜ëŠ” IP ì§ì ‘ ì‚¬ìš©
docker inspect vllm | grep IPAddress
```

### Issue 3: ëª¨ë¸ ë¡œë“œ ì—ëŸ¬
```bash
# 1ë‹¨ê³„: ëª¨ë¸ ìºì‹œ í™•ì¸
ls ~/.cache/huggingface/

# 2ë‹¨ê³„: ë³¼ë¥¨ ë§ˆìš´íŠ¸ í™•ì¸
docker inspect vllm | grep Mounts

# 3ë‹¨ê³„: í† í° ì„¤ì • (Llama 2ì˜ ê²½ìš°)
huggingface-cli login
```

---

## 7ï¸âƒ£ ì„±ëŠ¥ ìµœì í™”

### 7.1 vLLM ì„¤ì • ìµœì í™”
```bash
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16 \                          # ë©”ëª¨ë¦¬ ì ˆê°
  --max-model-len 4096 \                     # ìµœëŒ€ í† í° ê¸¸ì´
  --gpu-memory-utilization 0.9 \             # GPU í™œìš©ë¥  ë†’ì„
  --tensor-parallel-size 1 \                 # GPU ë‹¨ì¼ ì‚¬ìš©
  --max-num-seqs 256                         # ë°°ì¹˜ í¬ê¸°
```

### 7.2 STT Engine ì„¤ì • ìµœì í™”
```python
# api_server.py
import torch

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()

# ëª¨ë¸ ë¡œë“œ ì‹œ ë©”ëª¨ë¦¬ ë§µ
model_kwargs = {
    "torch_dtype": torch.float16,  # ë©”ëª¨ë¦¬ ì ˆê°
    "device_map": "auto"           # ìë™ í• ë‹¹
}
```

### 7.3 Docker Compose ë¦¬ì†ŒìŠ¤ ì„¤ì •
```yaml
services:
  vllm:
    # ... ê¸°ì¡´ ì„¤ì • ...
    deploy:
      resources:
        limits:
          cpus: '4'              # CPU ì œí•œ
          memory: 32G            # ë©”ëª¨ë¦¬ ì œí•œ
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']   # GPU 0ë²ˆë§Œ ì‚¬ìš©
              count: 1
              capabilities: [gpu]
```

---

## 8ï¸âƒ£ ì‹¤í–‰ ì˜ˆì‹œ

### ë¡œì»¬ í…ŒìŠ¤íŠ¸ (ê¶Œì¥ ìˆœì„œ)

```bash
# 1ë‹¨ê³„: vLLM ì‹œì‘ (í„°ë¯¸ë„ 1)
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# 2ë‹¨ê³„: STT Engine ì‹œì‘ (í„°ë¯¸ë„ 2)
cd /Users/a113211/workspace/stt_engine
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ (í„°ë¯¸ë„ 3)
python api_client.py --health
python api_client.py --process audio_samples/test.mp3
```

### Docker Composeë¡œ í•œ ë²ˆì— (ê¶Œì¥)

```bash
# 1ë‹¨ê³„: docker-compose.yml ì—…ë°ì´íŠ¸ (ìœ„ ì°¸ê³ )

# 2ë‹¨ê³„: ì‹œì‘
docker-compose up -d

# 3ë‹¨ê³„: í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/health

# 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8001/transcribe" \
  -F "file=@audio_samples/test.mp3" \
  -F "language=ko"

# 5ë‹¨ê³„: ì¤‘ì§€
docker-compose down
```

---

## ìš”ì•½

| í•­ëª© | ê°’ |
|------|-----|
| **STT Engine Port** | 8001 |
| **vLLM Port** | 8000 |
| **Endpoint í˜•ì‹** | `http://[host]:[port]/v1/completions` |
| **í™˜ê²½ë³€ìˆ˜** | `VLLM_API_URL` |
| **ë¡œì»¬ (localhost)** | `http://localhost:8000` |
| **Docker Compose** | `http://vllm:8000` |
| **ì›ê²© GPU ì„œë²„** | `http://[IP]:8000` |

âœ… ì´ì œ ìŒì„± íŒŒì¼ì„ STTë¡œ ì²˜ë¦¬í•˜ê³  vLLMìœ¼ë¡œ ì—°ì† ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
