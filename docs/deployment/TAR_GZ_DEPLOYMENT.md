# ğŸ“¦ tar.gz íŒŒì¼ êµ¬ì„± ë° ìš´ì˜ì„œë²„ ë°°í¬ ê°€ëŠ¥ì„± í™•ì¸

## ì§ˆë¬¸
tar.gz íŒŒì¼ì— CTranslate2 í¬ë§·ì´ í¬í•¨ë˜ì–´ ìˆê³ , ìš´ì˜ì„œë²„ì— ì••ì¶•ì„ í’€ì—ˆì„ ë•Œ 
faster-whisper, openai-whisper, whisper ì…‹ ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•œê°€?

## ë‹µë³€: âœ… **ë§ìŠµë‹ˆë‹¤!**

---

## 1. tar.gz íŒŒì¼ ë‚´ ëª¨ë¸ í¬ë§·

### ğŸ“‹ í˜„ì¬ í¬í•¨ëœ íŒŒì¼ êµ¬ì¡°
```
models/openai_whisper-large-v3-turbo/
â”œâ”€â”€ ctranslate2_model/              â† CTranslate2 í¬ë§·
â”‚   â”œâ”€â”€ model.bin                   (776MB - í•µì‹¬ ëª¨ë¸ íŒŒì¼)
â”‚   â”œâ”€â”€ config.json                 (ëª¨ë¸ ì„¤ì •)
â”‚   â””â”€â”€ vocabulary.json             (í† í¬ë‚˜ì´ì € ì‚¬ì „)
â”‚
â”œâ”€â”€ model.safetensors               â† PyTorch í¬ë§· #1
â”‚   (ì›ë³¸ ë‹¤ìš´ë¡œë“œëœ PyTorch ëª¨ë¸)
â”‚
â”œâ”€â”€ model.bin                        â† PyTorch í¬ë§· #2 (ì‹¬ë§í¬)
â”‚   (ctranslate2_model/model.binìœ¼ë¡œ í–¥í•˜ëŠ” ì‹¬ë§í¬)
â”‚
â””â”€â”€ .cache/huggingface/             â† Huggingface ìºì‹œ
    (config.json, tokenizer.json ë“±)
```

### ğŸ“Š íŒŒì¼ í¬ê¸° ë¶„ì„
```
ctranslate2_model/model.bin     : 776MB  (CTranslate2 ë°”ì´ë„ˆë¦¬)
model.safetensors               : 1.5GB  (PyTorch í¬ë§·)
ì „ì²´ ì••ì¶• í›„                     : 2.0GB  (33.1% ì••ì¶•ë¥ )
```

---

## 2. ìš´ì˜ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ

### âœ… ì…‹ ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!

#### **1ï¸âƒ£ faster-whisper (CTranslate2 í¬ë§·) - ğŸš€ ê°€ì¥ ë¹ ë¦„**
```python
from faster_whisper import WhisperModel

# CTranslate2 í¬ë§·ìœ¼ë¡œ ìë™ ë¡œë“œ
model = WhisperModel(
    "models/openai_whisper-large-v3-turbo/ctranslate2_model",
    device="cuda",
    compute_type="int8"
)
```
- âœ… tar.gzì— í¬í•¨ë¨ (ctranslate2_model/)
- ğŸ’¨ ê°€ì¥ ë¹ ë¥¸ ì¶”ë¡  ì†ë„
- ğŸ’¾ INT8 ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ğŸ”§ ì„¤ì •: convert_type = "int8" (ìë™)

#### **2ï¸âƒ£ OpenAI Whisper (PyTorch í¬ë§·) - ì•ˆì •ì **
```python
import whisper

# PyTorch í¬ë§·ìœ¼ë¡œ ìë™ ë¡œë“œ
model = whisper.load_model("large", device="cuda")

# ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
model = whisper.load_model(
    "models/openai_whisper-large-v3-turbo",
    device="cuda"
)
```
- âœ… tar.gzì— í¬í•¨ë¨ (model.safetensors)
- âœ… model.bin (ì‹¬ë§í¬)ë„ í¬í•¨
- ğŸŸ¡ ì¤‘ê°„ ì •ë„ ì„±ëŠ¥
- ğŸ“š ì˜ ì•Œë ¤ì§„ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### **3ï¸âƒ£ Huggingface Transformers (ì›ë³¸ ëª¨ë¸)**
```python
from transformers import AutoModelForSpeechSeq2Seq

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "models/openai_whisper-large-v3-turbo",
    local_files_only=True,
    device_map="cuda"
)
```
- âœ… tar.gzì— í¬í•¨ë¨ (ëª¨ë“  í•„ìš” íŒŒì¼)
- ğŸŸ¡ ê°€ì¥ ëŠë¦¼
- ğŸ“– ìœ ì—°í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

---

## 3. stt_engine.pyì˜ ë¡œë“œ ì „ëµ

### ì½”ë“œì—ì„œ ìë™ í´ë°± ë¡œì§
```python
def __init__(self, model_path: str):
    # 1ï¸âƒ£ faster-whisper ë¨¼ì € ì‹œë„ (ê°€ì¥ ë¹ ë¦„)
    if FASTER_WHISPER_AVAILABLE:
        self._try_faster_whisper()  # CTranslate2 í¬ë§· ì‚¬ìš©
    
    # 2ï¸âƒ£ faster-whisper ì‹¤íŒ¨ â†’ OpenAI Whisper ì‹œë„
    if self.backend is None and WHISPER_AVAILABLE:
        self._try_whisper()  # PyTorch í¬ë§· ì‚¬ìš©
    
    # 3ï¸âƒ£ ë‘˜ ë‹¤ ì‹¤íŒ¨ â†’ ì—ëŸ¬
    if self.backend is None:
        raise RuntimeError("ëª¨ë‘ ì‹¤íŒ¨")
```

### ì‹¤ì œ ë™ì‘
```
ìƒí™© 1: faster-whisper ì„¤ì¹˜ë¨
â”œâ”€ CTranslate2 ëª¨ë¸ë¡œë“œ ì‹œë„
â”œâ”€ ì„±ê³µ! âœ… ctranslate2_model/model.bin ì‚¬ìš©
â””â”€ ì†ë„: ê°€ì¥ ë¹ ë¦„

ìƒí™© 2: faster-whisper ì‹¤íŒ¨ + openai-whisper ì„¤ì¹˜ë¨
â”œâ”€ PyTorch ëª¨ë¸ë¡œë“œ ì‹œë„
â”œâ”€ ì„±ê³µ! âœ… model.safetensors ë˜ëŠ” model.bin ì‚¬ìš©
â””â”€ ì†ë„: ì¤‘ê°„

ìƒí™© 3: ë‘˜ ë‹¤ ì—†ìŒ
â”œâ”€ ì—ëŸ¬ ë°œìƒ âŒ
â””â”€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”
```

---

## 4. ìš´ì˜ì„œë²„ ë°°í¬ ë‹¨ê³„ë³„ í™•ì¸

### âœ… Step 1: ì••ì¶• í’€ê¸°
```bash
cd /app/stt_engine
tar -xzf whisper-large-v3-turbo_models_20260205_161222.tar.gz
```

ê²°ê³¼:
```
models/openai_whisper-large-v3-turbo/
â”œâ”€â”€ ctranslate2_model/        âœ… í¬í•¨
â”œâ”€â”€ model.safetensors         âœ… í¬í•¨
â””â”€â”€ model.bin                 âœ… í¬í•¨
```

### âœ… Step 2: íŒ¨í‚¤ì§€ í™•ì¸ (RHEL 8.9)
```bash
# ê¶Œì¥: faster-whisper + openai-whisper ë‘˜ ë‹¤ ì„¤ì¹˜
pip install faster-whisper openai-whisper
```

ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤:
- faster-whisperë§Œ ì„¤ì¹˜ â†’ CTranslate2 ì‚¬ìš©
- openai-whisperë§Œ ì„¤ì¹˜ â†’ PyTorch ì‚¬ìš©
- ë‘˜ ë‹¤ ì„¤ì¹˜ â†’ faster-whisper ë¨¼ì € ì‚¬ìš© (ë” ë¹ ë¦„)

### âœ… Step 3: ë¡œë“œ í…ŒìŠ¤íŠ¸
```bash
python -c "
from stt_engine import WhisperSTT

stt = WhisperSTT('models')
# ìë™ìœ¼ë¡œ ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì„ íƒ
print(f'ì‚¬ìš© ì¤‘ì¸ ë°±ì—”ë“œ: {stt.backend}')
"
```

---

## 5. ê° í¬ë§·ë³„ íŠ¹ì§• ì •ë¦¬

| í•­ëª© | CTranslate2 | PyTorch | Transformers |
|------|------------|---------|--------------|
| **tar.gz í¬í•¨** | âœ… | âœ… | âœ… |
| **ì¶”ë¡  ì†ë„** | ğŸš€ ê°€ì¥ ë¹ ë¦„ | ğŸŸ¡ ì¤‘ê°„ | ğŸ¢ ê°€ì¥ ëŠë¦¼ |
| **ë©”ëª¨ë¦¬** | ğŸ’¾ ë§¤ìš° íš¨ìœ¨ì  | ğŸŸ¡ ë³´í†µ | ğŸŸ¡ ë³´í†µ |
| **ë¼ì´ë¸ŒëŸ¬ë¦¬** | faster-whisper | openai-whisper | transformers |
| **ì–‘ìí™”** | INT8 ìë™ | ì—†ìŒ | ì„ íƒ ê°€ëŠ¥ |
| **CUDA ì§€ì›** | âœ… | âœ… | âœ… |
| **RHEL 8.9** | âœ… | âœ… | âœ… |

---

## 6. ê¶Œì¥ ë°°í¬ ì „ëµ

### ğŸ¯ RHEL 8.9 ìš´ì˜ì„œë²„ì—ì„œ
```bash
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (fastest-whisper ìš°ì„  ì¶”ì²œ)
pip install -r requirements.txt

# 2. ëª¨ë¸ ë°°í¬
tar -xzf whisper-large-v3-turbo_models_20260205_161222.tar.gz

# 3. í™•ì¸
python -c "from stt_engine import WhisperSTT; \
           stt = WhisperSTT('models'); \
           print(f'âœ… {stt.backend}ë¡œ ë¡œë“œë¨')"

# 4. Docker ì‹¤í–‰
docker run -v /app/stt_engine/models:/app/models \
           stt-engine:cuda129-v1.2
```

### ì„±ëŠ¥ ìˆœì„œ (RHEL 8.9 CUDA 12.9)
```
1. faster-whisper + CTranslate2   : 4-5ë°° ë¹ ë¦„ â­â­â­
2. openai-whisper + PyTorch       : ê¸°ë³¸ ì†ë„  â­â­
3. transformers                   : ëŠë¦¼      â­
```

---

## 7. ìµœì¢… í™•ì¸

### tar.gz íŒŒì¼ ë‚´ìš©
```
âœ… CTranslate2 í¬ë§·      : í¬í•¨ (ctranslate2_model/model.bin)
âœ… PyTorch í¬ë§·         : í¬í•¨ (model.safetensors, model.bin)
âœ… Huggingface ìºì‹œ     : í¬í•¨ (.cache/huggingface/)
```

### ìš´ì˜ì„œë²„ ì‚¬ìš© ê°€ëŠ¥ì„±
```
âœ… faster-whisper : ê°€ëŠ¥ (CTranslate2)
âœ… openai-whisper : ê°€ëŠ¥ (PyTorch)
âœ… transformers   : ê°€ëŠ¥ (Huggingface)
```

### ì½”ë“œ ì§€ì›
```
âœ… stt_engine.py : ìë™ í´ë°± ì§€ì›
âœ… íŒ¨í‚¤ì§€ ì—†ì„ ì‹œ: ìë™ ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥
âœ… ë¡œê¹…: ì–´ë–¤ ë°±ì—”ë“œ ì‚¬ìš© ì¤‘ì¸ì§€ í‘œì‹œ
```

---

## ğŸ‰ ê²°ë¡ 

**ì˜ˆ. ì™„ë²½í•˜ê²Œ ì§€ì›í•©ë‹ˆë‹¤!**

1. **tar.gzì—ëŠ” 3ê°€ì§€ í¬ë§·ì´ ëª¨ë‘ í¬í•¨**
   - CTranslate2 (fastest-whisperìš©)
   - PyTorch (openai-whisperìš©)
   - Huggingface ìºì‹œ

2. **ìš´ì˜ì„œë²„ì—ì„œ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥**
   - íŒ¨í‚¤ì§€ ì„¤ì¹˜í•˜ë©´ ìë™ ì„ íƒ
   - stt_engine.pyê°€ ìµœì ì˜ ë°±ì—”ë“œ ì‚¬ìš©

3. **ì¶”ì²œ êµ¬ì„± (RHEL 8.9 + CUDA 12.9)**
   ```bash
   pip install faster-whisper openai-whisper
   tar -xzf whisper-large-v3-turbo_models_20260205_161222.tar.gz
   # â†’ faster-whisper (CTranslate2) ì‚¬ìš© â†’ ê°€ì¥ ë¹ ë¦„
   ```

---

**ìƒì„±ì¼**: 2026-02-05
**ìƒíƒœ**: âœ… ìš´ì˜ì„œë²„ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ
