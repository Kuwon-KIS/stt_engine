# RHEL 8.9 í™˜ê²½ì— ìµœì í™”ëœ STT Engine ë¹Œë“œ & ë°°í¬ ê°€ì´ë“œ

## ğŸ“Š ëŒ€ìƒ í™˜ê²½ ì •ë³´

```
ìš´ì˜ ì„œë²„ (RHEL 8.9):
â”œâ”€ OS: RHEL 8.9 (Ootpa)
â”œâ”€ glibc: 2.28
â”œâ”€ Python: 3.11.5
â”œâ”€ CUDA: 12.9
â”œâ”€ NVIDIA Driver: 575.57.08
â””â”€ Status: âœ… ëª¨ë“  ì •ë³´ í™•ì¸ë¨
```

---

## ğŸ¯ ë¹Œë“œ ì „ëµ

### ì„ íƒ ì‚¬í•­

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | í˜¸í™˜ì„± |
|------|------|------|--------|
| **RHEL 8.9 EC2** ğŸ”´ | glibc ì™„ë²½ ì¼ì¹˜ | ì•½ê°„ ë¹„ìŒˆ | âœ… 100% |
| **Ubuntu 22.04 EC2** | ì €ë ´ | glibc ë¶ˆì¼ì¹˜ | âš ï¸ 90% |
| **ìš´ì˜ ì„œë²„ ì§ì ‘ ë¹Œë“œ** | ë¹„ìš© ì ˆê° | ë‹¤ìš´íƒ€ì„ | âœ… 100% |

### ğŸ”´ **ê¶Œì¥: RHEL 8.9 EC2 ë¹Œë“œ**
```
ì´ìœ :
1. íƒ€ê²Ÿ ì„œë²„ì™€ ë™ì¼í•œ glibc 2.28
2. ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± 100%
3. ì•ˆì „ì„± ìµœìš°ì„ 
```

---

## ğŸ“‹ Step 1: AWS EC2 ìƒì„± (RHEL 8.9)

### 1-1. AMI ì„ íƒ
```bash
# AWS Consoleì—ì„œ:
1. EC2 > Instances > Launch Instance
2. "RHEL" ê²€ìƒ‰
3. "Red Hat Enterprise Linux 8 (HVM)" ì„ íƒ
4. Version: 8.9
```

### 1-2. ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…
```
t3.large (4GB RAM, 2 vCPU)
ë˜ëŠ” t3.xlarge (8GB RAM, 4 vCPU - ê¶Œì¥)
```

### 1-3. Storage
```
EBS: 50GB ì´ìƒ (gp3 ê¶Œì¥)
```

### 1-4. Security Group
```
Inbound:
- SSH (Port 22) from your-ip
- Optional: HTTP (80), HTTPS (443)
```

---

## ğŸš€ Step 2: EC2ì— ì—°ê²° ë° í™˜ê²½ ì„¤ì •

### 2-1. SSH ì—°ê²°
```bash
ssh -i your-key.pem ec2-user@<ec2-ip>
```

### 2-2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### ë°©ë²• A: Docker ì„¤ì¹˜ (ê¶Œì¥ - ì‹¤ì œ Docker ì‚¬ìš©)

```bash
# RHEL 8.9 ê¸°ë³¸ ì—…ë°ì´íŠ¸
sudo yum update -y

# Development Tools ì„¤ì¹˜
sudo yum groupinstall -y "Development Tools"

# Git ì„¤ì¹˜
sudo yum install -y git

# Docker ì €ì¥ì†Œ ì¶”ê°€
sudo yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo

# Docker CE ì„¤ì¹˜ (Podman ëŒ€ì‹  ì‹¤ì œ Docker)
sudo yum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# Docker ë°ëª¬ ì‹œì‘ ë° ìë™ ì‹œì‘ í™œì„±í™”
sudo systemctl start docker
sudo systemctl enable docker

# í˜„ì¬ ì‚¬ìš©ìì—ê²Œ Docker ê¶Œí•œ ë¶€ì—¬
sudo usermod -aG docker ec2-user
newgrp docker

# ë²„ì „ í™•ì¸
docker --version
docker ps
git --version
```

**ğŸ’¡ íŒ**: `newgrp docker` í›„ ìƒˆ í„°ë¯¸ë„ì—ì„œ `sudo` ì—†ì´ docker ëª…ë ¹ ì‚¬ìš© ê°€ëŠ¥

---

#### ë°©ë²• B: Podman ì‚¬ìš© (ê¸°ë³¸ ì œê³µ - Docker ëª…ë ¹ í˜¸í™˜)

ë§Œì•½ Docker ì„¤ì¹˜ê°€ ì‹¤íŒ¨í•˜ë©´ Podmanì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Podmanì€ ì´ë¯¸ ì„¤ì¹˜ë¨ (ìœ„ì˜ docker ì„¤ì¹˜ ê±´ë„ˆëœ€)
# ë™ì¼í•œ ëª…ë ¹ìœ¼ë¡œ ì‘ë™:
docker --version   # Podmanìœ¼ë¡œ ì‹¤í–‰ë¨
docker ps
```

---

## âš ï¸ RHEL 8.9 íŠ¹ìˆ˜ ì‚¬í•­: Docker vs Podman

### ìƒí™©
ìœ„ì˜ **ë°©ë²• A**ë¡œ Dockerë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜í–ˆë‹¤ë©´ ì‹¤ì œ Dockerë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```bash
# í™•ì¸ ë°©ë²•
docker --version
# ì¶œë ¥: Docker version 25.x.x, build xxxxx  â† ì‹¤ì œ Docker
```

### Docker ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ

ë§Œì•½ Docker ì €ì¥ì†Œ ì¶”ê°€ê°€ ì‹¤íŒ¨í•˜ë©´ (ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ ë“±):

```bash
# Podmanìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥ (ê¸°ë³¸ ì œê³µ)
# docker ëª…ë ¹ì´ Podmanìœ¼ë¡œ ì‹¤í–‰ë¨
docker --version
# ì¶œë ¥: Emulate Docker CLI using podman. podman version 4.9.4-rhel

# ì´ ê²½ìš°ì—ë„ ëª¨ë“  docker ëª…ë ¹ ë™ì¼í•˜ê²Œ ì‘ë™:
docker run ...    # âœ… ì‘ë™
docker ps         # âœ… ì‘ë™
docker build ...  # âœ… ì‘ë™
```

---

## ğŸš€ Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡ 

```bash
# ë°©ë²• A: Git í´ë¡  (ê¶Œì¥)
cd ~
git clone https://github.com/Kuwon-KIS/stt_engine.git
cd stt_engine

# ë°©ë²• B: scpë¡œ ë¡œì»¬ íŒŒì¼ ì „ì†¡
# Macì—ì„œ:
scp -i your-key.pem -r ~/workspace/stt_engine ec2-user@<ec2-ip>:~/
```

---

## ğŸ—ï¸ Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ (20~40ë¶„)

### 4-1. ë¹Œë“œ ì‹¤í–‰

```bash
cd ~/stt_engine

# ë°©ë²• 1: RHEL 8.9 ì „ìš© ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)
bash scripts/build-stt-engine-rhel89.sh

# ë˜ëŠ” ë°©ë²• 2: ì§ì ‘ docker build ì‹¤í–‰
docker build \
  --platform linux/amd64 \
  -t stt-engine:cuda129-rhel89-v1.2 \
  -f docker/Dockerfile.engine.rhel89 \
  . 2>&1 | tee /tmp/build.log
```

### 4-2. ë¹Œë“œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ:
ssh -i your-key.pem ec2-user@<ec2-ip>
watch -n 10 'docker ps -a && echo "---" && df -h'

# ë˜ëŠ” ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f /tmp/build.log
```

### 4-3. ë¹Œë“œ ì™„ë£Œ í™•ì¸

```bash
# ì´ë¯¸ì§€ í™•ì¸
docker images | grep stt-engine

# ì˜ˆìƒ ì¶œë ¥:
# stt-engine   cuda129-rhel89-v1.2   HASH   7.3GB   1 minute ago

# ì´ë¯¸ì§€ ìƒì„¸ ì •ë³´ í™•ì¸
docker inspect stt-engine:cuda129-rhel89-v1.2 | jq '.Config.Env[] | select(startswith("LD_"))'

# ì˜ˆìƒ ì¶œë ¥ì— LD_LIBRARY_PATHê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
```

### 4-4. ë¹Œë“œ ì˜¤ë¥˜ ì²˜ë¦¬

```bash
# ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ë¡œê·¸ í™•ì¸
grep -i "error\|failed\|not found" /tmp/build.log | tail -20
```

---

## ğŸ“¦ Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„ (25~45ë¶„)

**ì¤‘ìš”**: EC2 ë¹Œë“œ ì„œë²„ì˜ **ë¡œì»¬ í™˜ê²½**ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤ (Dockerê°€ ì•„ë‹˜).

### 5-0. RHEL 8.9 Python í™˜ê²½ ì„¤ì • (í•„ìˆ˜!)

```bash
cd ~/stt_engine

# 1ï¸âƒ£ Python 3.11 ë° pip ì„¤ì¹˜ (RHEL 8.9 íŠ¹ìˆ˜)
sudo yum install -y python3.11-pip python3.11-devel

# ë˜ëŠ” pipê°€ ì—†ìœ¼ë©´ ensurepipë¡œ ì„¤ì¹˜
python3.11 -m ensurepip --upgrade

# 2ï¸âƒ£ pip ì—…ê·¸ë ˆì´ë“œ
python3.11 -m pip install --upgrade pip setuptools wheel

# 3ï¸âƒ£ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë³€í™˜ì— í•„ìš”í•œ í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# (ì´ë¯¸ì§€ ë‚´ë¶€ì—ëŠ” ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆì§€ë§Œ, í˜¸ìŠ¤íŠ¸ì—ì„œë„ í•„ìš”)
python3.11 -m pip install --upgrade \
    torch==2.6.0 \
    torchaudio==2.6.0 \
    transformers \
    ctranslate2 \
    huggingface-hub \
    scipy \
    numpy \
    librosa \
    pydantic \
    urllib3

# ì„¤ì¹˜ í™•ì¸
python3.11 -c "import torch, transformers, ctranslate2; print('âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨')"
```

**ì£¼ì˜**: ì´ ë‹¨ê³„ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ëŠ” 5~15ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.

### 5-1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° CTranslate2 ë³€í™˜

```bash
cd ~/stt_engine

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜ ì‹¤í–‰
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ:
#   1. openai/whisper-large-v3-turbo ë‹¤ìš´ë¡œë“œ (Hugging Face)
#   2. CTranslate2 í¬ë§· ë³€í™˜ (model.bin ìƒì„±)
#   3. ëª¨ë¸ êµ¬ì¡° ê²€ì¦
python3.11 download_model_hf.py 2>&1 | tee /tmp/model_download.log

# ì˜ˆìƒ ì¶œë ¥:
# ========================================================
# ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„
# ========================================================
# 
# ğŸ“Œ Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# âœ… ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ
#
# ğŸ“Œ Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# â³ openai/whisper-large-v3-turbo ë‹¤ìš´ë¡œë“œ ì¤‘...
# âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ì•½ 10-15ë¶„ ì†Œìš”)
#
# ğŸ“Œ Step 3: ëª¨ë¸ íŒŒì¼ ê²€ì¦
# âœ… config.json ê²€ì¦ ì™„ë£Œ
# âœ… pytorch_model.bin ê²€ì¦ ì™„ë£Œ
# âœ… tokenizer.json ê²€ì¦ ì™„ë£Œ
#
# ğŸ“Œ Step 4: CTranslate2 í¬ë§· ë³€í™˜
# â³ CTranslate2 ë³€í™˜ ì¤‘... (ì•½ 5~10ë¶„ ì†Œìš”)
# âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ
#
# ğŸ“Œ Step 5: ëª¨ë¸ êµ¬ì¡° ê²€ì¦
# âœ… ctranslate2_model êµ¬ì¡° í™•ì¸
#    - config.json âœ“
#    - model.bin âœ“
#    - vocabulary.json âœ“
#
# âœ… ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!
```

**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 25~45ë¶„ (ëª¨ë¸ í¬ê¸° ë‹¤ìš´ë¡œë“œ: 10~15ë¶„ + CTranslate2 ë³€í™˜: 5~10ë¶„)

### 5-2. ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
du -sh models/
du -sh models/openai_whisper-large-v3-turbo/
du -sh models/ctranslate2_model/

# ì˜ˆìƒ:
# 2.5G  models/
# 1.6G  models/openai_whisper-large-v3-turbo/
# 0.9G  models/ctranslate2_model/

# íŒŒì¼ í™•ì¸
find models/ -type f -name "*.json" -o -name "*.bin"
```

### 5-3. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (Python)

```bash
python3.11 << 'PYTHON_TEST'
from pathlib import Path

models_base = Path("models")
print("=" * 70)
print("ğŸ” ëª¨ë¸ êµ¬ì¡° ì„¸ë¶€ ê²€ì¦")
print("=" * 70)

# CTranslate2 ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ CTranslate2 ëª¨ë¸")
ct2_model = models_base / "ctranslate2_model"
required_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "vocabulary.json": "í† í¬ë‚˜ì´ì € ì–´íœ˜"
}

for fname, desc in required_files.items():
    fpath = ct2_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:20} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:20} NOT FOUND")

# OpenAI Whisper ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ OpenAI Whisper ëª¨ë¸")
whisper_model = models_base / "openai_whisper-large-v3-turbo"
required_whisper_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "pytorch_model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "tokenizer.json": "í† í¬ë‚˜ì´ì €"
}

for fname, desc in required_whisper_files.items():
    fpath = whisper_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:25} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:25} NOT FOUND")

print("\n" + "=" * 70)
PYTHON_TEST
```

---

## ğŸ§ª Step 6: ë¹Œë“œ ì„œë²„ì—ì„œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (20~30ë¶„)

### 6-1. í…ŒìŠ¤íŠ¸ìš© ì»¨í…Œì´ë„ˆ ì‹œì‘ (ëª¨ë¸ ë§ˆìš´íŠ¸)

```bash
cd ~/stt_engine

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸)
docker run -it \
  --name stt-test-engine \
  -v $(pwd)/models:/app/models \
  -e CUDA_VISIBLE_DEVICES=0 \
  stt-engine:cuda129-rhel89-v1.2 \
  /bin/bash
```

### 6-2. ì»¨í…Œì´ë„ˆ ë‚´ ë§ˆìš´íŠ¸ëœ ëª¨ë¸ í™•ì¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# ë§ˆìš´íŠ¸ í™•ì¸
ls -lh /app/models/
du -sh /app/models/*
```

### 6-3. CUDA ë° PyTorch ê²€ì¦

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import torch
import torchaudio
import os

print("=" * 70)
print("ğŸ” CUDA & PyTorch ê²€ì¦")
print("=" * 70)

print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… torchaudio: {torchaudio.__version__}")
print(f"âœ… CUDA Available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"âœ… CUDA Device: {torch.cuda.get_device_name(0)}")
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.matmul(x, y)
    print(f"âœ… CUDA Matrix Multiplication: Success")

print(f"âœ… LD_LIBRARY_PATH: {bool(os.environ.get('LD_LIBRARY_PATH'))}")
print("=" * 70)

PYTHON_TEST
```

### 6-4. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# Faster-Whisper (CTranslate2 ëª¨ë¸)
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ Faster-Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from faster_whisper import WhisperModel
    
    print("\nâ³ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = WhisperModel(
        "/app/models/ctranslate2_model",
        device="auto",
        compute_type="float32",
        download_root="/opt/app-root/src/.cache",
        local_files_only=True
    )
    
    print("âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST

# OpenAI Whisper (ì›ë³¸ ëª¨ë¸)
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ OpenAI Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
    
    print("\nâ³ Processor ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("â³ Model ë¡œë“œ ì¤‘...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST
```

### 6-5. ì»¨í…Œì´ë„ˆ ì¢…ë£Œ

```bash
# ì»¨í…Œì´ë„ˆì—ì„œ exit ì‹¤í–‰
exit

# ë˜ëŠ” ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ
docker rm stt-test-engine
```

---

## ğŸ’¾ Step 7: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥ (5~10ë¶„)

### 7-1. EC2ì—ì„œ ì´ë¯¸ì§€ì™€ ëª¨ë¸ ì €ì¥

```bash
cd ~/stt_engine

# Docker ì´ë¯¸ì§€ ì €ì¥
mkdir -p ~/build/output
docker save stt-engine:cuda129-rhel89-v1.2 | gzip > ~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz

# ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
ls -lh models/
du -sh models/

# ë¹Œë“œ ë¡œê·¸ ì €ì¥
cp /tmp/build.log ~/build/output/build-$(date +%Y%m%d-%H%M%S).log
cp /tmp/model_download.log ~/build/output/model-$(date +%Y%m%d-%H%M%S).log

# ìµœì¢… íŒŒì¼ í™•ì¸
ls -lh ~/build/output/
```

**ì†Œìš” ì‹œê°„: 5~10ë¶„**

---

## ğŸš¢ Step 8: ìš´ì˜ ì„œë²„ì— ë°°í¬

### 8-1. Macìœ¼ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­ - ë¡œì»¬ ê²€ì¦ìš©)

```bash
# Mac ë¡œì»¬ í„°ë¯¸ë„:
scp -i your-key.pem ec2-user@<ec2-ip>:~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz \
    ~/Downloads/

# íŒŒì¼ í™•ì¸
ls -lh ~/Downloads/stt-engine-cuda129-rhel89-v1.2.tar.gz
```

**ì†Œìš” ì‹œê°„: 2~5ë¶„ (ë„¤íŠ¸ì›Œí¬ì— ë”°ë¼)**

### 8-2. EC2 ë¹Œë“œ ì„œë²„ì—ì„œ ìš´ì˜ ì„œë²„ë¡œ ì§ì ‘ ì „ì†¡ (ê¶Œì¥)

```bash
# EC2 ë¹Œë“œ ì„œë²„ì—ì„œ:
# 1. Docker ì´ë¯¸ì§€ ë¡œë“œ
scp -i your-key.pem \
  ~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz \
  deploy-user@production-server:/tmp/

# 2. ëª¨ë¸ ë””ë ‰í† ë¦¬ ì „ì†¡ (ëŒ€ìš©ëŸ‰ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦¼)
scp -r -i your-key.pem \
  ~/stt_engine/models \
  deploy-user@production-server:/path/to/deployment/
```

### 8-3. ìš´ì˜ ì„œë²„ì—ì„œ ë¡œë“œ

```bash
# RHEL 8.9 ìš´ì˜ ì„œë²„:
cd /tmp

# 1. Docker ì´ë¯¸ì§€ ë¡œë“œ
docker load < stt-engine-cuda129-rhel89-v1.2.tar.gz

# 2. ì´ë¯¸ì§€ í™•ì¸
docker images | grep stt-engine
# ì¶œë ¥: stt-engine  cuda129-rhel89-v1.2  <image-id>  7.3GB
```

---

## âœ… Step 9: ì´ë¯¸ì§€ ê²€ì¦ (ìš´ì˜ ì„œë²„)

### 9-1. PyTorch/CUDA ê²€ì¦
```bash
docker run --rm stt-engine:cuda129-rhel89-v1.2 python3.11 -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')
print(f'âœ… CUDA Available: {torch.cuda.is_available()}')
print(f'âœ… cuDNN: OK')
"

# ì˜ˆìƒ ì¶œë ¥:
# âœ… PyTorch: 2.6.0
# âœ… CUDA Available: True
# âœ… cuDNN: OK
```

### 9-2. Whisper ê²€ì¦
```bash
docker run --rm stt-engine:cuda129-rhel89-v1.2 python3.11 -c "
try:
    import faster_whisper
    print('âœ… faster-whisper: ë¡œë“œë¨')
except:
    print('âš ï¸  faster-whisper: ë¯¸ì‚¬ìš©')
    
try:
    from transformers import AutoModelForSpeechSeq2Seq
    print('âœ… transformers: ë¡œë“œë¨')
except:
    print('âš ï¸  transformers: ë¯¸ì‚¬ìš©')
"
```

### 9-3. ëª¨ë¸ ë§ˆìš´íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸í•˜ì—¬ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -it \
  --name stt-final-test \
  -v /path/to/models:/app/models \
  -e CUDA_VISIBLE_DEVICES=0 \
  stt-engine:cuda129-rhel89-v1.2 \
  /bin/bash

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("âœ… ìµœì¢… í†µí•© ê²€ì¦")
print("=" * 70)

# Faster-Whisper ë¡œë“œ í™•ì¸
try:
    from faster_whisper import WhisperModel
    model = WhisperModel(
        "/app/models/ctranslate2_model",
        device="auto",
        compute_type="float32",
        local_files_only=True
    )
    print("\nâœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"\nâŒ Faster-Whisper ì˜¤ë¥˜: {e}")

# OpenAI Whisper ë¡œë“œ í™•ì¸
try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
    processor = AutoProcessor.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True
    )
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True
    )
    print("âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ OpenAI Whisper ì˜¤ë¥˜: {e}")

print("\n" + "=" * 70)
print("ğŸ‰ ëª¨ë“  ê²€ì¦ ì™„ë£Œ!")
print("=" * 70)
PYTHON_TEST

# ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
exit
```

---

## ğŸ“Š ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ì „ì²´)

| ë‹¨ê³„ | ì˜ˆìƒ ì‹œê°„ |
|------|----------|
| Step 1: EC2 ìƒì„± | 2ë¶„ |
| Step 2: í™˜ê²½ ì„¤ì • | 5ë¶„ |
| Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡  | 2ë¶„ |
| Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ | 20~40ë¶„ |
| **Step 5-0: Python í™˜ê²½ ì„¤ì • (NEW)** | **5~15ë¶„** |
| Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + ë³€í™˜ | 20~30ë¶„ |
| Step 6: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ | 20~30ë¶„ |
| Step 7: ì´ë¯¸ì§€/ëª¨ë¸ ì €ì¥ | 5~10ë¶„ |
| Step 8: ìš´ì˜ ì„œë²„ ë°°í¬ | 5~15ë¶„ |
| Step 9: ì´ë¯¸ì§€ ê²€ì¦ | 5ë¶„ |
| **ì´ ì†Œìš” ì‹œê°„** | **95~170ë¶„ (1.6~2.8ì‹œê°„)** |

**ì£¼ìš” ë³€ê²½**: Step 5-0ì—ì„œ Python í™˜ê²½ ì„¤ì • ì¶”ê°€ (pip, PyTorch ë“±)

---

## âœ… ìµœì¢… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë“  í•­ëª©ì´ âœ… ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤:

```
Build ì„œë²„ (EC2):
  âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì„±ê³µ (7.3GB)
  âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (2.5GB)
  âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ (model.bin ìƒì„±)
  âœ… ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì™„ë£Œ
  âœ… í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ

Production ì„œë²„ (RHEL 8.9):
  âœ… Docker ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ
  âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ê°€ëŠ¥
  âœ… PyTorch/CUDA ì •ìƒ ì‘ë™
  âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ
  âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ
  âœ… ìµœì¢… í†µí•© ê²€ì¦ í†µê³¼
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (ìš´ì˜ ì„œë²„)

ë¹Œë“œ ë° ê²€ì¦ ì™„ë£Œ í›„:

1. **STT API ì„œë²„ ì‹¤í–‰**
   ```bash
   docker run -d \
     --name stt-api \
     --gpus all \
     -p 5000:5000 \
     -v /path/to/models:/app/models \
     -e CUDA_VISIBLE_DEVICES=0 \
     stt-engine:cuda129-rhel89-v1.2
   ```

2. **í—¬ìŠ¤ ì²´í¬**
   ```bash
   sleep 10
   curl http://localhost:5000/health
   # ì˜ˆìƒ: {"status":"ok","backend":"faster-whisper"}
   ```

3. **STT ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸**
   ```bash
   curl -X POST http://localhost:5000/transcribe \
     -F "file=@/path/to/audio.wav"
   ```

---

## ğŸ“ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

| ë¬¸ì œ | í•´ê²°ì±… |
|------|--------|
| `/usr/bin/python3.11: No module named pip` | `sudo yum install -y python3.11-pip` ë˜ëŠ” `python3.11 -m ensurepip --upgrade` |
| `ModuleNotFoundError: No module named 'urllib3'` | Step 5-0 ì˜ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ í™•ì¸ |
| `ModuleNotFoundError: torch` | PyTorch 2.6.0 ì„¤ì¹˜ í™•ì¸: `python3.11 -m pip install torch==2.6.0` |
| `ModuleNotFoundError: transformers` | `python3.11 -m pip install transformers` |
| `ModuleNotFoundError: ctranslate2` | `python3.11 -m pip install ctranslate2` |
| Docker ë¹Œë“œ ì‹¤íŒ¨ | ì¸í„°ë„· ì—°ê²° í™•ì¸, `grep -i error /tmp/build.log` í™•ì¸ |
| ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ | HuggingFace ì ‘ê·¼ì„± í™•ì¸, VPN ì‚¬ìš©, í”„ë¡ì‹œ ì„¤ì • |
| CUDA ì¸ì‹ ì•ˆë¨ | ìš´ì˜ ì„œë²„ì˜ `nvidia-smi` í™•ì¸ |
| ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ | ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸, `/app/models` ë§ˆìš´íŠ¸ í™•ì¸ |
| ë””ìŠ¤í¬ ë¶€ì¡± | EC2: `df -h` í™•ì¸, ìš´ì˜ ì„œë²„: ì¶©ë¶„í•œ ìŠ¤í† ë¦¬ì§€ í™•ë³´ |

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì™„ì „ ë²„ì „)

```
[ ] RHEL 8.9 ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ
    - OS: 8.9
    - glibc: 2.28
    - Python: 3.11.5
    - CUDA: 12.9
    - NVIDIA Driver: 575.57.08

[ ] AWS EC2 RHEL 8.9 ìƒì„±
    - t3.large ì´ìƒ (ë˜ëŠ” t3.xlarge ê¶Œì¥)
    - 100GB ì´ìƒ ìŠ¤í† ë¦¬ì§€ (Docker 7GB + ëª¨ë¸ 2.5GB + ì—¬ìœ )
    - Security Group ì„¤ì •

[ ] EC2 í™˜ê²½ ì„¤ì •
    - Docker ì„¤ì¹˜ ë° ì‹¤í–‰
    - Git ì„¤ì¹˜
    - ì‚¬ìš©ì ê¶Œí•œ ì„¤ì •

[ ] Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
    - git clone ì™„ë£Œ
    - ì½”ë“œ ìµœì‹ í™” í™•ì¸

[ ] Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ
    - ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    - ë¹Œë“œ ì™„ë£Œ (7.3GB)
    - LD_LIBRARY_PATH ì„¤ì • í™•ì¸

[ ] Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
    - download_model_hf.py ì‹¤í–‰ ì™„ë£Œ
    - models/ ë””ë ‰í† ë¦¬ (2.5GB) ìƒì„±
    - CTranslate2 ë³€í™˜ ì™„ë£Œ
    - ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì™„ë£Œ

[ ] Step 6: ë¹Œë“œ ì„œë²„ì—ì„œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    - í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
    - CUDA ë° PyTorch ê²€ì¦
    - Faster-Whisper ë¡œë“œ ì„±ê³µ
    - OpenAI Whisper ë¡œë“œ ì„±ê³µ

[ ] Step 7: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥
    - Docker ì´ë¯¸ì§€ ì €ì¥ (tar.gz)
    - ë¹Œë“œ ë¡œê·¸ ì €ì¥
    - ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸

[ ] Step 8: ìš´ì˜ ì„œë²„ ë°°í¬
    - ì´ë¯¸ì§€ ë° ëª¨ë¸ ì „ì†¡ ì™„ë£Œ
    - ìš´ì˜ ì„œë²„ ë¡œë“œ ì™„ë£Œ

[ ] Step 9: ì´ë¯¸ì§€ ê²€ì¦
    - PyTorch/CUDA ê²€ì¦
    - Whisper ê²€ì¦
    - ëª¨ë¸ ë§ˆìš´íŠ¸ í…ŒìŠ¤íŠ¸
    - í†µí•© ê²€ì¦ ì™„ë£Œ
```

---

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸ ì •ë¦¬

### Build ì„œë²„ (AWS EC2 RHEL 8.9)ì—ì„œ:

1. **Step 1~3**: AWS EC2 ì¤€ë¹„ ë° ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
   - âœ… ì•½ 10ë¶„

2. **Step 4**: Docker ì´ë¯¸ì§€ ë¹Œë“œ
   - âœ… ì•½ 20~40ë¶„
   - ê²°ê³¼: `stt-engine:cuda129-rhel89-v1.2` (7.3GB)

3. **Step 5**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + CTranslate2 ë³€í™˜ (NEW)
   - âœ… ì•½ 25~45ë¶„
   - ê²°ê³¼: `models/` ë””ë ‰í† ë¦¬ (2.5GB)
     - `openai_whisper-large-v3-turbo/` (1.6GB)
     - `ctranslate2_model/` (0.9GB)

4. **Step 6**: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (NEW)
   - âœ… ì•½ 20~30ë¶„
   - Faster-Whisper + OpenAI Whisper ëª¨ë‘ í…ŒìŠ¤íŠ¸

5. **Step 7**: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥
   - âœ… ì•½ 5~10ë¶„

6. **Step 8~9**: ìš´ì˜ ì„œë²„ ë°°í¬ ë° ê²€ì¦
   - âœ… ì•½ 15~25ë¶„

### ìµœì¢… ê²°ê³¼:
- âœ… Production Ready Docker ì´ë¯¸ì§€
- âœ… ì™„ë²½í•˜ê²Œ ê²€ì¦ëœ ëª¨ë¸ ì„¸íŠ¸
- âœ… RHEL 8.9 100% í˜¸í™˜ì„± ë³´ì¥

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026ë…„ 2ì›” 7ì¼
