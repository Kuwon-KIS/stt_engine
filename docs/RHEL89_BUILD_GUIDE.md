# RHEL 8.9 í™˜ê²½ì— ìµœì í™”ëœ STT Engine ë¹Œë“œ & ë°°í¬ ê°€ì´ë“œ

## ğŸ“Š ëŒ€ìƒ í™˜ê²½ ì •ë³´

```
ìš´ì˜ ì„œë²„ (RHEL 8.9):
â”œâ”€ OS: RHEL 8.9 (Ootpa)
â”œâ”€ glibc: 2.28
â”œâ”€ Python: 3.11.5
â”œâ”€ CUDA: 12.9
â”œâ”€ NVIDIA Driver: 575.57.08
â””â”€ Status: âœ… ëª¨ë“  ì •ë³´ í™•ì¸ë¨
```

---

## ğŸ¯ ë¹Œë“œ ì „ëµ

### ì„ íƒ ì‚¬í•­

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | í˜¸í™˜ì„± |
|------|------|------|--------|
| **RHEL 8.9 EC2** ğŸ”´ | glibc ì™„ë²½ ì¼ì¹˜ | ì•½ê°„ ë¹„ìŒˆ | âœ… 100% |
| **Ubuntu 22.04 EC2** | ì €ë ´ | glibc ë¶ˆì¼ì¹˜ | âš ï¸ 90% |
| **ìš´ì˜ ì„œë²„ ì§ì ‘ ë¹Œë“œ** | ë¹„ìš© ì ˆê° | ë‹¤ìš´íƒ€ì„ | âœ… 100% |

### ğŸ”´ **ê¶Œì¥: RHEL 8.9 EC2 ë¹Œë“œ**
```
ì´ìœ :
1. íƒ€ê²Ÿ ì„œë²„ì™€ ë™ì¼í•œ glibc 2.28
2. ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± 100%
3. ì•ˆì „ì„± ìµœìš°ì„ 
```

---

## ğŸ“‹ Step 1: AWS EC2 ìƒì„± (RHEL 8.9)

### 1-1. AMI ì„ íƒ
```bash
# AWS Consoleì—ì„œ:
1. EC2 > Instances > Launch Instance
2. "RHEL" ê²€ìƒ‰
3. "Red Hat Enterprise Linux 8 (HVM)" ì„ íƒ
4. Version: 8.9
```

### 1-2. ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…
```
t3.large (4GB RAM, 2 vCPU)
ë˜ëŠ” t3.xlarge (8GB RAM, 4 vCPU - ê¶Œì¥)
```

### 1-3. Storage
```
EBS: 50GB ì´ìƒ (gp3 ê¶Œì¥)
```

### 1-4. Security Group
```
Inbound:
- SSH (Port 22) from your-ip
- Optional: HTTP (80), HTTPS (443)
```

---

## ğŸš€ Step 2: EC2ì— ì—°ê²° ë° í™˜ê²½ ì„¤ì •

### 2-1. SSH ì—°ê²°
```bash
ssh -i your-key.pem ec2-user@<ec2-ip>
```

### 2-2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### ë°©ë²• A: Docker ì„¤ì¹˜ (ê¶Œì¥ - ì‹¤ì œ Docker ì‚¬ìš©)

```bash
# RHEL 8.9 ê¸°ë³¸ ì—…ë°ì´íŠ¸
sudo yum update -y

# Development Tools ì„¤ì¹˜
sudo yum groupinstall -y "Development Tools"

# Git ì„¤ì¹˜
sudo yum install -y git

# Docker ì €ì¥ì†Œ ì¶”ê°€
sudo yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo

# Docker CE ì„¤ì¹˜ (Podman ëŒ€ì‹  ì‹¤ì œ Docker)
sudo yum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# Docker ë°ëª¬ ì‹œì‘ ë° ìë™ ì‹œì‘ í™œì„±í™”
sudo systemctl start docker
sudo systemctl enable docker

# í˜„ì¬ ì‚¬ìš©ìì—ê²Œ Docker ê¶Œí•œ ë¶€ì—¬
sudo usermod -aG docker ec2-user
newgrp docker

# ë²„ì „ í™•ì¸
docker --version
docker ps
git --version
```

**ğŸ’¡ íŒ**: `newgrp docker` í›„ ìƒˆ í„°ë¯¸ë„ì—ì„œ `sudo` ì—†ì´ docker ëª…ë ¹ ì‚¬ìš© ê°€ëŠ¥

---

#### ë°©ë²• B: Podman ì‚¬ìš© (ê¸°ë³¸ ì œê³µ - Docker ëª…ë ¹ í˜¸í™˜)

ë§Œì•½ Docker ì„¤ì¹˜ê°€ ì‹¤íŒ¨í•˜ë©´ Podmanì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Podmanì€ ì´ë¯¸ ì„¤ì¹˜ë¨ (ìœ„ì˜ docker ì„¤ì¹˜ ê±´ë„ˆëœ€)
# ë™ì¼í•œ ëª…ë ¹ìœ¼ë¡œ ì‘ë™:
docker --version   # Podmanìœ¼ë¡œ ì‹¤í–‰ë¨
docker ps
```

---

## âš ï¸ RHEL 8.9 íŠ¹ìˆ˜ ì‚¬í•­: Docker vs Podman

### ìƒí™©
ìœ„ì˜ **ë°©ë²• A**ë¡œ Dockerë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜í–ˆë‹¤ë©´ ì‹¤ì œ Dockerë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```bash
# í™•ì¸ ë°©ë²•
docker --version
# ì¶œë ¥: Docker version 25.x.x, build xxxxx  â† ì‹¤ì œ Docker
```

### Docker ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ

ë§Œì•½ Docker ì €ì¥ì†Œ ì¶”ê°€ê°€ ì‹¤íŒ¨í•˜ë©´ (ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ ë“±):

```bash
# Podmanìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥ (ê¸°ë³¸ ì œê³µ)
# docker ëª…ë ¹ì´ Podmanìœ¼ë¡œ ì‹¤í–‰ë¨
docker --version
# ì¶œë ¥: Emulate Docker CLI using podman. podman version 4.9.4-rhel

# ì´ ê²½ìš°ì—ë„ ëª¨ë“  docker ëª…ë ¹ ë™ì¼í•˜ê²Œ ì‘ë™:
docker run ...    # âœ… ì‘ë™
docker ps         # âœ… ì‘ë™
docker build ...  # âœ… ì‘ë™
```

---

## ğŸš€ Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡ 

```bash
# ë°©ë²• A: Git í´ë¡  (ê¶Œì¥)
cd ~
git clone https://github.com/Kuwon-KIS/stt_engine.git
cd stt_engine

# ë°©ë²• B: scpë¡œ ë¡œì»¬ íŒŒì¼ ì „ì†¡
# Macì—ì„œ:
scp -i your-key.pem -r ~/workspace/stt_engine ec2-user@<ec2-ip>:~/
```

---

## ğŸ—ï¸ Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ (20~40ë¶„)

### ë°©ë²• 1ï¸âƒ£: ìë™ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

```bash
cd ~/stt_engine

# Docker ì´ë¯¸ì§€ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
bash scripts/build-server-image.sh

# ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ:
# 1. ê¸°ì¡´ ì´ë¯¸ì§€ í™•ì¸ (ìˆìœ¼ë©´ ì¬ì‚¬ìš© ì—¬ë¶€ ë¬¼ì–´ë´„)
# 2. Docker ì´ë¯¸ì§€ ë¹Œë“œ (í•„ìš”ì‹œ)
# 3. ì´ë¯¸ì§€ë¥¼ tar.gzë¡œ ì €ì¥
# 4. ë¹Œë“œ ì •ë³´ íŒŒì¼ ìƒì„±
```

### ë°©ë²• 2ï¸âƒ£: ìˆ˜ë™ docker build ì‹¤í–‰

```bash
cd ~/stt_engine

# ì§ì ‘ docker build ì‹¤í–‰
docker build \
  --platform linux/amd64 \
  -t stt-engine:cuda129-rhel89-v1.2 \
  -f docker/Dockerfile.engine.rhel89 \
  . 2>&1 | tee /tmp/build.log
```

### 4-2. ë¹Œë“œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ:
ssh -i your-key.pem ec2-user@<ec2-ip>
watch -n 10 'docker ps -a && echo "---" && df -h'

# ë˜ëŠ” ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f /tmp/build-image-*.log
```

### 4-3. ë¹Œë“œ ì™„ë£Œ í™•ì¸

```bash
# ì´ë¯¸ì§€ í™•ì¸
docker images | grep stt-engine

# ì˜ˆìƒ ì¶œë ¥:
# stt-engine   cuda129-rhel89-v1.2   HASH   7.3GB   1 minute ago

# ì´ë¯¸ì§€ ìƒì„¸ ì •ë³´ í™•ì¸
docker inspect stt-engine:cuda129-rhel89-v1.2 | jq '.Config.Env[] | select(startswith("LD_"))'
```

---

## ğŸ“¦ Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ (50~90ë¶„)

### ì¤‘ìš”: build-server-image.sh ì™„ë£Œ í›„ ì‹¤í–‰

build-server-image.shê°€ ì™„ë£Œë˜ì–´ì•¼ Docker ì´ë¯¸ì§€ê°€ ì¤€ë¹„ë˜ë¯€ë¡œ, ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

### ë°©ë²•: ëª¨ë¸ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

```bash
cd ~/stt_engine

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
bash scripts/build-server-models.sh

# ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ:
# 1ï¸âƒ£ Python í™˜ê²½ ì„¤ì •
#    - pip ì„¤ì¹˜ í™•ì¸ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)
#    - í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)
#    - ëˆ„ë½ëœ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
#
# 2ï¸âƒ£ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
#    - ê¸°ì¡´ ëª¨ë¸ í™•ì¸ (ìˆìœ¼ë©´ ì¬ì‚¬ìš© ì—¬ë¶€ ë¬¼ì–´ë´„)
#    - Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
#    - CTranslate2 í¬ë§· ë³€í™˜
#
# 3ï¸âƒ£ ëª¨ë¸ êµ¬ì¡° ê²€ì¦
#    - í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
#
# 4ï¸âƒ£ Docker ì»¨í…Œì´ë„ˆ í…ŒìŠ¤íŠ¸
#    - CUDA & PyTorch ê²€ì¦
#    - Faster-Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸
#    - OpenAI Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸
```

### ì˜ˆìƒ ì†Œìš”ì‹œê°„ ë¶„ì„

```
Step 5-0: Python í™˜ê²½ ì„¤ì •
  - ì²˜ìŒ: 5~15ë¶„ (pip, PyTorch ë“± ì„¤ì¹˜)
  - ì¬ì‹¤í–‰: 0~1ë¶„ (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ê±´ë„ˆëœ€)

Step 5-1: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
  - ì²˜ìŒ: 20~30ë¶„ (Hugging Face ë‹¤ìš´ë¡œë“œ 10~15ë¶„ + ë³€í™˜ 5~10ë¶„)
  - ì¬ì‹¤í–‰: 0~1ë¶„ (ëª¨ë¸ ì¬ì‚¬ìš© ì„ íƒ)

Step 5-2: ëª¨ë¸ êµ¬ì¡° ê²€ì¦
  - 1~2ë¶„

Step 5-3: Docker í…ŒìŠ¤íŠ¸
  - 20~30ë¶„

ì´ ì²˜ìŒ: 50~90ë¶„
ì¬ì‹¤í–‰: 0~3ë¶„ (ëª¨ë“  ë‹¨ê³„ ê±´ë„ˆëœ€)
```

---

### âš ï¸ ì´ì „ ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª… (ì°¸ê³ ìš©)

# ì˜ˆìƒ ì¶œë ¥:
# ========================================================
# ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„
# ========================================================
# 
# ğŸ“Œ Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# âœ… ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ
#
# ğŸ“Œ Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# â³ openai/whisper-large-v3-turbo ë‹¤ìš´ë¡œë“œ ì¤‘...
# âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ì•½ 10-15ë¶„ ì†Œìš”)
#
# ğŸ“Œ Step 3: ëª¨ë¸ íŒŒì¼ ê²€ì¦
# âœ… config.json ê²€ì¦ ì™„ë£Œ
# âœ… pytorch_model.bin ê²€ì¦ ì™„ë£Œ
# âœ… tokenizer.json ê²€ì¦ ì™„ë£Œ
#
# ğŸ“Œ Step 4: CTranslate2 í¬ë§· ë³€í™˜
# â³ CTranslate2 ë³€í™˜ ì¤‘... (ì•½ 5~10ë¶„ ì†Œìš”)
# âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ
#
# ğŸ“Œ Step 5: ëª¨ë¸ êµ¬ì¡° ê²€ì¦
# âœ… ctranslate2_model êµ¬ì¡° í™•ì¸
#    - config.json âœ“
#    - model.bin âœ“
#    - vocabulary.json âœ“
#
# âœ… ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!
```

**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 25~45ë¶„ (ëª¨ë¸ í¬ê¸° ë‹¤ìš´ë¡œë“œ: 10~15ë¶„ + CTranslate2 ë³€í™˜: 5~10ë¶„)

### 5-2. ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
du -sh models/
du -sh models/openai_whisper-large-v3-turbo/
du -sh models/ctranslate2_model/

# ì˜ˆìƒ:
# 2.5G  models/
# 1.6G  models/openai_whisper-large-v3-turbo/
# 0.9G  models/ctranslate2_model/

# íŒŒì¼ í™•ì¸
find models/ -type f -name "*.json" -o -name "*.bin"
```

### 5-3. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (Python)

```bash
python3.11 << 'PYTHON_TEST'
from pathlib import Path

models_base = Path("models")
print("=" * 70)
print("ğŸ” ëª¨ë¸ êµ¬ì¡° ì„¸ë¶€ ê²€ì¦")
print("=" * 70)

# CTranslate2 ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ CTranslate2 ëª¨ë¸")
ct2_model = models_base / "ctranslate2_model"
required_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "vocabulary.json": "í† í¬ë‚˜ì´ì € ì–´íœ˜"
}

for fname, desc in required_files.items():
    fpath = ct2_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:20} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:20} NOT FOUND")

# OpenAI Whisper ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ OpenAI Whisper ëª¨ë¸")
whisper_model = models_base / "openai_whisper-large-v3-turbo"
required_whisper_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "pytorch_model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "tokenizer.json": "í† í¬ë‚˜ì´ì €"
}

for fname, desc in required_whisper_files.items():
    fpath = whisper_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:25} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:25} NOT FOUND")

print("\n" + "=" * 70)
PYTHON_TEST
```

---

## ğŸ§ª Step 6: ë¹Œë“œ ì„œë²„ì—ì„œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (20~30ë¶„)

### 6-1. í…ŒìŠ¤íŠ¸ìš© ì»¨í…Œì´ë„ˆ ì‹œì‘ (ëª¨ë¸ ë§ˆìš´íŠ¸)

```bash
cd ~/stt_engine

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸)
docker run -it \
  --name stt-test-engine \
  -v $(pwd)/models:/app/models \
  -e CUDA_VISIBLE_DEVICES=0 \
  stt-engine:cuda129-rhel89-v1.2 \
  /bin/bash
```

### 6-2. ì»¨í…Œì´ë„ˆ ë‚´ ë§ˆìš´íŠ¸ëœ ëª¨ë¸ í™•ì¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# ë§ˆìš´íŠ¸ í™•ì¸
ls -lh /app/models/
du -sh /app/models/*
```

### 6-3. CUDA ë° PyTorch ê²€ì¦

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import torch
import torchaudio
import os

print("=" * 70)
print("ğŸ” CUDA & PyTorch ê²€ì¦")
print("=" * 70)

print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… torchaudio: {torchaudio.__version__}")
print(f"âœ… CUDA Available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"âœ… CUDA Device: {torch.cuda.get_device_name(0)}")
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.matmul(x, y)
    print(f"âœ… CUDA Matrix Multiplication: Success")

print(f"âœ… LD_LIBRARY_PATH: {bool(os.environ.get('LD_LIBRARY_PATH'))}")
print("=" * 70)

PYTHON_TEST
```

### 6-4. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# Faster-Whisper (CTranslate2 ëª¨ë¸)
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ Faster-Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from faster_whisper import WhisperModel
    
    print("\nâ³ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = WhisperModel(
        "/app/models/ctranslate2_model",
        device="auto",
        compute_type="float32",
        download_root="/opt/app-root/src/.cache",
        local_files_only=True
    )
    
    print("âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST

# OpenAI Whisper (ì›ë³¸ ëª¨ë¸)
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ OpenAI Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
    
    print("\nâ³ Processor ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("â³ Model ë¡œë“œ ì¤‘...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST
```

### 6-5. ì»¨í…Œì´ë„ˆ ì¢…ë£Œ

```bash
# ì»¨í…Œì´ë„ˆì—ì„œ exit ì‹¤í–‰
exit

# ë˜ëŠ” ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ
docker rm stt-test-engine
```

---

## ğŸ’¾ Step 7: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥ (5~10ë¶„)

### 7-1. EC2ì—ì„œ ì´ë¯¸ì§€ì™€ ëª¨ë¸ ì €ì¥

```bash
cd ~/stt_engine

# Docker ì´ë¯¸ì§€ ì €ì¥
mkdir -p ~/build/output
docker save stt-engine:cuda129-rhel89-v1.2 | gzip > ~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz

# ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
ls -lh models/
du -sh models/

# ë¹Œë“œ ë¡œê·¸ ì €ì¥
cp /tmp/build.log ~/build/output/build-$(date +%Y%m%d-%H%M%S).log
cp /tmp/model_download.log ~/build/output/model-$(date +%Y%m%d-%H%M%S).log

# ìµœì¢… íŒŒì¼ í™•ì¸
ls -lh ~/build/output/
```

**ì†Œìš” ì‹œê°„: 5~10ë¶„**

---

## ğŸš¢ Step 8: ìš´ì˜ ì„œë²„ì— ë°°í¬

### 8-1. Macìœ¼ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­ - ë¡œì»¬ ê²€ì¦ìš©)

```bash
# Mac ë¡œì»¬ í„°ë¯¸ë„:
scp -i your-key.pem ec2-user@<ec2-ip>:~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz \
    ~/Downloads/

# íŒŒì¼ í™•ì¸
ls -lh ~/Downloads/stt-engine-cuda129-rhel89-v1.2.tar.gz
```

**ì†Œìš” ì‹œê°„: 2~5ë¶„ (ë„¤íŠ¸ì›Œí¬ì— ë”°ë¼)**

### 8-2. EC2 ë¹Œë“œ ì„œë²„ì—ì„œ ìš´ì˜ ì„œë²„ë¡œ ì§ì ‘ ì „ì†¡ (ê¶Œì¥)

```bash
# EC2 ë¹Œë“œ ì„œë²„ì—ì„œ:
# 1. Docker ì´ë¯¸ì§€ ë¡œë“œ
scp -i your-key.pem \
  ~/build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz \
  deploy-user@production-server:/tmp/

# 2. ëª¨ë¸ ë””ë ‰í† ë¦¬ ì „ì†¡ (ëŒ€ìš©ëŸ‰ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦¼)
scp -r -i your-key.pem \
  ~/stt_engine/models \
  deploy-user@production-server:/path/to/deployment/
```

### 8-3. ìš´ì˜ ì„œë²„ì—ì„œ ë¡œë“œ

```bash
# RHEL 8.9 ìš´ì˜ ì„œë²„:
cd /tmp

# 1. Docker ì´ë¯¸ì§€ ë¡œë“œ
docker load < stt-engine-cuda129-rhel89-v1.2.tar.gz

# 2. ì´ë¯¸ì§€ í™•ì¸
docker images | grep stt-engine
# ì¶œë ¥: stt-engine  cuda129-rhel89-v1.2  <image-id>  7.3GB
```

---

## âœ… Step 9: ì´ë¯¸ì§€ ê²€ì¦ (ìš´ì˜ ì„œë²„)

### 9-1. PyTorch/CUDA ê²€ì¦
```bash
docker run --rm stt-engine:cuda129-rhel89-v1.2 python3.11 -c "
import torch
print(f'âœ… PyTorch: {torch.__version__}')
print(f'âœ… CUDA Available: {torch.cuda.is_available()}')
print(f'âœ… cuDNN: OK')
"

# ì˜ˆìƒ ì¶œë ¥:
# âœ… PyTorch: 2.6.0
# âœ… CUDA Available: True
# âœ… cuDNN: OK
```

### 9-2. Whisper ê²€ì¦
```bash
docker run --rm stt-engine:cuda129-rhel89-v1.2 python3.11 -c "
try:
    import faster_whisper
    print('âœ… faster-whisper: ë¡œë“œë¨')
except:
    print('âš ï¸  faster-whisper: ë¯¸ì‚¬ìš©')
    
try:
    from transformers import AutoModelForSpeechSeq2Seq
    print('âœ… transformers: ë¡œë“œë¨')
except:
    print('âš ï¸  transformers: ë¯¸ì‚¬ìš©')
"
```

### 9-3. ëª¨ë¸ ë§ˆìš´íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸í•˜ì—¬ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -it \
  --name stt-final-test \
  -v /path/to/models:/app/models \
  -e CUDA_VISIBLE_DEVICES=0 \
  stt-engine:cuda129-rhel89-v1.2 \
  /bin/bash

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("âœ… ìµœì¢… í†µí•© ê²€ì¦")
print("=" * 70)

# Faster-Whisper ë¡œë“œ í™•ì¸
try:
    from faster_whisper import WhisperModel
    model = WhisperModel(
        "/app/models/ctranslate2_model",
        device="auto",
        compute_type="float32",
        local_files_only=True
    )
    print("\nâœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"\nâŒ Faster-Whisper ì˜¤ë¥˜: {e}")

# OpenAI Whisper ë¡œë“œ í™•ì¸
try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
    processor = AutoProcessor.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True
    )
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True
    )
    print("âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ OpenAI Whisper ì˜¤ë¥˜: {e}")

print("\n" + "=" * 70)
print("ğŸ‰ ëª¨ë“  ê²€ì¦ ì™„ë£Œ!")
print("=" * 70)
PYTHON_TEST

# ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
exit
```

---

## ğŸ“Š ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ì „ì²´)

### ì²˜ìŒ ë¹Œë“œ ì‹œ (ì „ì²´ ì²˜ìŒë¶€í„° ëê¹Œì§€)

| ë‹¨ê³„ | ì˜ˆìƒ ì‹œê°„ |
|------|----------|
| Step 1: EC2 ìƒì„± | 2ë¶„ |
| Step 2: í™˜ê²½ ì„¤ì • | 5ë¶„ |
| Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡  | 2ë¶„ |
| Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ | 20~40ë¶„ |
| Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ê²€ì¦ | 50~90ë¶„ |
| **ì´ ì†Œìš” ì‹œê°„** | **80~140ë¶„ (1.3~2.3ì‹œê°„)** |

### ì¬ì‹¤í–‰ ì‹œ (ì´ë¯¸ì§€/ëª¨ë¸ì´ ì´ë¯¸ ìˆì„ ë•Œ)

| ë‹¨ê³„ | ì˜ˆìƒ ì‹œê°„ |
|------|----------|
| Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ | 0~1ë¶„ (ê¸°ì¡´ ì´ë¯¸ì§€ ì¬ì‚¬ìš©) |
| Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ê²€ì¦ | 0~1ë¶„ (ê¸°ì¡´ ëª¨ë¸ ì¬ì‚¬ìš©) |
| **ì´ ì†Œìš” ì‹œê°„** | **0~2ë¶„** |

---

## ğŸ“ ë‘ ê°€ì§€ ë¹Œë“œ ë°©ì‹

### ë°©ì‹ A: ë¶„ë¦¬ëœ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥ - íš¨ìœ¨ì )

**ì¥ì **: 
- ì´ë¯¸ì§€ì™€ ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ ê´€ë¦¬
- ë¶ˆí•„ìš”í•œ ì¬êµ¬ì„± ì œê±°
- ë¹ ë¥¸ ì¬ì‹¤í–‰

```bash
# Step 1: Docker ì´ë¯¸ì§€ ë¹Œë“œ (í•œ ë²ˆë§Œ í•„ìš”)
bash scripts/build-server-image.sh   # 20~40ë¶„

# Step 2: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ê²€ì¦ (ë³„ë„ë¡œ ì‹¤í–‰ ê°€ëŠ¥)
bash scripts/build-server-models.sh  # 50~90ë¶„
```

### ë°©ì‹ B: í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (í•œë²ˆì— ëª¨ë“  ì‘ì—…)

**ì¥ì **:
- í•œ ë²ˆì˜ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ ì™„ë£Œ
- ê°„ë‹¨í•œ ì‚¬ìš©

```bash
# ì „ì²´ ë¹Œë“œ (ì´ë¯¸ì§€ + ëª¨ë¸)
bash scripts/build-server-complete.sh  # 80~140ë¶„
```

---

## âœ… ìµœì¢… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë“  í•­ëª©ì´ âœ… ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤:

```
Build ì„œë²„ (EC2):
  âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì„±ê³µ (7.3GB)
  âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (2.5GB)
  âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ (model.bin ìƒì„±)
  âœ… ëª¨ë“  ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì™„ë£Œ
  âœ… í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ëª¨ë¸ ë¡œë“œ ì„±ê³µ

Production ì„œë²„ (RHEL 8.9):
  âœ… Docker ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ
  âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸ ê°€ëŠ¥
  âœ… PyTorch/CUDA ì •ìƒ ì‘ë™
  âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ
  âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ
  âœ… ìµœì¢… í†µí•© ê²€ì¦ í†µê³¼
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (ìš´ì˜ ì„œë²„)

ë¹Œë“œ ë° ê²€ì¦ ì™„ë£Œ í›„:

1. **STT API ì„œë²„ ì‹¤í–‰**
   ```bash
   docker run -d \
     --name stt-api \
     --gpus all \
     -p 5000:5000 \
     -v /path/to/models:/app/models \
     -e CUDA_VISIBLE_DEVICES=0 \
     stt-engine:cuda129-rhel89-v1.2
   ```

2. **í—¬ìŠ¤ ì²´í¬**
   ```bash
   sleep 10
   curl http://localhost:5000/health
   # ì˜ˆìƒ: {"status":"ok","backend":"faster-whisper"}
   ```

3. **STT ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸**
   ```bash
   curl -X POST http://localhost:5000/transcribe \
     -F "file=@/path/to/audio.wav"
   ```

---

## ğŸ“ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

| ë¬¸ì œ | í•´ê²°ì±… |
|------|--------|
| `/usr/bin/python3.11: No module named pip` | `sudo yum install -y python3.11-pip` ë˜ëŠ” `python3.11 -m ensurepip --upgrade` |
| `ModuleNotFoundError: No module named 'urllib3'` | Step 5-0 ì˜ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ í™•ì¸ |
| `ModuleNotFoundError: torch` | PyTorch 2.6.0 ì„¤ì¹˜ í™•ì¸: `python3.11 -m pip install torch==2.6.0` |
| `ModuleNotFoundError: transformers` | `python3.11 -m pip install transformers` |
| `ModuleNotFoundError: ctranslate2` | `python3.11 -m pip install ctranslate2` |
| Docker ë¹Œë“œ ì‹¤íŒ¨ | ì¸í„°ë„· ì—°ê²° í™•ì¸, `grep -i error /tmp/build.log` í™•ì¸ |
| ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ | HuggingFace ì ‘ê·¼ì„± í™•ì¸, VPN ì‚¬ìš©, í”„ë¡ì‹œ ì„¤ì • |
| CUDA ì¸ì‹ ì•ˆë¨ | ìš´ì˜ ì„œë²„ì˜ `nvidia-smi` í™•ì¸ |
| ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ | ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸, `/app/models` ë§ˆìš´íŠ¸ í™•ì¸ |
| ë””ìŠ¤í¬ ë¶€ì¡± | EC2: `df -h` í™•ì¸, ìš´ì˜ ì„œë²„: ì¶©ë¶„í•œ ìŠ¤í† ë¦¬ì§€ í™•ë³´ |

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì™„ì „ ë²„ì „)

```
[ ] RHEL 8.9 ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ
    - OS: 8.9
    - glibc: 2.28
    - Python: 3.11.5
    - CUDA: 12.9
    - NVIDIA Driver: 575.57.08

[ ] AWS EC2 RHEL 8.9 ìƒì„±
    - t3.large ì´ìƒ (ë˜ëŠ” t3.xlarge ê¶Œì¥)
    - 100GB ì´ìƒ ìŠ¤í† ë¦¬ì§€ (Docker 7GB + ëª¨ë¸ 2.5GB + ì—¬ìœ )
    - Security Group ì„¤ì •

[ ] EC2 í™˜ê²½ ì„¤ì •
    - Docker ì„¤ì¹˜ ë° ì‹¤í–‰
    - Git ì„¤ì¹˜
    - ì‚¬ìš©ì ê¶Œí•œ ì„¤ì •

[ ] Step 3: ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
    - git clone ì™„ë£Œ
    - ì½”ë“œ ìµœì‹ í™” í™•ì¸

[ ] Step 4: Docker ì´ë¯¸ì§€ ë¹Œë“œ
    - ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    - ë¹Œë“œ ì™„ë£Œ (7.3GB)
    - LD_LIBRARY_PATH ì„¤ì • í™•ì¸

[ ] Step 5: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
    - download_model_hf.py ì‹¤í–‰ ì™„ë£Œ
    - models/ ë””ë ‰í† ë¦¬ (2.5GB) ìƒì„±
    - CTranslate2 ë³€í™˜ ì™„ë£Œ
    - ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì™„ë£Œ

[ ] Step 6: ë¹Œë“œ ì„œë²„ì—ì„œ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    - í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
    - CUDA ë° PyTorch ê²€ì¦
    - Faster-Whisper ë¡œë“œ ì„±ê³µ
    - OpenAI Whisper ë¡œë“œ ì„±ê³µ

[ ] Step 7: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥
    - Docker ì´ë¯¸ì§€ ì €ì¥ (tar.gz)
    - ë¹Œë“œ ë¡œê·¸ ì €ì¥
    - ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸

[ ] Step 8: ìš´ì˜ ì„œë²„ ë°°í¬
    - ì´ë¯¸ì§€ ë° ëª¨ë¸ ì „ì†¡ ì™„ë£Œ
    - ìš´ì˜ ì„œë²„ ë¡œë“œ ì™„ë£Œ

[ ] Step 9: ì´ë¯¸ì§€ ê²€ì¦
    - PyTorch/CUDA ê²€ì¦
    - Whisper ê²€ì¦
    - ëª¨ë¸ ë§ˆìš´íŠ¸ í…ŒìŠ¤íŠ¸
    - í†µí•© ê²€ì¦ ì™„ë£Œ
```

---

## ğŸ”§ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ê°€ì´ë“œ

### 1ï¸âƒ£ build-server-image.sh (Docker ì´ë¯¸ì§€ ë¹Œë“œ)

**ëª©ì **: RHEL 8.9 í˜¸í™˜ Docker ì´ë¯¸ì§€ë§Œ ë¹Œë“œ

```bash
bash scripts/build-server-image.sh
```

**ë™ì‘**:
```
Step 0: ì‚¬ì „ í™•ì¸
  âœ… Docker ì„¤ì¹˜ í™•ì¸
  âœ… git ì„¤ì¹˜ í™•ì¸
  âœ… ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (100GB+)
  âœ… ì¸í„°ë„· ì—°ê²° í™•ì¸

ê¸°ì¡´ ì´ë¯¸ì§€ í™•ì¸
  - ì´ë¯¸ì§€ ìˆìŒ â†’ ì¬ì‚¬ìš©í• ì§€ ë¬¼ì–´ë´„
  - ì´ë¯¸ì§€ ì—†ìŒ â†’ ìƒˆë¡œ ë¹Œë“œ

Step 1: Docker ì´ë¯¸ì§€ ë¹Œë“œ (20~40ë¶„)
  â†’ stt-engine:cuda129-rhel89-v1.2 (7.3GB)

Step 2: ì´ë¯¸ì§€ ì €ì¥
  â†’ build/output/stt-engine-cuda129-rhel89-v1.2.tar.gz
```

**ì¶œë ¥ ë¡œê·¸**:
```
/tmp/build-image-YYYYMMDD-HHMMSS.log
build/output/BUILD_IMAGE_INFO.txt
```

**ì–¸ì œ ì‚¬ìš©**:
- Docker ì´ë¯¸ì§€ ì²˜ìŒ ë¹Œë“œ
- Dockerfile ë³€ê²½ í›„ ì¬ë¹Œë“œ
- ì´ë¯¸ì§€ ë²„ì „ ì—…ê·¸ë ˆì´ë“œ

---

### 2ï¸âƒ£ build-server-models.sh (ëª¨ë¸ ì¤€ë¹„)

**ëª©ì **: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, ë³€í™˜, ê²€ì¦

```bash
bash scripts/build-server-models.sh
```

**ë™ì‘**:
```
Step 0: ì‚¬ì „ í™•ì¸
  âœ… Docker ì„¤ì¹˜ í™•ì¸
  âœ… Docker ì´ë¯¸ì§€ í™•ì¸ (stt-engine:cuda129-rhel89-v1.2)
  âœ… ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (50GB+)
  âœ… ì¸í„°ë„· ì—°ê²° í™•ì¸

Step 1: Python í™˜ê²½ ì„¤ì • (ì²˜ìŒ: 5~15ë¶„, ì¬ì‹¤í–‰: 0~1ë¶„)
  1. pip ì„¤ì¹˜ í™•ì¸ (ì—†ìœ¼ë©´ ì„¤ì¹˜)
  2. ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ í™•ì¸ (ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)
  3. ëˆ„ë½ëœ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
  â†’ PyTorch 2.6.0, transformers, ctranslate2, etc.

Step 2: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜ (ì²˜ìŒ: 20~30ë¶„, ì¬ì‹¤í–‰: 0~1ë¶„)
  1. ê¸°ì¡´ ëª¨ë¸ í™•ì¸
     - ìˆìŒ â†’ ì¬ì‚¬ìš©í• ì§€ ë¬¼ì–´ë´„
     - ì—†ìŒ â†’ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
  2. Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (10~15ë¶„)
  3. CTranslate2 í¬ë§· ë³€í™˜ (5~10ë¶„)
  â†’ models/ctranslate2_model/
  â†’ models/openai_whisper-large-v3-turbo/

Step 3: ëª¨ë¸ êµ¬ì¡° ê²€ì¦ (1~2ë¶„)
  âœ… CTranslate2 ëª¨ë¸ íŒŒì¼ í™•ì¸
  âœ… OpenAI Whisper ëª¨ë¸ íŒŒì¼ í™•ì¸

Step 4: Docker í…ŒìŠ¤íŠ¸ (20~30ë¶„)
  1. í…ŒìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
  2. CUDA & PyTorch ê²€ì¦
  3. Faster-Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸
  4. OpenAI Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸
  5. ì»¨í…Œì´ë„ˆ ì¢…ë£Œ
```

**ì¶œë ¥ ë¡œê·¸**:
```
/tmp/build-models-YYYYMMDD-HHMMSS.log
```

**ì–¸ì œ ì‚¬ìš©**:
- ëª¨ë¸ ì²˜ìŒ ë‹¤ìš´ë¡œë“œ
- ëª¨ë¸ ì¬ê²€ì¦ í•„ìš”
- ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½

---

### 3ï¸âƒ£ build-server-complete.sh (ì „ì²´ ë¹Œë“œ)

**ëª©ì **: Docker ì´ë¯¸ì§€ + ëª¨ë¸ì„ ëª¨ë‘ êµ¬ì„±

```bash
bash scripts/build-server-complete.sh
```

**ë™ì‘**: ìœ„ì˜ ë‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰
- Step 1-2: build-server-image.sh ì‹¤í–‰
- Step 3-6: build-server-models.sh ì‹¤í–‰

**ì–¸ì œ ì‚¬ìš©**:
- ì²˜ìŒ ë¹Œë“œ ì„œë²„ êµ¬ì„±
- í•œ ë²ˆì— ëª¨ë“  ì‘ì—…ì„ ëë‚´ê³  ì‹¶ì„ ë•Œ

---

## ğŸ“‹ ë¹Œë“œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì´ë“œ

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì²˜ìŒ ë¹Œë“œ

```bash
# 1ë‹¨ê³„: Docker ì´ë¯¸ì§€ ë¹Œë“œ
bash scripts/build-server-image.sh
# ì˜ˆìƒ: 20~40ë¶„

# ë˜ëŠ” 2ë‹¨ê³„ë¥¼ ë°”ë¡œ ì‹¤í–‰í•´ë„ ë¨ (2ë‹¨ê³„ê°€ ì´ë¯¸ì§€ í™•ì¸)
# 2ë‹¨ê³„: ëª¨ë¸ ì¤€ë¹„
bash scripts/build-server-models.sh
# ì˜ˆìƒ: 50~90ë¶„

# ì´: 70~130ë¶„
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì´ë¯¸ì§€ëŠ” ìˆê³  ëª¨ë¸ë§Œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (1ë‹¨ê³„ ê±´ë„ˆëœ€)
bash scripts/build-server-models.sh
# ì˜ˆìƒ: 50~90ë¶„

# ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ:
# 1. Python íŒ¨í‚¤ì§€ í™•ì¸ (ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)
# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# 3. í…ŒìŠ¤íŠ¸
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ì¬ê²€ì¦ (ëª¨ë“  ê²ƒì´ ì´ë¯¸ ìˆìŒ)

```bash
# ê¸°ì¡´ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ë§Œ ì¬ì‹¤í–‰
bash scripts/build-server-models.sh
# í”„ë¡¬í”„íŠ¸ì—ì„œ "ì‚¬ìš©" ì„ íƒ (ê¸°ì¡´ ëª¨ë¸ ì¬ì‚¬ìš©)
# ì˜ˆìƒ: 20~30ë¶„ (í…ŒìŠ¤íŠ¸ë§Œ)

# ë˜ëŠ” Python í™˜ê²½ ì¬êµ¬ì„± í›„ í…ŒìŠ¤íŠ¸
# ì˜ˆìƒ: 30~45ë¶„ (Python ì¬ì„¤ì¹˜ + í…ŒìŠ¤íŠ¸)
```

---

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸ ì •ë¦¬

### Build ì„œë²„ (AWS EC2 RHEL 8.9)ì—ì„œ:

1. **Step 1~3**: AWS EC2 ì¤€ë¹„ ë° ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
   - âœ… ì•½ 10ë¶„

2. **Step 4**: Docker ì´ë¯¸ì§€ ë¹Œë“œ
   - âœ… ì•½ 20~40ë¶„
   - ê²°ê³¼: `stt-engine:cuda129-rhel89-v1.2` (7.3GB)

3. **Step 5**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + CTranslate2 ë³€í™˜ (NEW)
   - âœ… ì•½ 25~45ë¶„
   - ê²°ê³¼: `models/` ë””ë ‰í† ë¦¬ (2.5GB)
     - `openai_whisper-large-v3-turbo/` (1.6GB)
     - `ctranslate2_model/` (0.9GB)

4. **Step 6**: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (NEW)
   - âœ… ì•½ 20~30ë¶„
   - Faster-Whisper + OpenAI Whisper ëª¨ë‘ í…ŒìŠ¤íŠ¸

5. **Step 7**: ì´ë¯¸ì§€ ë° ëª¨ë¸ ì €ì¥
   - âœ… ì•½ 5~10ë¶„

6. **Step 8~9**: ìš´ì˜ ì„œë²„ ë°°í¬ ë° ê²€ì¦
   - âœ… ì•½ 15~25ë¶„

### ìµœì¢… ê²°ê³¼:
- âœ… Production Ready Docker ì´ë¯¸ì§€
- âœ… ì™„ë²½í•˜ê²Œ ê²€ì¦ëœ ëª¨ë¸ ì„¸íŠ¸
- âœ… RHEL 8.9 100% í˜¸í™˜ì„± ë³´ì¥

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026ë…„ 2ì›” 7ì¼
