# Whisper STT API - ìŒì„± ì¸ì‹ ì—”ì§„

OpenAIì˜ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” REST API ì„œë²„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **Whisper ê¸°ë°˜ STT**: OpenAIì˜ whisper-large-v3-turbo ëª¨ë¸ ì‚¬ìš©
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›
- **GPU ìµœì í™”**: CUDA GPUë¥¼ í™œìš©í•œ ë¹ ë¥¸ ì²˜ë¦¬
- **Docker í™˜ê²½**: ì»¨í…Œì´ë„ˆí™”ëœ ê°„ë‹¨í•œ ë°°í¬
- **FastAPI ì„œë²„**: REST APIë¥¼ í†µí•œ ì‰¬ìš´ ì ‘ê·¼
- **faster-whisper**: CTransformers ê¸°ë°˜ìœ¼ë¡œ 3-4ë°° ë¹ ë¥¸ ì¶”ë¡ 

**ì°¸ê³ **: vLLM í…ìŠ¤íŠ¸ ì²˜ë¦¬ëŠ” ë³„ë„ë¡œ ë°°í¬ëœ ì„œë²„ì—ì„œ ë‹´ë‹¹í•©ë‹ˆë‹¤.

## ğŸ” faster-whisperì™€ Whisper ëª¨ë¸ì˜ ê´€ê³„

### Whisper Large Turbo v3 (ëª¨ë¸)
- **ì—­í• **: OpenAIì—ì„œ ì œê³µí•˜ëŠ” í•™ìŠµëœ AI ëª¨ë¸
- **íŒŒë¼ë¯¸í„°**: ~1.5B (ì•½ 15ì–µ ê°œ)
- **ê¸°ëŠ¥**: ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì‹ ê²½ë§
- **ì €ì¥ìœ„ì¹˜**: `models/openai_whisper-large-v3-turbo/` ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨
- **íŒŒì¼ í¬ê¸°**: ~2.7GB (ëª¨ë¸ ê°€ì¤‘ì¹˜)

### faster-whisper (ì¶”ë¡  ì—”ì§„)
- **ì—­í• **: Whisper ëª¨ë¸ì„ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ëŠ” ìµœì í™”ëœ ì—”ì§„
- **ê¸°ìˆ **: CTranslate2 + ONNX Runtime ì‚¬ìš©
- **ì„±ëŠ¥**: 3-4ë°° ë¹ ë¥¸ ì¶”ë¡  ì†ë„, 30-40% ì ì€ VRAM ì‚¬ìš©
- **í˜¸í™˜ì„±**: Whisper ëª¨ë¸ê³¼ 100% í˜¸í™˜

### ì‹¤í–‰ íë¦„
```
ìŒì„± íŒŒì¼ 
    â†“
faster-whisper ì—”ì§„ (CTranslate2 ìµœì í™”)
    â†“
Whisper Large Turbo v3 ëª¨ë¸ (ê°€ì¤‘ì¹˜ ë¡œë“œ)
    â†“
í…ìŠ¤íŠ¸ ì¶œë ¥
```

### ê¸°ìˆ ì  ì°¨ì´

| í•­ëª© | OpenAI Whisper | faster-whisper |
|------|---|---|
| **ë¼ì´ë¸ŒëŸ¬ë¦¬** | Transformers (PyTorch) | CTranslate2 + ONNX |
| **ì¶”ë¡  ì†ë„** | ëŠë¦¼ (1ë°°) | ë¹ ë¦„ (3-4ë°°) âš¡ |
| **VRAM ì‚¬ìš©** | 4-6GB | 2.5-3.5GB |
| **ëª¨ë¸ í˜•ì‹** | PyTorch `.pt` | ONNX `.onnx` |
| **ì •í™•ë„** | 100% | 100% (ë™ì¼) |
| **ì˜¤ë””ì˜¤ ì²˜ë¦¬** | ìˆ˜ë™ (librosa) | ìë™ ë‚´ì¥ |

### ì™œ faster-whisperë¥¼ ì‚¬ìš©í•˜ë‚˜?
âœ… **í”„ë¡œë•ì…˜ í™˜ê²½ì— ìµœì í™”**: ë” ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„  
âœ… **ë¹„ìš© ì ˆê°**: ë‚®ì€ VRAMìœ¼ë¡œ ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥  
âœ… **ì—ë„ˆì§€ íš¨ìœ¨**: ì „ë ¥ ì†Œë¹„ ê°ì†Œ  
âœ… **í˜¸í™˜ì„±**: ê¸°ì¡´ Whisper ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©  
âœ… **ì˜¤í”„ë¼ì¸ ìµœì í™”**: RHEL 8.9 ì„œë²„ì— ì í•©

## ğŸ“‹ ì¤€ë¹„ ì‚¬í•­

- Python 3.11+
- CUDA 12.4+ (GPU ì‚¬ìš© ì‹œ, CUDA 11.8 í˜¸í™˜ë„ ê°€ëŠ¥)
- Docker & Docker Compose
- ìµœì†Œ 4GB VRAM (ê¶Œì¥ 8GB ì´ìƒ)

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Docker)

### Docker Compose ì‚¬ìš© (ê¶Œì¥)

### 1ë‹¨ê³„: ì´ë¯¸ì§€ ë¹Œë“œ
```bash
docker build -f docker/Dockerfile.gpu -t whisper-stt:latest .
```

### 2ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```bash
docker-compose -f docker/docker-compose.yml up -d
```

### 3ë‹¨ê³„: ìƒíƒœ í™•ì¸
```bash
curl http://localhost:8003/health
```

### Docker ì§ì ‘ ì‚¬ìš© (docker-compose ì—†ì´)

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -f docker/Dockerfile.gpu -t whisper-stt:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (GPU ì‚¬ìš©)
docker run -d \
  --name whisper-stt \
  --gpus all \
  -p 8003:8003 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/audio:/app/audio \
  -v $(pwd)/logs:/app/logs \
  -e WHISPER_DEVICE=cuda \
  -e SERVER_HOST=0.0.0.0 \
  -e SERVER_PORT=8003 \
  whisper-stt:latest

# ìƒíƒœ í™•ì¸
curl http://localhost:8003/health

# ë¡œê·¸ í™•ì¸
docker logs -f whisper-stt

# ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker stop whisper-stt
docker rm whisper-stt
```

### 4ë‹¨ê³„: ìŒì„± ì¸ì‹ í…ŒìŠ¤íŠ¸
```bash
curl -X POST -F "file=@audio.wav" -F "language=ko" \
  http://localhost:8003/transcribe
```

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8003/health
```

### ìŒì„± ì¸ì‹ (STT)
```bash
curl -X POST \
  -F "file=@audio.wav" \
  -F "language=ko" \
  http://localhost:8003/transcribe
```

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "success": true,
  "text": "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì¸ê³µì§€ëŠ¥ ìŒì„±ì¸ì‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
  "language": "ko"
}
```

## ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (vLLM)

**ì°¸ê³ **: STTë¡œ ë³€í™˜í•œ í…ìŠ¤íŠ¸ë¥¼ ë³„ë„ë¡œ ë°°í¬ëœ vLLM ì„œë²„ë¡œ ë³´ë‚´ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```bash
# 1ë‹¨ê³„: Whisper STTë¡œ ìŒì„± ì¸ì‹
curl -X POST http://localhost:8003/transcribe \
  -F "file=@audio.wav" \
  -F "language=ko"

# ì‘ë‹µ: {"success": true, "text": "...", "language": "ko"}

# 2ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ vLLM ì„œë²„ë¡œ ì „ì†¡
curl -X POST http://your-vllm-server:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "prompt": "ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸",
    "max_tokens": 100
  }'
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
stt_engine/
â”œâ”€â”€ download_model.py          # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ stt_engine.py             # STT í•µì‹¬ ëª¨ë“ˆ
â”œâ”€â”€ vllm_client.py            # vLLM í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ api_server.py             # FastAPI ì„œë²„
â”œâ”€â”€ Dockerfile                # Docker ì´ë¯¸ì§€ ì •ì˜
â”œâ”€â”€ docker-compose.yml        # Docker Compose ì„¤ì •
â”œâ”€â”€ requirements.txt          # Python ì˜ì¡´ì„±
â”œâ”€â”€ .env.example              # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ
â”œâ”€â”€ README.md                 # ì´ íŒŒì¼
â”œâ”€â”€ models/                   # Whisper ëª¨ë¸ ì €ì¥ ìœ„ì¹˜
â”œâ”€â”€ audio/                    # í…ŒìŠ¤íŠ¸ ìŒì„± íŒŒì¼ ìœ„ì¹˜
â””â”€â”€ logs/                     # ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### GPU ì‚¬ìš© ì„¤ì •

#### docker-compose.ymlì—ì„œ GPU í™œì„±í™”:
```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

#### í™˜ê²½ ë³€ìˆ˜ì—ì„œ GPU ì„¤ì •:
```bash
WHISPER_DEVICE=cuda
```

### vLLM ëª¨ë¸ ë³€ê²½

docker-compose.ymlì—ì„œ ëª¨ë¸ ì´ë¦„ ë³€ê²½:
```yaml
environment:
  - MODEL_NAME=meta-llama/Llama-2-13b-hf  # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ë¡œì»¬ í…ŒìŠ¤íŠ¸
```bash
# STT ì—”ì§„ í…ŒìŠ¤íŠ¸
python stt_engine.py

# vLLM ì—°ê²° í…ŒìŠ¤íŠ¸
python vllm_client.py
```

### Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker-compose exec stt-engine python stt_engine.py
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸
```bash
# ìµœê·¼ 100ì¤„ ë¡œê·¸
tail -100f logs/*.log

# íŠ¹ì • ë¡œê·¸ ë³´ê¸°
docker-compose logs -f stt-engine
```

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
docker stats stt-engine vllm-server
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**: ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ì´ ìƒë‹¹íˆ í¼ (ìˆ˜ GB)ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **GPU ë©”ëª¨ë¦¬**: GPUë¥¼ ì‚¬ìš©í•  ê²½ìš° ì¶©ë¶„í•œ VRAMì´ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 8GB ê¶Œì¥).
3. **vLLM ì„œë²„**: STTì™€ vLLMì„ í•¨ê»˜ ì‚¬ìš©í•˜ë ¤ë©´ vLLM ì„œë²„ê°€ ë°˜ë“œì‹œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# Hugging Face í† í° ì„¤ì •
export HUGGINGFACE_HUB_TOKEN=your_token_here
python download_model.py
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# CPU ëª¨ë“œë¡œ ì‹¤í–‰
export WHISPER_DEVICE=cpu
python stt_engine.py
```

### vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
curl http://localhost:8000/health

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart vllm-server
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ğŸ‘¥ ê¸°ì—¬

ì´ìŠˆ ë° í’€ ë¦¬í€˜ìŠ¤íŠ¸ëŠ” ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!
