# âš ï¸ âŒ [2026-02-03 v1.0-DEPRECATED] PyTorch CUDA: None ë¬¸ì œ ë¶„ì„ (íê¸°ë¨)

**ë²„ì „**: v1.0-DEPRECATED (íê¸°ë¨)  
**ë°œê²¬ì¼**: 2026-02-03  
**ìƒíƒœ**: âŒ **ì‚¬ìš© ê¸ˆì§€** (Docker ì´ë¯¸ì§€ ë‚´ ìˆ˜ì • ë°©ì‹)

---

## ğŸ“‹ ë¬¸ì„œ ìƒíƒœ

| ë²„ì „ | ë‚ ì§œ | ìƒíƒœ | ì„¤ëª… |
|------|------|------|------|
| v1.0-DEPRECATED | 2026-02-03 | âŒ **íê¸°** | Docker ì´ë¯¸ì§€ ë‚´ì—ì„œ CUDA 12.4 wheel ì„¤ì¹˜ ì‹œë„ (ë¶ˆê°€ëŠ¥) |

### âš ï¸ ì´ ë¬¸ì„œë¥¼ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ì´ìœ 
- âŒ Macì—ì„œ CUDA wheel ë‹¤ìš´ë¡œë“œ (ì•„í‚¤í…ì²˜ ë¶ˆì¼ì¹˜)
- âŒ Docker ì´ë¯¸ì§€ ë‚´ì—ì„œ ìˆ˜ì • (ì´ë¯¸ì§€ê°€ ì˜ëª»ë˜ì—ˆìœ¼ë©´ ì‹¤íŒ¨)
- âŒ CPU ë²„ì „ ì„¤ì¹˜ ê°€ëŠ¥ì„±

### âœ… ëŒ€ì‹  ì´ ë¬¸ì„œë¥¼ ì½ìœ¼ì„¸ìš”
ğŸ‘‰ **[CORRECT_FINAL_DEPLOYMENT.md](CORRECT_FINAL_DEPLOYMENT.md)**

---

# âŒ ì´ì „ ë‚´ìš© (ì‘ë™í•˜ì§€ ì•ŠìŒ)

## ğŸ”´ ì¤‘ìš”: PyTorch CUDA: None ë¬¸ì œ ë¶„ì„ ë° í•´ê²°

---

## ğŸ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?

### PyTorch CUDA ë²„ì „ í™•ì¸ ê²°ê³¼

```bash
$ docker run -it stt-engine:linux-x86_64 python3 -c \
  "import torch; print(f'PyTorch CUDA: {torch.version.cuda}')"

# ê²°ê³¼
PyTorch CUDA: None
```

### í•´ì„

| ê²°ê³¼ | ì˜ë¯¸ | GPU ì‚¬ìš© |
|------|------|---------|
| `PyTorch CUDA: 12.1` | CUDA 12.1 ì§€ì› PyTorch | âœ… ê°€ëŠ¥ |
| `PyTorch CUDA: None` | CPU ì „ìš© PyTorch | âŒ ë¶ˆê°€ëŠ¥ |

**í˜„ ìƒí™©**: 
- PyTorchëŠ” ì„¤ì¹˜ë˜ì–´ ìˆìŒ
- **í•˜ì§€ë§Œ CUDA ì§€ì› ì—†ìŒ**
- GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥

---

## ğŸ¤” ì™œ ì´ëŸ° ì¼ì´ ë°œìƒí–ˆì„ê¹Œ?

### ê°€ëŠ¥ì„± 1: wheelsì— CPU ì „ìš© PyTorchê°€ ìˆì—ˆìŒ âœ… í™•ì¸ë¨

```bash
# deployment_package/wheels/ ì— ìˆëŠ” PyTorch wheel í™•ì¸
$ ls deployment_package/wheels/ | grep torch

# ì‹¤ì œ ê²°ê³¼:
# torch-2.1.2+cpu-cp311-cp311-linux_x86_64.whl â† CPU ë²„ì „!
# torchaudio-2.1.2+cpu-cp311-cp311-linux_x86_64.whl â† CPU ë²„ì „!
```

**CPU vs CUDA wheel ì´ë¦„ ë¹„êµ**:
```
CPU ë²„ì „:    torch-2.1.2+cpu-cp311-cp311-linux_x86_64.whl â† í˜„ì¬!
CUDA 12.1:   torch-2.1.2-cp311-cp311-cu121-linux_x86_64.whl
CUDA 12.4:   torch-2.1.2-cp311-cp311-cu124-linux_x86_64.whl
             â†‘ CUDA ë²„ì „ì´ ëª…ì‹œë¨ (ì—†ìœ¼ë©´ CPU ë²„ì „)

í˜„ì¬: torch-2.1.2+cpu-cp311-cp311-linux_x86_64.whl
      â†’ **CPU ì „ìš©** (ì´ê²ƒì´ ë¬¸ì œ!)
```

**í™•ì¸**:
- `+cpu` í‘œê¸° = CPU ì „ìš©
- `cu124` í‘œê¸° = CUDA 12.4 ì§€ì›
- ì•„ë¬´ í‘œê¸° ì—†ìœ¼ë©´ = CPU ë˜ëŠ” í˜¸í™˜ì„± ë¬¸ì œ

### ê°€ëŠ¥ì„± 2: ì›ë˜ ë‹¤ìš´ë¡œë“œí•  ë•Œ --index-urlì„ ì§€ì •í•˜ì§€ ì•Šì•„ì„œ

```bash
# ì˜ëª»ëœ ëª…ë ¹ì–´ (ì¸ë±ìŠ¤ ì§€ì • ì•ˆ í•¨)
pip wheel torch==2.1.2 -w deployment_package/wheels/
# â†’ ìµœì‹  CPU ë²„ì „ ë‹¤ìš´ë¡œë“œ

# ì˜¬ë°”ë¥¸ ëª…ë ¹ì–´
pip wheel torch==2.1.2 --index-url https://download.pytorch.org/whl/cu121 -w deployment_package/wheels/
# â†’ CUDA 12.1 ë²„ì „ ë‹¤ìš´ë¡œë“œ
```

---

## âœ… í•´ê²°ì±…

### í˜„ì¬ ìƒí™©ì—ì„œ ì¦‰ì‹œ í•  ì¼

**í˜„ì¬ ì´ë¯¸ì§€ëŠ” GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, CUDA 12.4 ìƒˆ ì´ë¯¸ì§€ ë¹Œë“œê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.**

```
CPU PyTorch (í˜„ì¬) â†’ GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥ âŒ
       â†“
CUDA 12.4 PyTorch (ìƒˆ ë¹Œë“œ) â†’ GPU ì‚¬ìš© ê°€ëŠ¥ âœ…
```

---

## ğŸš€ CUDA 12.4 ì´ë¯¸ì§€ ë¹Œë“œ (ì •í™•í•œ ëª…ë ¹ì–´)

### Step 1: wheels-cu124 í´ë” ìƒì„±

```bash
cd /Users/a113211/workspace/stt_engine
mkdir -p deployment_package/wheels-cu124
```

### Step 2: CUDA 12.4 PyTorch wheels ë‹¤ìš´ë¡œë“œ (ë§¤ìš° ì¤‘ìš”!)

```bash
# âš ï¸ ë°˜ë“œì‹œ --index-urlì„ í¬í•¨í•  ê²ƒ!

python3 -m pip wheel torch==2.1.2 torchaudio==2.1.2 \
  --index-url https://download.pytorch.org/whl/cu124 \
  -w deployment_package/wheels-cu124/ \
  --no-deps

# ê²€ì¦: ë‹¤ìš´ë¡œë“œëœ wheel ì´ë¦„ í™•ì¸
ls -lh deployment_package/wheels-cu124/ | grep torch

# ì˜ˆìƒ (cu124ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨):
# torch-2.1.2-cp311-cp311-cu124-linux_x86_64.whl âœ…
# torchaudio-2.1.2-cp311-cp311-cu124-linux_x86_64.whl âœ…
```

**ì¤‘ìš”**: 
- âŒ `torch-2.1.2-cp311-cp311-linux_x86_64.whl` (cu124 ì—†ìŒ = CPU ë²„ì „)
- âœ… `torch-2.1.2-cp311-cp311-cu124-linux_x86_64.whl` (cu124 ìˆìŒ = CUDA ë²„ì „)

### Step 3: ë‹¤ë¥¸ ì˜ì¡´ì„± ë‹¤ìš´ë¡œë“œ

```bash
python3 -m pip wheel \
  faster-whisper==1.0.3 \
  librosa==0.10.0 \
  numpy==1.24.3 \
  scipy==1.12.0 \
  huggingface-hub==0.21.4 \
  python-dotenv==1.0.0 \
  pydantic==2.5.3 \
  fastapi==0.109.0 \
  uvicorn==0.27.0 \
  requests==2.31.0 \
  pyyaml==6.0.1 \
  python-multipart==0.0.22 \
  -w deployment_package/wheels-cu124/ \
  --no-deps

# í™•ì¸
ls -1 deployment_package/wheels-cu124/ | wc -l
# ì˜ˆìƒ: 60ê°œ ì´ìƒ
```

### Step 4: Dockerfile.engine-cu124 ìƒì„± ë˜ëŠ” í™•ì¸

íŒŒì¼: `docker/Dockerfile.engine-cu124`

```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy CUDA 12.4 wheels
COPY deployment_package/wheels-cu124/ /wheels/

# Install packages from wheels (offline)
RUN python3.11 -m pip install --no-index --find-links=/wheels/ \
    torch torchaudio faster-whisper \
    librosa scipy numpy \
    fastapi uvicorn requests pydantic \
    huggingface-hub python-dotenv pyyaml \
    python-multipart && \
    rm -rf /wheels/

# Copy application files
COPY stt_engine.py /app/
COPY api_server.py /app/
COPY requirements.txt /app/

# Create directories for models and logs
RUN mkdir -p /app/models /app/logs /app/audio

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/app/models
ENV STT_DEVICE=auto

# Expose port
EXPOSE 8003

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

# Run API server
CMD ["python3.11", "api_server.py"]
```

### Step 5: Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
cd /Users/a113211/workspace/stt_engine

docker build \
  -t stt-engine:cu124 \
  -f docker/Dockerfile.engine-cu124 \
  .

# ë¹Œë“œ ì™„ë£Œ í™•ì¸
docker images | grep cu124
```

### Step 6: CUDA ì§€ì› í™•ì¸ (ì¤‘ìš”!)

```bash
# ìƒˆ ì´ë¯¸ì§€ì—ì„œ PyTorch CUDA ë²„ì „ í™•ì¸
docker run -it stt-engine:cu124 python3 -c \
  "import torch; print(f'PyTorch CUDA: {torch.version.cuda}'); \
   print(f'CUDA Available: {torch.cuda.is_available()}')"

# ì˜ˆìƒ ì¶œë ¥:
# PyTorch CUDA: 12.4  âœ…
# CUDA Available: False  (ì„œë²„ GPUê°€ ì—†ìœ¼ë¯€ë¡œ)
```

**ì¤‘ìš”**: 
- âŒ `PyTorch CUDA: None` â†’ CPU ë²„ì „ (ì‹¤íŒ¨)
- âœ… `PyTorch CUDA: 12.4` â†’ CUDA ë²„ì „ (ì„±ê³µ)

### Step 7: ì´ë¯¸ì§€ë¥¼ tar.gzë¡œ ì €ì¥

```bash
mkdir -p build/output

docker save stt-engine:cu124 | gzip > build/output/stt-engine-cu124.tar.gz

# í¬ê¸° í™•ì¸
ls -lh build/output/stt-engine-cu124.tar.gz
# ì˜ˆìƒ: ~1.0-1.1GB
```

---

## ğŸ–¥ï¸ ì„œë²„ì—ì„œ ë°°í¬

### ì´ë¯¸ì§€ ì „ì†¡ ë° ë¡œë“œ

```bash
# ë¡œì»¬ì—ì„œ ì„œë²„ë¡œ ì „ì†¡
scp build/output/stt-engine-cu124.tar.gz ddpapp@dlddpgai1:/data/stt/

# ì„œë²„ì— SSH ì ‘ì†
ssh ddpapp@dlddpgai1

# ì´ë¯¸ì§€ ë¡œë“œ
docker load -i /data/stt/stt-engine-cu124.tar.gz

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop stt-engine 2>/dev/null || true
docker rm stt-engine 2>/dev/null || true

# ìƒˆ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -v /data/models:/app/models \
  -v /data/logs:/app/logs \
  --gpus all \
  stt-engine:cu124

# ë¡œê·¸ì—ì„œ CUDA í™•ì¸
docker logs -f stt-engine

# ì˜ˆìƒ ë¡œê·¸:
# âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: cuda, compute: float16)
```

### ê²€ì¦

```bash
# CUDA ì‚¬ìš© í™•ì¸
docker exec stt-engine python3 -c \
  "import torch; print(f'PyTorch CUDA: {torch.version.cuda}'); \
   print(f'CUDA Available: {torch.cuda.is_available()}'); \
   print(f'Device: {torch.cuda.current_device()}')"

# ì˜ˆìƒ ì¶œë ¥:
# PyTorch CUDA: 12.4  âœ…
# CUDA Available: True  âœ…
# Device: 0  âœ…
```

---

## ğŸš¨ í˜„ì¬ ìƒí™© ì •ë¦¬

| í•­ëª© | ìƒíƒœ |
|------|------|
| í˜„ì¬ ì´ë¯¸ì§€ (stt-engine:linux-x86_64) | CPU PyTorch (CUDA: None) âŒ |
| GPU ë“œë¼ì´ë²„ (575.57.08) | âœ… ì™„ë²½ |
| CUDA Runtime (12.9) | âœ… ìˆìŒ |
| í•„ìš”í•œ ê²ƒ | CUDA 12.4 PyTorch ìƒˆ ì´ë¯¸ì§€ ë¹Œë“œ |

---

## ğŸ“‹ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
ë¡œì»¬ ë¹Œë“œ (40ë¶„)
â˜‘ deployment_package/wheels-cu124/ í´ë” ìƒì„±
â˜‘ pip wheel torch==2.1.2 --index-url https://download.pytorch.org/whl/cu124
  â†’ cu124ê°€ í¬í•¨ëœ wheel ë‹¤ìš´ë¡œë“œ í™•ì¸ âœ…
â˜‘ ë‹¤ë¥¸ ì˜ì¡´ì„± wheels ë‹¤ìš´ë¡œë“œ
â˜‘ Dockerfile.engine-cu124 ìƒì„±/í™•ì¸
â˜‘ docker build -t stt-engine:cu124 ì‹¤í–‰
â˜‘ docker runìœ¼ë¡œ CUDA ë²„ì „ í™•ì¸: "PyTorch CUDA: 12.4" âœ…
â˜‘ docker saveë¡œ tar.gz ìƒì„±

ì„œë²„ ë°°í¬ (15ë¶„)
â˜‘ scpë¡œ ì´ë¯¸ì§€ ì „ì†¡
â˜‘ docker load ì‹¤í–‰
â˜‘ docker run --gpus all ì‹¤í–‰
â˜‘ docker logsì—ì„œ "Device: cuda" í™•ì¸
â˜‘ docker execë¡œ PyTorch CUDA ë²„ì „ ì¬í™•ì¸: "PyTorch CUDA: 12.4" âœ…

ìµœì¢… ê²€ì¦
â˜‘ nvidia-smiì—ì„œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë³´ì„
â˜‘ curl /health â†’ {"status":"healthy","device":"cuda"}
â˜‘ ìŒì„± íŒŒì¼ STT í…ŒìŠ¤íŠ¸ (GPU ì‚¬ìš© í™•ì¸)
```

---

## âš ï¸ ì¤‘ìš” ì£¼ì˜ì‚¬í•­

### --index-url ë°˜ë“œì‹œ í¬í•¨!

```bash
# âŒ ì˜ëª»ëœ ëª…ë ¹ì–´ (CPU ë²„ì „ ë‹¤ìš´ë¡œë“œ)
pip wheel torch==2.1.2 -w deployment_package/wheels-cu124/

# âœ… ì˜¬ë°”ë¥¸ ëª…ë ¹ì–´ (CUDA 12.4 ë²„ì „ ë‹¤ìš´ë¡œë“œ)
pip wheel torch==2.1.2 \
  --index-url https://download.pytorch.org/whl/cu124 \
  -w deployment_package/wheels-cu124/
```

**--index-urlì´ ì—†ìœ¼ë©´**:
- PyPI ê¸°ë³¸ ì¸ë±ìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œ
- CPU ì „ìš© ìµœì‹  ë²„ì „ (ë˜ëŠ” CUDA ì§€ì› ì•ˆ í•˜ëŠ” ë²„ì „) ë‹¤ìš´ë¡œë“œ
- ê°™ì€ ë¬¸ì œ ë°˜ë³µ!

### ë‹¤ìš´ë¡œë“œ í›„ í™•ì¸!

```bash
# ë‹¤ìš´ë¡œë“œ í›„ ë°˜ë“œì‹œ í™•ì¸
ls -lh deployment_package/wheels-cu124/ | grep torch

# âœ… ì •í™•í•œ íŒŒì¼ëª…:
# torch-2.1.2-cp311-cp311-cu124-linux_x86_64.whl
# torchaudio-2.1.2-cp311-cp311-cu124-linux_x86_64.whl
```

---

**ê²°ë¡ **: CUDA 12.4 PyTorch ì´ë¯¸ì§€ë¥¼ ì •í™•íˆ ë¹Œë“œí•˜ë©´ GPU ì§€ì› ì™„ë²½í•˜ê²Œ ë©ë‹ˆë‹¤! ğŸ¯
