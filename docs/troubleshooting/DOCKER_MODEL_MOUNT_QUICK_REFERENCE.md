# ğŸ³ STT Engine Docker with Model Mount - Quick Reference

**ë‚ ì§œ**: 2026-02-03  
**ëª©ì **: ëª¨ë¸ì„ ë§ˆìš´íŠ¸í•˜ê³  Health Checkë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•

---

## ğŸš€ ê°€ì¥ ë¹ ë¥¸ ë°©ë²•: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

### ìë™ ì‹¤í–‰ (ê¶Œì¥)

```bash
bash /Users/a113211/workspace/stt_engine/docker-run-with-models.sh
```

**ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…:**
1. âœ… ì´ì „ ì»¨í…Œì´ë„ˆ ì •ë¦¬
2. âœ… ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
3. âœ… Docker ì´ë¯¸ì§€ í™•ì¸
4. âœ… ëª¨ë¸ ë§ˆìš´íŠ¸ (-v ì˜µì…˜)
5. âœ… ì»¨í…Œì´ë„ˆ ì‹¤í–‰
6. âœ… Health Check ìˆ˜í–‰ (ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°)
7. âœ… ëª¨ë¸ ê²½ë¡œ í™•ì¸
8. âœ… PyTorch ì •ë³´ ì¶œë ¥

---

## ğŸ“‹ ìˆ˜ë™ ì‹¤í–‰ (í•œ ì¤„ ëª…ë ¹ì–´)

### Step 1: ì´ì „ ì»¨í…Œì´ë„ˆ ì •ë¦¬

```bash
docker stop stt-engine-test 2>/dev/null || true
docker rm stt-engine-test 2>/dev/null || true
```

### Step 2: ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ëª¨ë¸ ë§ˆìš´íŠ¸ í¬í•¨)

```bash
docker run -d \
  --name stt-engine-test \
  -p 8003:8003 \
  -v /Users/a113211/workspace/stt_engine/models:/app/models \
  -e STT_DEVICE=cpu \
  -e HF_HOME=/app/models \
  stt-engine:cuda129-v1.0
```

**ì£¼ìš” ì˜µì…˜ ì„¤ëª…:**
| ì˜µì…˜ | ì„¤ëª… |
|------|------|
| `-d` | ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ |
| `--name stt-engine-test` | ì»¨í…Œì´ë„ˆ ì´ë¦„ |
| `-p 8003:8003` | í¬íŠ¸ ë§¤í•‘ |
| `-v ë¡œì»¬ê²½ë¡œ:ì»¨í…Œì´ë„ˆê²½ë¡œ` | **ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸** (í•µì‹¬!) |
| `-e STT_DEVICE=cpu` | CPU ëª¨ë“œ ì‚¬ìš© |
| `-e HF_HOME=/app/models` | Hugging Face ìºì‹œ ê²½ë¡œ |

### Step 3: Health Check

```bash
curl -X GET http://localhost:8003/health
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{"status":"healthy"}
```

---

## ğŸ” ìƒíƒœ í™•ì¸ ëª…ë ¹ì–´

### 1. ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìƒíƒœ

```bash
docker ps | grep stt-engine
```

**ì˜ˆìƒ ì¶œë ¥:**
```
CONTAINER ID   IMAGE                    STATUS        PORTS
abc12345...    stt-engine:cuda129...    Up 2 minutes  0.0.0.0:8003->8003/tcp
```

### 2. ì‹¤ì‹œê°„ ë¡œê·¸ ë³´ê¸° (ëª¨ë¸ ë¡œë”© í™•ì¸)

```bash
docker logs -f stt-engine-test
```

**ì˜ˆìƒ ë¡œê·¸:**
```
Loading model 'openai_whisper-large-v3-turbo'...
âœ… Model loaded successfully (Device: cpu, compute: int8)
Uvicorn running on http://0.0.0.0:8003
```

### 3. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ëª¨ë¸ í™•ì¸

```bash
docker exec stt-engine-test ls -lh /app/models/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
openai_whisper-large-v3-turbo/  [ëª¨ë¸ ë””ë ‰í† ë¦¬]
```

### 4. PyTorch ì •ë³´ í™•ì¸

```bash
docker exec stt-engine-test python3 << 'EOF'
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'Device: {"cuda" if torch.cuda.is_available() else "cpu"}')
EOF
```

**ì˜ˆìƒ ì¶œë ¥:**
```
PyTorch: 2.6.0
CUDA Available: False
CUDA Version: None
Device: cpu
```

### 5. API Health Check ìƒì„¸ ì •ë³´

```bash
curl -v http://localhost:8003/health
```

---

## ğŸ§ª STT í…ŒìŠ¤íŠ¸

### ìŒì„± íŒŒì¼ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ìŒì„± íŒŒì¼ ê²½ë¡œ (ì˜ˆì‹œ)
TEST_AUDIO="/path/to/test_audio.wav"

curl -X POST http://localhost:8003/transcribe \
  -F "file=@$TEST_AUDIO"
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{
  "text": "ì¸ì‹ëœ ìŒì„± í…ìŠ¤íŠ¸...",
  "duration_seconds": 5.2,
  "processing_time_seconds": 0.8,
  "model": "whisper-large-v3-turbo",
  "device": "cpu"
}
```

---

## ğŸ”´ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: "Health Check ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ)"

**ì›ì¸:** ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨  
**í™•ì¸ ë°©ë²•:**
```bash
docker logs stt-engine-test | tail -50
```

**í•´ê²°ì±…:**
```bash
# 1. ëª¨ë¸ ê²½ë¡œ í™•ì¸
docker exec stt-engine-test ls -lh /app/models/

# 2. ì»¨í…Œì´ë„ˆ ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰
docker run -it --rm \
  -v /Users/a113211/workspace/stt_engine/models:/app/models \
  stt-engine:cuda129-v1.0 \
  python3 -c "from faster_whisper import WhisperModel; \
              model = WhisperModel('large-v3-turbo', device='cpu')"
```

### ë¬¸ì œ 2: "í¬íŠ¸ 8003ì´ ì´ë¯¸ ì‚¬ìš© ì¤‘"

```bash
# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰
docker run -d \
  --name stt-engine-test \
  -p 8004:8003 \  # 8004:8003ìœ¼ë¡œ ë³€ê²½
  -v /Users/a113211/workspace/stt_engine/models:/app/models \
  -e STT_DEVICE=cpu \
  stt-engine:cuda129-v1.0

# ìƒˆ í¬íŠ¸ë¡œ í™•ì¸
curl http://localhost:8004/health
```

### ë¬¸ì œ 3: "ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

```bash
# 1. ë¡œì»¬ ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la /Users/a113211/workspace/stt_engine/models/

# 2. ëª¨ë¸ êµ¬ì¡° í™•ì¸ (openai_whisper-large-v3-turbo ìˆëŠ”ì§€)
ls -la /Users/a113211/workspace/stt_engine/models/openai_whisper-large-v3-turbo/

# 3. ì»¨í…Œì´ë„ˆ ë‚´ ë§ˆìš´íŠ¸ í™•ì¸
docker inspect stt-engine-test | grep -A 5 "Mounts"
```

---

## ğŸ§¹ ì •ë¦¬

### ì»¨í…Œì´ë„ˆ ì¤‘ì§€

```bash
docker stop stt-engine-test
```

### ì»¨í…Œì´ë„ˆ ì™„ì „ ì œê±°

```bash
docker rm stt-engine-test
```

### ì´ë¯¸ì§€ ì œê±° (í•„ìš”í•œ ê²½ìš°)

```bash
docker rmi stt-engine:cuda129-v1.0
```

---

## ğŸ“Š ì£¼ìš” íŒŒì¼ ê²½ë¡œ

| í•­ëª© | ê²½ë¡œ |
|------|------|
| ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ | `/Users/a113211/workspace/stt_engine/docker-run-with-models.sh` |
| ë¡œì»¬ ëª¨ë¸ | `/Users/a113211/workspace/stt_engine/models/` |
| ì»¨í…Œì´ë„ˆ ëª¨ë¸ ê²½ë¡œ | `/app/models/` |
| Docker ì´ë¯¸ì§€ | `stt-engine:cuda129-v1.0` |

---

## ğŸ’¡ íŒ

### 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
# ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë“  ê²€ì‚¬ ìë™í™”
bash docker-run-with-models.sh
```

### 2. ëª¨ë¸ ì—…ë°ì´íŠ¸

ë¡œì»¬ ëª¨ë¸ì„ ë³€ê²½í•˜ë©´ ì»¨í…Œì´ë„ˆì— ìë™ìœ¼ë¡œ ë°˜ì˜ë¨ (ë§ˆìš´íŠ¸ ë•ë¶„)

### 3. ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆ ë™ì‹œ ì‹¤í–‰

```bash
# ë‹¤ë¥¸ í¬íŠ¸ì™€ ì´ë¦„ìœ¼ë¡œ ì‹¤í–‰
docker run -d --name stt-engine-gpu -p 8004:8003 \
  -v /Users/a113211/workspace/stt_engine/models:/app/models \
  -e STT_DEVICE=cuda \
  stt-engine:cuda129-v1.0
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026-02-03  
**ìƒíƒœ**: âœ… ì‚¬ìš© ê°€ëŠ¥
