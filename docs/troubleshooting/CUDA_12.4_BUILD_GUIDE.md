# ğŸš€ CUDA 12.4 PyTorch ì´ë¯¸ì§€ ë¹Œë“œ ì‹¤í–‰ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-02-03  
**ëª©í‘œ**: CUDA 12.9 ì„œë²„ì™€ í˜¸í™˜ë˜ëŠ” ì´ë¯¸ì§€ ë¹Œë“œ  
**ì†Œìš” ì‹œê°„**: 40-50ë¶„

---

## 1ï¸âƒ£ ì‚¬ì „ ê²€ì¦ (ì„œë²„ì—ì„œ ë¨¼ì € í™•ì¸)

```bash
# SSHë¡œ ì„œë²„ ì ‘ì† í›„

# 1. GPU ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸
nvidia-smi

# ì¶œë ¥ ì˜ˆ:
# NVIDIA-SMI 555.42.02
# CUDA Version: 12.9
# ...

# ë“œë¼ì´ë²„ê°€ 555.xx ì´ìƒì´ë©´ â†’ CUDA 12.4 í˜¸í™˜ âœ…
# ë“œë¼ì´ë²„ê°€ 550.xx ì´í•˜ì´ë©´ â†’ ì—…ê·¸ë ˆì´ë“œ í•„ìš” âŒ

# 2. CUDA Toolkit í™•ì¸
nvcc --version

# ì¶œë ¥ ì˜ˆ:
# nvcc: NVIDIA (R) Cuda compiler driver
# release 12.9, V12.9.1
```

**ì¡°ê±´ í™•ì¸**:
- â˜‘ GPU ë“œë¼ì´ë²„: 555.xx ì´ìƒ
- â˜‘ CUDA Toolkit: 12.9 ì„¤ì¹˜ë¨

---

## 2ï¸âƒ£ ë¡œì»¬ì—ì„œ CUDA 12.4 Wheel ì¤€ë¹„

### Step 1: ìƒˆ wheels ë””ë ‰í† ë¦¬ ìƒì„±

```bash
cd /Users/a113211/workspace/stt_engine

# ê¸°ì¡´ wheelsëŠ” ìœ ì§€í•˜ê³  ìƒˆë¡œ ë§Œë“¤ê¸°
mkdir -p deployment_package/wheels-cu124

cd deployment_package/wheels-cu124
```

### Step 2: CUDA 12.4ìš© PyTorch wheels ë‹¤ìš´ë¡œë“œ

```bash
# í˜„ì¬ ìœ„ì¹˜: deployment_package/wheels-cu124/

# PyTorch CUDA 12.4 ë²„ì „ ë‹¤ìš´ë¡œë“œ
python3 -m pip wheel torch==2.1.2 torchaudio==2.1.2 \
  --index-url https://download.pytorch.org/whl/cu124 \
  -w . \
  --no-deps

# ë‹¤ë¥¸ ì˜ì¡´ì„± (ì´ì „ê³¼ ë™ì¼)
python3 -m pip wheel \
  faster-whisper==1.0.3 \
  librosa==0.10.0 \
  numpy==1.24.3 \
  scipy==1.12.0 \
  huggingface-hub==0.21.4 \
  python-dotenv==1.0.0 \
  pydantic==2.5.3 \
  fastapi==0.109.0 \
  uvicorn==0.27.0 \
  requests==2.31.0 \
  pyyaml==6.0.1 \
  python-multipart==0.0.22 \
  -w . \
  --no-deps

# í™•ì¸: wheels íŒŒì¼ ê°œìˆ˜
ls -1 | wc -l
# ì˜ˆìƒ: 60ê°œ ì´ìƒ

# í¬ê¸° í™•ì¸
du -sh .
# ì˜ˆìƒ: 400MB ì •ë„
```

**ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸**:
```bash
$ ls -lh | grep -E "torch|cuda|pytorch"
# ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•¨:
# torch-2.1.2-cp311-cp311-linux_x86_64.whl
# torchaudio-2.1.2-cp311-cp311-linux_x86_64.whl
```

---

## 3ï¸âƒ£ CUDA 12.4ìš© Dockerfile ìƒì„±

```bash
# ë¡œì»¬ workspaceì—ì„œ

cd /Users/a113211/workspace/stt_engine
```

`docker/Dockerfile.engine-cu124` íŒŒì¼ì„ ìƒì„±:

```dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy CUDA 12.4 wheels
COPY deployment_package/wheels-cu124/ /wheels/

# Install packages from wheels (offline)
RUN python3.11 -m pip install --no-index --find-links=/wheels/ \
    torch torchaudio faster-whisper \
    librosa scipy numpy \
    fastapi uvicorn requests pydantic \
    huggingface-hub python-dotenv pyyaml \
    python-multipart && \
    rm -rf /wheels/

# Copy application files
COPY stt_engine.py /app/
COPY api_server.py /app/
COPY requirements.txt /app/

# Create directories for models and logs
RUN mkdir -p /app/models /app/logs /app/audio

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/app/models
ENV STT_DEVICE=auto

# Expose port
EXPOSE 8003

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

# Run API server
CMD ["python3.11", "api_server.py"]
```

---

## 4ï¸âƒ£ CUDA 12.4 ì´ë¯¸ì§€ ë¹Œë“œ

```bash
cd /Users/a113211/workspace/stt_engine

# ë¹Œë“œ ì‹œì‘ (30-40ë¶„ ì†Œìš”)
docker build \
  -t stt-engine:cu124 \
  -f docker/Dockerfile.engine-cu124 \
  . \
  2>&1 | tee /tmp/build-cu124.log

# ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ
nohup docker build \
  -t stt-engine:cu124 \
  -f docker/Dockerfile.engine-cu124 \
  . > /tmp/build-cu124.log 2>&1 &

# ë¹Œë“œ ì§„í–‰ ìƒí™© í™•ì¸
tail -f /tmp/build-cu124.log
```

**ë¹Œë“œ í™•ì¸**:
```bash
# ì™„ë£Œ í›„
docker images | grep cu124

# ì˜ˆìƒ ì¶œë ¥
# stt-engine           cu124          abc123...     1.1GB
```

---

## 5ï¸âƒ£ ì´ë¯¸ì§€ë¥¼ tar.gzë¡œ ì €ì¥

```bash
# ë¹Œë“œ ì™„ë£Œ í›„

mkdir -p build/output

# ì´ë¯¸ì§€ë¥¼ ì••ì¶• íŒŒì¼ë¡œ ì €ì¥
docker save stt-engine:cu124 | gzip > build/output/stt-engine-cu124.tar.gz

# í¬ê¸° í™•ì¸
ls -lh build/output/stt-engine-cu124.tar.gz

# ì˜ˆìƒ: 1.0-1.1GB
```

---

## 6ï¸âƒ£ ì„œë²„ë¡œ ì´ë¯¸ì§€ ì „ì†¡

```bash
# ë¡œì»¬ì—ì„œ ì„œë²„ë¡œ ì „ì†¡
scp build/output/stt-engine-cu124.tar.gz ddpapp@dlddpgai1:/data/stt/

# ë˜ëŠ”
scp build/output/stt-engine-cu124.tar.gz user@server:/path/to/

# ì „ì†¡ ì§„í–‰ ìƒí™© í™•ì¸
# (ì•½ 5-10ë¶„ ì†Œìš”, ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼)
```

---

## 7ï¸âƒ£ ì„œë²„ì—ì„œ ë°°í¬

```bash
# ì„œë²„ì— SSH ì ‘ì†
ssh user@server

# ë˜ëŠ”
ssh ddpapp@dlddpgai1

# ì´ë¯¸ì§€ ë¡œë“œ
cd /data/stt
docker load -i stt-engine-cu124.tar.gz

# ë¡œë“œ í™•ì¸
docker images | grep cu124
```

### ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker stop stt-engine 2>/dev/null || true

# ì»¨í…Œì´ë„ˆ ì œê±°
docker rm stt-engine 2>/dev/null || true

# í™•ì¸
docker ps -a | grep stt-engine
# (ì•„ë¬´ê²ƒë„ ë‚˜ì˜¤ì§€ ì•Šì•„ì•¼ í•¨)
```

### ìƒˆ ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```bash
# CUDA 12.4 ì´ë¯¸ì§€ë¡œ ì‹¤í–‰
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -v /data/models:/app/models \
  -v /data/logs:/app/logs \
  --gpus all \
  -e STT_DEVICE=auto \
  stt-engine:cu124

# ë˜ëŠ” CUDA ìë™ ì„ íƒ
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -v /data/models:/app/models \
  -v /data/logs:/app/logs \
  --gpus all \
  stt-engine:cu124
```

**ì°¸ê³ **: 
- `--gpus all`: GPU ëª¨ë“  CUDA ê¸°ëŠ¥ í™œì„±í™”
- `STT_DEVICE=auto`: ìë™ ê°ì§€ (ë” ì•ˆì „)

### ë°°í¬ ê²€ì¦

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps | grep stt-engine

# ì˜ˆìƒ: stt-engine ... Up ... 0.0.0.0:8003->8003/tcp

# ë¡œê·¸ í™•ì¸ (CUDA ì´ˆê¸°í™” ë©”ì‹œì§€ í™•ì¸)
docker logs -f stt-engine

# ì˜ˆìƒ ë¡œê·¸:
# âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: cuda, compute: float16)
# INFO:     Started server process
# INFO:     Uvicorn running on http://0.0.0.0:8003
```

### í—¬ìŠ¤ ì²´í¬

```bash
# API ì‘ë‹µ í™•ì¸
curl -X GET http://localhost:8003/health

# ì˜ˆìƒ ì‘ë‹µ:
# {"status": "healthy", "device": "cuda"}
```

### ìŒì„± íŒŒì¼ í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ìŒì„± íŒŒì¼ì´ ìˆìœ¼ë©´
curl -X POST http://localhost:8003/transcribe \
  -F "file=@/data/test_audio.wav"

# ì˜ˆìƒ ì‘ë‹µ:
# {
#   "text": "recognized text...",
#   "duration_seconds": 5.2,
#   "processing_time_seconds": 0.8,
#   "model": "whisper-large-v3-turbo",
#   "device": "cuda"
# }
```

---

## ğŸ“Š ë¹Œë“œ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
ë¡œì»¬ ì¤€ë¹„ (40ë¶„)
â˜‘ CUDA 12.4 wheels ë‹¤ìš´ë¡œë“œ (deployment_package/wheels-cu124/)
â˜‘ Dockerfile.engine-cu124 ìƒì„±
â˜‘ Docker ì´ë¯¸ì§€ ë¹Œë“œ (docker build ...)
â˜‘ ì´ë¯¸ì§€ tar.gz ì €ì¥ (docker save ...)
â˜‘ íŒŒì¼ í¬ê¸° í™•ì¸ (~1.1GB)

ì„œë²„ ì „ì†¡ (10ë¶„)
â˜‘ scpë¡œ ì´ë¯¸ì§€ íŒŒì¼ ì „ì†¡ (build/output/stt-engine-cu124.tar.gz)
â˜‘ ì „ì†¡ ì™„ë£Œ í™•ì¸ (/data/stt/stt-engine-cu124.tar.gz)

ì„œë²„ ë°°í¬ (5ë¶„)
â˜‘ docker load -i stt-engine-cu124.tar.gz
â˜‘ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì¤‘ì§€/ì œê±°
â˜‘ ìƒˆ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (--gpus all)
â˜‘ docker logs í™•ì¸ (Device: cuda)
â˜‘ curl /health í—¬ìŠ¤ ì²´í¬
â˜‘ ìŒì„± íŒŒì¼ STT í…ŒìŠ¤íŠ¸

ìµœì¢… ê²€ì¦
â˜‘ GPU ì‚¬ìš© í™•ì¸ (nvidia-smi)
â˜‘ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ (ì—ëŸ¬ ì—†ìŒ)
â˜‘ STT ì„±ëŠ¥ í™•ì¸ (ì²˜ë¦¬ ì‹œê°„)
```

---

## ğŸ”„ ë§Œì•½ CUDA 12.4ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë²„ì „ì´ í•„ìš”í•˜ë©´?

### PyTorch CUDA ë²„ì „ ë³€ê²½

```bash
# CUDA 12.4 ëŒ€ì‹ :
# - CUDA 11.8: --index-url https://download.pytorch.org/whl/cu118
# - CUDA 12.1: --index-url https://download.pytorch.org/whl/cu121
# - CUDA 12.5: --index-url https://download.pytorch.org/whl/cu125

# ì˜ˆ: CUDA 11.8ìš©
pip wheel torch==2.1.2 torchaudio==2.1.2 \
  --index-url https://download.pytorch.org/whl/cu118 \
  -w deployment_package/wheels-cu118/
```

---

## â±ï¸ ì†Œìš” ì‹œê°„ ì˜ˆì¸¡

| ë‹¨ê³„ | ì˜ˆìƒ ì‹œê°„ |
|------|----------|
| wheels ë‹¤ìš´ë¡œë“œ | 10-15ë¶„ |
| Docker ë¹Œë“œ | 20-30ë¶„ |
| tar.gz ì €ì¥ | 2-3ë¶„ |
| scp ì „ì†¡ | 5-10ë¶„ (ë„¤íŠ¸ì›Œí¬) |
| ì„œë²„ ë¡œë“œ | 2-3ë¶„ |
| ì»¨í…Œì´ë„ˆ ì‹¤í–‰ | 1-2ë¶„ |
| **ì´ê³„** | **40-60ë¶„** |

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

**CUDA 12.9 ì„œë²„ì—ëŠ” CUDA 12.4 ì´ë¯¸ì§€ê°€ ì •ë‹µì…ë‹ˆë‹¤!**

```bash
# 1ë‹¨ê³„: ì„œë²„ í™•ì¸ (ë“œë¼ì´ë²„ 555.xx ì´ìƒ)
nvidia-smi

# 2ë‹¨ê³„: ë¡œì»¬ì—ì„œ ë¹Œë“œ (40ë¶„)
python3 -m pip wheel torch==2.1.2 --index-url https://download.pytorch.org/whl/cu124 -w deployment_package/wheels-cu124/
docker build -t stt-engine:cu124 -f docker/Dockerfile.engine-cu124 .

# 3ë‹¨ê³„: ì„œë²„ ë°°í¬ (10ë¶„)
scp build/output/stt-engine-cu124.tar.gz server:/data/
ssh server "docker load -i /data/stt-engine-cu124.tar.gz && \
  docker run -d --gpus all -p 8003:8003 stt-engine:cu124"

# 4ë‹¨ê³„: ê²€ì¦ (5ë¶„)
curl http://server:8003/health
```

---

**ìƒíƒœ**: ğŸŸ¢ CUDA 12.9 í™˜ê²½ ì™„ë²½ ì§€ì› âœ…
