# êµ¬í˜„ ë¡œë“œë§µ ë° ì•¡ì…˜ í”Œëœ

## í˜„ì¬ ìƒí™© ìš”ì•½

### í™•ì¸ëœ ì‚¬ì‹¤
1. **ê¸°ì¡´ API**: `/transcribe` (v2) + `/transcribe_legacy` ì´ì¤‘ êµ¬ì¡°
2. **ì˜µì…˜**: privacy_llm_type, classification_llm_type ì—†ìŒ â†’ LLM ì„ íƒ ë¶ˆê°€ëŠ¥
3. **í…ìŠ¤íŠ¸ ì…ë ¥**: ì•„ì§ êµ¬í˜„ ì•ˆ ë¨
4. **vLLM/Ollama**: í´ë¼ì´ì–¸íŠ¸ ë¯¸êµ¬í˜„

---

## ê¶Œì¥ êµ¬í˜„ ìˆœì„œ

### âœ… Phase 1: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì§€ì› (í•„ìˆ˜)

**ì‹œê°„**: 1-2ì‹œê°„  
**ëª©í‘œ**: `stt_text` íŒŒë¼ë¯¸í„°ë¡œ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ê°€ëŠ¥

**ìˆ˜ì • íŒŒì¼**:
- `api_server/app.py` - `transcribe_v2()` í•¨ìˆ˜
- `api_server/transcribe_endpoint.py` - `validate_and_prepare_file()` í•¨ìˆ˜

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
```
[ ] file_pathë¥¼ ì„ íƒì‚¬í•­ìœ¼ë¡œ ë³€ê²½ (ê¸°ë³¸ê°’: None)
[ ] stt_text íŒŒë¼ë¯¸í„° ì¶”ê°€
[ ] ì…ë ¥ ê²€ì¦ ë¡œì§ ì¶”ê°€ (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ)
[ ] STT ë‹¨ê³„ ì¡°ê±´ë¶€ ì²˜ë¦¬ (file_pathê°€ ìˆì„ ë•Œë§Œ)
[ ] ì‘ë‹µì— skip_stt í”Œë˜ê·¸ ì¶”ê°€
[ ] ê¸°ì¡´ í˜¸ì¶œ í˜¸í™˜ì„± ìœ ì§€
```

**êµ¬í˜„ ì˜ˆ**:
```python
# Before
@app.post("/transcribe")
async def transcribe_v2(
    file_path: str = Form(...),  # í•„ìˆ˜
    ...
):

# After
@app.post("/transcribe")
async def transcribe_v2(
    file_path: Optional[str] = Form(None),  # ì„ íƒ
    stt_text: Optional[str] = Form(None),   # NEW
    ...
):
    # ì…ë ¥ ê²€ì¦
    if not file_path and not stt_text:
        raise HTTPException(400, "file_path ë˜ëŠ” stt_text ì œê³µ í•„ìˆ˜")
    if file_path and stt_text:
        raise HTTPException(400, "ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì œê³µí•˜ì„¸ìš”")
    
    # STT ë‹¨ê³„ ë¶„ê¸°
    if file_path:
        stt_result = await perform_stt(...)  # ê¸°ì¡´
    else:
        stt_result = {'text': stt_text, 'skipped': True}  # ìƒˆë¡œìš´
```

### ğŸ”„ Phase 2: LLM íƒ€ì… ì„ íƒ ì˜µì…˜ ì¶”ê°€ (ì„ íƒ)

**ì‹œê°„**: 2-3ì‹œê°„  
**ëª©í‘œ**: privacy_llm_type, classification_llm_type íŒŒë¼ë¯¸í„°ë¡œ LLM ì„ íƒ ê°€ëŠ¥

**ìˆ˜ì • íŒŒì¼**:
- `api_server/app.py`
- `api_server/transcribe_endpoint.py`
- `api_server/services/privacy_remover.py`
- `api_server/services/classification_service.py`

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
```
[ ] privacy_llm_type íŒŒë¼ë¯¸í„° ì¶”ê°€ ("openai" | "vllm" | "ollama")
[ ] classification_llm_type íŒŒë¼ë¯¸í„° ì¶”ê°€
[ ] vllm_model_name, ollama_model_name íŒŒë¼ë¯¸í„° ì¶”ê°€
[ ] perform_privacy_removal()ì— llm_type ì „ë‹¬
[ ] perform_classification()ì— llm_type ì „ë‹¬
[ ] ê° LLM í´ë¼ì´ì–¸íŠ¸ì—ì„œ ëª¨ë¸ëª… ì²˜ë¦¬
```

**êµ¬í˜„ ì˜ˆ**:
```python
# perform_privacy_removal() í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ í™•ì¥
async def perform_privacy_removal(
    text: str,
    prompt_type: str = "privacy_remover_default_v6",
    llm_type: str = "openai",  # NEW
    model_name: Optional[str] = None,  # NEW
) -> dict:
    """
    llm_type = "openai": OpenAI API (ê¸°ì¡´)
    llm_type = "vllm": vLLM ë¡œì»¬ (NEW)
    llm_type = "ollama": Ollama ë¡œì»¬ (NEW)
    """
    
    if llm_type == "openai":
        client = LLMClientFactory.create_client("gpt-4o")
    elif llm_type == "vllm":
        # vLLMClient êµ¬í˜„ í•„ìš”
        client = VLLMClient(model_name or "default-vllm-model")
    elif llm_type == "ollama":
        # OllamaClient êµ¬í˜„ í•„ìš”
        client = OllamaClient(
            model_name or "llama2",
            api_url=os.getenv("OLLAMA_API_URL", "http://localhost:11434")
        )
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM íƒ€ì…: {llm_type}")
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = load_prompt(prompt_type)
    
    # LLM í˜¸ì¶œ (ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
    response = await client.generate_response(
        prompt=f"{prompt}\n\n{text}",
        temperature=0.3,
        max_tokens=2048
    )
    
    return parse_response(response)
```

### ğŸ“‹ Phase 3: vLLM/Ollama í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ (í•„ìš”ì‹œ)

**ì‹œê°„**: 3-4ì‹œê°„  
**ëª©í‘œ**: vLLMê³¼ Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM í˜¸ì¶œ ê°€ëŠ¥

**ìˆ˜ì • íŒŒì¼**:
- `api_server/services/privacy_remover.py` - `VLLMClient` í´ë˜ìŠ¤ ì¶”ê°€
- `api_server/services/privacy_remover.py` - `OllamaClient` í´ë˜ìŠ¤ ì¶”ê°€

**êµ¬í˜„ ì˜ˆ**:
```python
class VLLMClient:
    """vLLM ë¡œì»¬ ì„œë²„ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, model_name: str, api_url: str = "http://localhost:8000"):
        self.model_name = model_name
        self.api_url = api_url
    
    async def generate_response(self, prompt: str, **kwargs):
        """
        vLLM API í˜¸ì¶œ
        POST http://localhost:8000/v1/completions
        """
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 2048)
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.api_url}/v1/completions",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                return result["choices"][0]["text"]


class OllamaClient:
    """Ollama ë¡œì»¬ ì„œë²„ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, model_name: str, api_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.api_url = api_url
    
    async def generate_response(self, prompt: str, **kwargs):
        """
        Ollama API í˜¸ì¶œ
        POST http://localhost:11434/api/generate
        """
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": kwargs.get("temperature", 0.7),
            "stream": False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.api_url}/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                return result["response"]
```

---

## ë¹ ë¥¸ ì‹œì‘ (Phase 1ë§Œ ì§„í–‰)

### Step 1: í™•ì¸ ì‘ì—… (5ë¶„)
```bash
# í˜„ì¬ API ë™ì‘ í™•ì¸
cd /Users/a113211/workspace/stt_engine

# STT API ì‹¤í–‰
python api_server.py &

# ê¸°ì¡´ ë°©ì‹ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8003/transcribe \
  -F 'file_path=/app/audio/test.wav' \
  -F 'privacy_removal=false'
```

### Step 2: Phase 1 êµ¬í˜„ (1-2ì‹œê°„)
1. `api_server/app.py`ì˜ `transcribe_v2()` í•¨ìˆ˜ ìˆ˜ì •
2. `api_server/transcribe_endpoint.py` í•¨ìˆ˜ ìˆ˜ì •
3. ì…ë ¥ ê²€ì¦ ë¡œì§ ì¶”ê°€
4. STT ë‹¨ê³„ ì¡°ê±´ë¶€ ì²˜ë¦¬

### Step 3: í…ŒìŠ¤íŠ¸ (30ë¶„)
```bash
# í…ìŠ¤íŠ¸ ì…ë ¥ ë°©ì‹ í…ŒìŠ¤íŠ¸ (NEW)
curl -X POST http://localhost:8003/transcribe \
  -F 'stt_text=ê³ ê°ë‹˜, ì €í¬ ìƒí’ˆ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤' \
  -F 'privacy_removal=true'

# ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ (ê¸°ì¡´)
curl -X POST http://localhost:8003/transcribe \
  -F 'file_path=/app/audio/test.wav' \
  -F 'privacy_removal=true'
```

---

## ì˜µì…˜ ì²˜ë¦¬ ì„¤ê³„ ê²°ì •

### ì§ˆë¬¸: "í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ vs í…ìŠ¤íŠ¸ ê¸°ë°˜?"

**ë‹µë³€**: **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)**

**ì´ìœ **:
1. í˜„ì¬ êµ¬ì¡°ê°€ ì´ë¯¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ âœ…
2. í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œëŠ” ë³µì¡í•œ ì§€ì‹œì‚¬í•­ ì „ë‹¬ ë¶ˆê°€
3. í”„ë¡¬í”„íŠ¸ íƒ€ì…ìœ¼ë¡œ ìœ ì—°ì„± í™•ë³´ ê°€ëŠ¥

**ì˜µì…˜ ì²˜ë¦¬ ë°©ì‹**:
```
ì…ë ¥: stt_text + privacy_prompt_type + privacy_llm_type

ì²˜ë¦¬ ë¡œì§:
1. prompt_type ì„ íƒ (privacy_remover_default_v6, loosed, strict ë“±)
2. LLM íƒ€ì… ì„ íƒ (openai, vllm, ollama)
3. í”„ë¡¬í”„íŠ¸ ë¡œë“œ + í…ìŠ¤íŠ¸ ê²°í•©
4. í•´ë‹¹ LLM í˜¸ì¶œ
5. ì‘ë‹µ íŒŒì‹±
```

**ì˜ˆ**:
```bash
# privacy_prompt_typeìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
curl -X POST http://localhost:8003/transcribe \
  -F 'stt_text=...' \
  -F 'privacy_removal=true' \
  -F 'privacy_prompt_type=privacy_remover_default_v6' \
  -F 'privacy_llm_type=vllm' \
  -F 'vllm_model_name=llama2'
  
# ì²˜ë¦¬ íë¦„:
# 1. privacy_remover_default_v6.prompt íŒŒì¼ ë¡œë“œ
# 2. í…ìŠ¤íŠ¸ì™€ ê²°í•©
# 3. vLLM (llama2) í˜¸ì¶œ
# 4. ì‘ë‹µ íŒŒì‹±
```

---

## í†µí•© vs ë¶„ë¦¬ ê²°ì •

### í˜„ì¬: `/transcribe` + `/transcribe_legacy`

| API | ìƒíƒœ | êµ¬í˜„ ë²”ìœ„ | ìœ ì§€ ì—¬ë¶€ |
|-----|------|---------|---------|
| /transcribe | í˜„ì¬ ì‚¬ìš©ì¤‘ | ëª¨ë“  ê¸°ëŠ¥ | **í†µí•© ëŒ€ìƒ** |
| /transcribe_legacy | í˜¸í™˜ì„± | ê¸°ë³¸ë§Œ | **ì œê±° ëŒ€ìƒ** |

### ê¶Œì¥ ë°©ì•ˆ: í†µí•©

```
Before (í˜„ì¬):
/transcribe â”€â”€â”€â”€â”€â”€â–º (v2, ëª¨ë“  ê¸°ëŠ¥)
/transcribe_legacy â†’ (ê¸°ë³¸ë§Œ)

After (í†µí•©):
/transcribe â”€â”€â”€â”€â”€â”€â–º (ëª¨ë“  ê¸°ëŠ¥ + í…ìŠ¤íŠ¸ ì…ë ¥)
```

**ì¥ì **:
- API í•˜ë‚˜ë¡œ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬
- ì˜µì…˜ ê´€ë¦¬ ë‹¨ìˆœí™”
- ìœ ì§€ë³´ìˆ˜ ìš©ì´

**ì‘ì—…**:
1. Phase 1-3 êµ¬í˜„ ì™„ë£Œ
2. `/transcribe_legacy` ì—”ë“œí¬ì¸íŠ¸ ì œê±° (ë˜ëŠ” deprecated ì²˜ë¦¬)
3. ë¬¸ì„œ ì—…ë°ì´íŠ¸

---

## ë‹¤ìŒ ì§„í–‰ ë°©ì‹

**ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ì–´ìš”?**

### Option A: Phase 1ë§Œ ë¨¼ì € (ê¶Œì¥)
- í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ëŠ¥ ì¶”ê°€
- ê¸°ì¡´ ê¸°ëŠ¥ í˜¸í™˜ì„± ìœ ì§€
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ (1-2ì‹œê°„)
- ë‚˜ì¤‘ì— Phase 2-3 ì§„í–‰ ê°€ëŠ¥

### Option B: Phase 1-3 í•œë²ˆì—
- ì™„ì „í•œ LLM ì„ íƒ ê¸°ëŠ¥ í¬í•¨
- ì‹œê°„ ì†Œìš” (5-6ì‹œê°„)
- ì¦‰ì‹œ vLLM/Ollama í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

### Option C: Phase 1 + 2ë§Œ
- í…ìŠ¤íŠ¸ ì…ë ¥ + LLM ì„ íƒ
- ì ì ˆí•œ ìˆ˜ì¤€ (3-4ì‹œê°„)
- Phase 3ëŠ” ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ

