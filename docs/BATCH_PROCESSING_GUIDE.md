# Batch ìŒì„± ì²˜ë¦¬ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ë°±ì—”ë“œ ë¡œë“œ ë°©ì‹](#ë°±ì—”ë“œ-ë¡œë“œ-ë°©ì‹)
2. [ë™ì  ë°±ì—”ë“œ ì „í™˜](#ë™ì -ë°±ì—”ë“œ-ì „í™˜--ìƒˆë¡œìš´-ê¸°ëŠ¥)
3. [Batch ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤](#batch-ì²˜ë¦¬-ì‹œë‚˜ë¦¬ì˜¤)
4. [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)
5. [ìµœì í™” íŒ](#ìµœì í™”-íŒ)
6. [ì‹¤ì œ ìš´ì˜ ì‚¬ë¡€](#ì‹¤ì œ-ìš´ì˜-ì‚¬ë¡€)

---

## ë°±ì—”ë“œ ë¡œë“œ ë°©ì‹

**ì´ˆê¸°í™” ì‹œ: ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œ 1ê°œë§Œ ë¡œë“œ**

```python
# __init__ì—ì„œ:
if FASTER_WHISPER_AVAILABLE:
    self._try_faster_whisper()  # ì„±ê³µí•˜ë©´ ì—¬ê¸°ì„œ ë!

if self.backend is None and TRANSFORMERS_AVAILABLE:
    self._try_transformers()

if self.backend is None and WHISPER_AVAILABLE:
    self._try_whisper()
```

**ê²°ê³¼:**
- âœ… faster-whisper ì„±ê³µ â†’ transformers/whisper ë¡œë“œ ì•ˆ í•¨
- âœ… transformersë§Œ ê°€ëŠ¥ â†’ whisper ë¡œë“œ ì•ˆ í•¨
- âœ… whisperë§Œ ê°€ëŠ¥ â†’ ë¡œë“œ

### transcribeì˜ backend íŒŒë¼ë¯¸í„° (êµ¬ë²„ì „)
- âš ï¸ ì´ì œ ë¬´ì‹œë¨ (deprecated)
- ë°±ì—”ë“œë¥¼ ë³€ê²½í•˜ë ¤ë©´ `reload_backend()` ì‚¬ìš©

```python
# êµ¬ë²„ì „ (ë” ì´ìƒ ì‘ë™í•˜ì§€ ì•ŠìŒ)
stt.transcribe(audio, backend="transformers")  # âŒ ë¬´ì‹œë¨

# ì‹ ê·œ ë°©ì‹
stt.reload_backend("transformers")  # âœ… ë°±ì—”ë“œ ì „í™˜
stt.transcribe(audio)               # âœ… ìƒˆ ë°±ì—”ë“œë¡œ ì²˜ë¦¬
```

**ì§€ì›í•˜ëŠ” Backend ì´ë¦„:**

| ì •ì‹ëª… | ë³„ì¹­ | ì„¤ëª… |
|--------|------|------|
| faster-whisper | faster_whisper | CTranslate2 ê¸°ë°˜, ğŸš€ ê°€ì¥ ë¹ ë¦„ |
| transformers | - | HuggingFace ëª¨ë¸, âš¡ ì¤‘ê°„ ì†ë„ |
| openai-whisper | openai_whisper, whisper | OpenAI ê³µì‹ ëª¨ë¸, ğŸ”„ í˜¸í™˜ì„± ìš°ìˆ˜ |


---

## ë™ì  ë°±ì—”ë“œ ì „í™˜ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)

### ğŸ¯ í•µì‹¬ ê°œì„ : reload_backend() ë©”ì„œë“œ

ì´ì œ **ì‘ìš© í”„ë¡œê·¸ë¨ì„ ì¬ì‹œì‘í•˜ì§€ ì•Šê³ ë„ ë°±ì—”ë“œë¥¼ ì „í™˜**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

```python
from stt_engine import WhisperSTT

# ì´ˆê¸°í™” (faster-whisper ìë™ ë¡œë“œ)
stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# í˜„ì¬ ë°±ì—”ë“œ í™•ì¸
backend_info = stt.get_backend_info()
print(f"Current: {backend_info['current_backend']}")  # faster-whisper

# 100ê°œ íŒŒì¼ ì²˜ë¦¬
for file in audio_files[:100]:
    result = stt.transcribe(file, language="ko")
    save_result(result)

# âœ¨ ë°±ì—”ë“œ ë³€ê²½ (ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬)
stt.reload_backend("transformers")

# ë‹¤ë¥¸ 100ê°œ íŒŒì¼ì„ transformersë¡œ ì²˜ë¦¬
for file in audio_files[100:]:
    result = stt.transcribe(file, language="ko")
    save_result(result)
```

### API ì—”ë“œí¬ì¸íŠ¸

**í˜„ì¬ ë°±ì—”ë“œ í™•ì¸:**
```bash
curl http://localhost:8003/backend/current | jq
```

**ë°±ì—”ë“œ ì „í™˜:**
```bash
curl -X POST http://localhost:8003/backend/reload \
  -H "Content-Type: application/json" \
  -d '{"backend": "transformers"}' | jq
```

ìì„¸í•œ ë‚´ìš©ì€ [BACKEND_SWITCHING_GUIDE.md](BACKEND_SWITCHING_GUIDE.md) ì°¸ê³ 

---

## Batch ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¨ì¼ ë°±ì—”ë“œ ìˆœì°¨ ì²˜ë¦¬ (ê¸°ë³¸) âœ… ê¶Œì¥
```python
from stt_engine import WhisperSTT
from pathlib import Path

# ëª¨ë¸ 1íšŒ ë¡œë“œ (faster-whisper)
stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# 100ê°œ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬
audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    result = stt.transcribe(str(audio_file), language="ko")
    results.append({
        "file": audio_file.name,
        "text": result.get("text"),
        "language": result.get("language"),
        "duration": result.get("duration")
    })

import json
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
```

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨: ëª¨ë¸ 1íšŒ ë¡œë“œ, 100ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘ì—ë„ ë©”ëª¨ë¦¬ ê³ ì •
- êµ¬í˜„ ê°„ë‹¨: ê¸°ì¡´ transcribe() ì‚¬ìš©
- ì•ˆì •ì„±: ë¡œë“œëœ ë°±ì—”ë“œ 1ê°œë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì—ëŸ¬ ê°€ëŠ¥ì„± ë‚®ìŒ

**ì„±ëŠ¥:** faster-whisper: 100ê°œ íŒŒì¼ = ~50ì´ˆ

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë™ì  ë°±ì—”ë“œ ì „í™˜ (ì‹ ê·œ!) â­
```python
from stt_engine import WhisperSTT

stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# í˜„ì¬ ë°±ì—”ë“œ í™•ì¸
print(f"Backend: {stt.get_backend_info()['current_backend']}")

# ë°±ì—”ë“œë³„ë¡œ íŒŒì¼ ê·¸ë£¹ ì²˜ë¦¬
backends_and_files = [
    ("faster-whisper", audio_files[:500]),    # ë¹ ë¥¸ ì²˜ë¦¬
    ("transformers", audio_files[500:1000]),   # ì¼ë°˜ ì²˜ë¦¬
    ("openai-whisper", audio_files[1000:])    # ì—¬ìœ ìˆê²Œ ì²˜ë¦¬
]

results = []

for backend_name, files in backends_and_files:
    # ë°±ì—”ë“œ ì „í™˜
    loaded = stt.reload_backend(backend_name)
    print(f"âœ… Switched to {loaded}")
    
    # ì´ ë°±ì—”ë“œë¡œ íŒŒì¼ ì²˜ë¦¬
    for audio_file in files:
        result = stt.transcribe(str(audio_file), language="ko")
        results.append({
            "file": audio_file.name,
            "text": result.get("text"),
            "backend": backend_name
        })

import json
with open("transcribed_multi_backend.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**ì¥ì :**
- ë°±ì—”ë“œë³„ íŠ¹ì„± í™œìš© (ì†ë„, ë©”ëª¨ë¦¬, ì •í™•ë„)
- ë¦¬ì†ŒìŠ¤ ì œì•½ ì‹œ ë°±ì—”ë“œ ì „í™˜ìœ¼ë¡œ ëŒ€ì‘
- ë™ì¼ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ëª¨ë“  ë°±ì—”ë“œ ì‚¬ìš© ê°€ëŠ¥

**ì„±ëŠ¥:** ì´ 1000ê°œ íŒŒì¼ = ~100ì´ˆ (ë°±ì—”ë“œ ì „í™˜ 5ì´ˆ í¬í•¨)

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: API ì„œë²„ë¡œ Batch ì²˜ë¦¬ (ê¶Œì¥ for ìš´ì˜) â­â­
```bash
# 1. Docker ì‹¤í–‰ (faster-whisper ìë™ ë¡œë“œ)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -v $(pwd)/models:/app/models \
  stt-engine:latest

# 2. ë°±ì—”ë“œ í™•ì¸
curl http://localhost:8003/backend/current | jq

# 3. Python í´ë¼ì´ì–¸íŠ¸ë¡œ ìˆœì°¨ ìš”ì²­
```

```python
from pathlib import Path
import requests
import json

audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    with open(audio_file, "rb") as f:
        files = {"file": f}
        data = {"language": "ko"}
        
        response = requests.post(
            "http://localhost:8003/transcribe",
            files=files,
            data=data
        )
        
        if response.status_code == 200:
            result = response.json()
            results.append({
                "file": audio_file.name,
                "text": result.get("text"),
                "backend": result.get("backend")
            })
            print(f"âœ… {audio_file.name}")
        else:
            print(f"âŒ {audio_file.name}: {response.status_code}")

with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**ë°±ì—”ë“œ ì „í™˜ í›„ ì²˜ë¦¬:**
```python
# ì¤‘ê°„ì— ë°±ì—”ë“œ ì „í™˜
requests.post("http://localhost:8003/backend/reload",
              json={"backend": "transformers"})

# ì´í›„ ìš”ì²­ë“¤ì€ transformers ì‚¬ìš©
```

**ì¥ì :**
- ë©”ëª¨ë¦¬: ì„œë²„ ë©”ëª¨ë¦¬ ê³ ì • (ì¬ì‹œì‘ ì•ˆ í•¨)
- í™•ì¥ì„±: ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ ë™ì‹œ ìš”ì²­ ê°€ëŠ¥
- ì•ˆì •ì„±: í•œ ìš”ì²­ ì‹¤íŒ¨ â‰  ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨
- ëª¨ë‹ˆí„°ë§: API ë¡œê·¸ë¡œ ê° íŒŒì¼ ì²˜ë¦¬ ì¶”ì  ê°€ëŠ¥
- ë°±ì—”ë“œ ë™ì  ì „í™˜: APIë¡œ ì–¸ì œë“  ì „í™˜ ê°€ëŠ¥

**ì„±ëŠ¥:** 100ê°œ íŒŒì¼ ìˆœì°¨ ìš”ì²­ = ~60ì´ˆ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ í¬í•¨)

---

## ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ | ì†ë„ | êµ¬í˜„ | ìš©ë„ | íŠ¹ì§• |
|------|-------|------|------|------|------|
| **ìˆœì°¨ ì²˜ë¦¬** | âœ… ë‚®ìŒ | ë³´í†µ | ê°„ë‹¨ | < 100ê°œ | ê¸°ë³¸ ë°©ì‹ |
| **ë™ì  ì „í™˜** | âœ… ë‚®ìŒ | ë³´í†µ | ê°„ë‹¨ | 100-1000ê°œ | ë°±ì—”ë“œ ìµœì í™” â­ |
| **API ì„œë²„** | âœ… ë‚®ìŒ | ë³´í†µ | ê°„ë‹¨ | ëŒ€ê·œëª¨ | 24/7 ì„œë¹„ìŠ¤ â­â­ |
| **ë³‘ë ¬ ì²˜ë¦¬** | âš ï¸ ë†’ìŒ | ë¹ ë¦„ | ë³µì¡ | 1000+ | ê³ ì„±ëŠ¥ í•„ìš” ì‹œ |

### ë°±ì—”ë“œë³„ ì„±ëŠ¥

| ë°±ì—”ë“œ | ì†ë„ (10ì´ˆ ìŒì„±) | ë©”ëª¨ë¦¬ | ì •í™•ë„ | ìš©ë„ |
|--------|-----------------|--------|--------|------|
| faster-whisper | 2-3ì´ˆ (GPU) | 2GB | 95% | ë¹ ë¥¸ ì²˜ë¦¬ (ê¸°ë³¸) |
| transformers | 10-15ì´ˆ (GPU) | 3GB | 95% | ì•ˆì •ì„± ì¤‘ìš” ì‹œ |
| openai-whisper | 20-30ì´ˆ (GPU) | 3GB | 95% | fallback |

# ëª¨ë¸ 1íšŒ ë¡œë“œ
stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# 100ê°œ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬
audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    result = stt.transcribe(
        str(audio_file),
        language="ko",
        backend="faster-whisper"  # íŠ¹ì • ë°±ì—”ë“œ ì§€ì •
    )
    results.append({
        "file": audio_file.name,
        "text": result.get("text"),
        "language": result.get("language"),
        "duration": result.get("duration")
    })

# ê²°ê³¼ ì €ì¥
import json
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
```

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨: ëª¨ë¸ 1íšŒ ë¡œë“œ, 100ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘ì—ë„ ë©”ëª¨ë¦¬ ì¦ê°€ ìµœì†Œ
- êµ¬í˜„ ê°„ë‹¨: ê¸°ì¡´ transcribe() ì‚¬ìš©
- Backend ìœ ì—°ì„±: ê° íŒŒì¼ë§ˆë‹¤ ë‹¤ë¥¸ backend ì„ íƒ ê°€ëŠ¥

**ì„±ëŠ¥ ì˜ˆìƒ:**
- faster-whisper: 8ì´ˆ ìŒì„± = ~0.5ì´ˆ (GPU)
- 100ê°œ íŒŒì¼ = ~50ì´ˆ

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë³‘ë ¬ ì²˜ë¦¬ (ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤)
```python
from stt_engine import WhisperSTT
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

# ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ì„ ê°œë³„ ë¡œë“œ (ë©”ëª¨ë¦¬ ì¦ê°€)
def transcribe_file(audio_path):
    stt = WhisperSTT(
        "models/openai_whisper-large-v3-turbo",
        device="cuda"  # âš ï¸ ì£¼ì˜: GPU ë©”ëª¨ë¦¬ ì¦ê°€
    )
    return stt.transcribe(audio_path, language="ko")

audio_files = list(Path("audio/samples").glob("**/*.wav"))

# 4ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì²˜ë¦¬
with ProcessPoolExecutor(max_workers=4) as executor:
    futures = {
        executor.submit(transcribe_file, str(f)): f.name
        for f in audio_files
    }
    
    results = []
    for future in as_completed(futures):
        filename = futures[future]
        try:
            result = future.result()
            results.append({"file": filename, **result})
            print(f"âœ… {filename} ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {filename} ì‹¤íŒ¨: {e}")

print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ")
```

**ì£¼ì˜ì‚¬í•­:**
- âš ï¸ GPU ë©”ëª¨ë¦¬: í”„ë¡œì„¸ìŠ¤ë‹¹ ëª¨ë¸ ë©”ëª¨ë¦¬ í•„ìš” (e.g., 4 Ã— 2GB = 8GB)
- âš ï¸ CPU ë©”ëª¨ë¦¬: ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
- âœ… ì†ë„: ëŒ€ì‹  ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ (CPU ì½”ì–´ í™œìš©)

**ê¶Œì¥ ì„¤ì •:**
```python
# GPU ë©”ëª¨ë¦¬ 4GBì¸ ê²½ìš°
max_workers = 2  # 2ê°œ í”„ë¡œì„¸ìŠ¤ë§Œ (2GB Ã— 2)

# GPU ë©”ëª¨ë¦¬ 16GBì¸ ê²½ìš°
max_workers = 4  # 4ê°œ í”„ë¡œì„¸ìŠ¤ (2GB Ã— 4)
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: API ì„œë²„ (ê¶Œì¥ â­)
```bash
# 1. Docker ì‹¤í–‰ (ëª¨ë¸ 1íšŒ ë¡œë“œ, ë©”ëª¨ë¦¬ ê³ ì •)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/audio/samples:/app/audio/samples \
  stt-engine:cuda129-rhel89-v1.5

# 2. Python í´ë¼ì´ì–¸íŠ¸ë¡œ ìˆœì°¨ ìš”ì²­
from pathlib import Path
import requests
import json

audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    with open(audio_file, "rb") as f:
        files = {"file": f}
        data = {"language": "ko", "backend": "faster-whisper"}
        
        response = requests.post(
            "http://localhost:8003/transcribe",
            files=files,
            data=data
        )
        
        if response.status_code == 200:
            result = response.json()
            results.append({
                "file": audio_file.name,
                "text": result.get("text"),
                "language": result.get("language")
            })
            print(f"âœ… {audio_file.name}")
        else:
            print(f"âŒ {audio_file.name}: {response.status_code}")

# ê²°ê³¼ ì €ì¥
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**ì¥ì :**
- ë©”ëª¨ë¦¬: ì„œë²„ ë©”ëª¨ë¦¬ ê³ ì • (ì¬ì‹œì‘ ì•ˆ í•¨)
- í™•ì¥ì„±: ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ ë™ì‹œ ìš”ì²­ ê°€ëŠ¥
- ì•ˆì •ì„±: í•œ ìš”ì²­ ì‹¤íŒ¨ â‰  ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨
- ëª¨ë‹ˆí„°ë§: API ë¡œê·¸ë¡œ ê° íŒŒì¼ ì²˜ë¦¬ ì¶”ì  ê°€ëŠ¥

**ì„±ëŠ¥:**
- 100ê°œ íŒŒì¼ ìˆœì°¨ ìš”ì²­ = ~60ì´ˆ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ í¬í•¨)

---

## ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì†ë„ | êµ¬í˜„ ë³µì¡ë„ | ì¶”ì²œ ìš©ë„ |
|------|-----------|------|-----------|---------|
| **ìˆœì°¨ ì²˜ë¦¬** (í˜„ì¬) | âœ… ë‚®ìŒ | ë³´í†µ | âœ… ê°„ë‹¨ | ì†Œê·œëª¨ (< 100ê°œ) |
| **ë³‘ë ¬ ì²˜ë¦¬** | âš ï¸ ë†’ìŒ | â­ ë¹ ë¦„ | ë³µì¡ | ì¤‘ê·œëª¨ (100-1000ê°œ) |
| **API ì„œë²„** | âœ… ë‚®ìŒ | ë³´í†µ | â­ ê°„ë‹¨ | ëŒ€ê·œëª¨, ì§€ì† ì„œë¹„ìŠ¤ |

---

## ìµœì í™” íŒ

### 1ï¸âƒ£ ë°±ì—”ë“œ í™•ì¸ ë° ì„ íƒ
```python
from stt_engine import WhisperSTT

stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# ë¡œë“œëœ ë°±ì—”ë“œ í™•ì¸
backend_info = stt.get_backend_info()
print(f"Current: {backend_info['current_backend']}")
print(f"Available: {backend_info['available_backends']}")

# í•„ìš”ì‹œ ë°±ì—”ë“œ ë³€ê²½
if some_condition:
    stt.reload_backend("transformers")
```

### 2ï¸âƒ£ Batch ì²˜ë¦¬ ì¤‘ ë©”ëª¨ë¦¬ ìµœì í™”
```python
import gc
import torch

for i, audio_file in enumerate(audio_files):
    result = stt.transcribe(audio_file, language="ko")
    results.append(result)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)
    if (i + 1) % 50 == 0:
        gc.collect()
        torch.cuda.empty_cache()
        print(f"âœ… {i + 1}/{len(audio_files)} ì²˜ë¦¬, ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
```

### 3ï¸âƒ£ API ì‚¬ìš© ì‹œ ì¬ì‹œë„ ë° íƒ€ì„ì•„ì›ƒ
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def requests_with_retry(retries=3, timeout=300):
    session = requests.Session()
    retry = Retry(total=retries, backoff_factor=1)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

session = requests_with_retry()

for audio_file in audio_files:
    with open(audio_file, "rb") as f:
        files = {"file": f}
        
        try:
            response = session.post(
                "http://localhost:8003/transcribe",
                files=files,
                timeout=300  # 5ë¶„
            )
            if response.status_code == 200:
                result = response.json()
                results.append(result)
        except requests.exceptions.Timeout:
            print(f"â±ï¸  íƒ€ì„ì•„ì›ƒ: {audio_file.name}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {audio_file.name}: {e}")
```

### 4ï¸âƒ£ ë°±ì—”ë“œë³„ íŒŒì¼ ê·¸ë£¹í™” (ì‹ ê·œ!)
```python
import json
from pathlib import Path

# íŒŒì¼ì„ í¬ê¸°ë³„ë¡œ ê·¸ë£¹í™”
small_files = [f for f in audio_files if f.stat().st_size < 1_000_000]
large_files = [f for f in audio_files if f.stat().st_size >= 1_000_000]

results = []

# ì‘ì€ íŒŒì¼: faster-whisper (ë¹ ë¦„)
stt.reload_backend("faster-whisper")
for f in small_files:
    result = stt.transcribe(str(f), language="ko")
    results.append({"file": f.name, "text": result["text"], "backend": "faster-whisper"})

# í° íŒŒì¼: transformers (ì•ˆì •ì )
stt.reload_backend("transformers")
for f in large_files:
    result = stt.transcribe(str(f), language="ko")
    results.append({"file": f.name, "text": result["text"], "backend": "transformers"})

with open("results.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

---

## ì‹¤ì œ ìš´ì˜ ì‚¬ë¡€

### ìš´ì˜ ì‚¬ë¡€ 1: Docker + API ì„œë²„ (ê¶Œì¥) â­â­
```bash
# Step 1: Docker ì‹¤í–‰ (ëª¨ë¸ ë¡œë“œ: ~30ì´ˆ)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  --gpus all \
  -e STT_DEVICE=cuda \
  -e STT_COMPUTE_TYPE=float16 \
  -v $(pwd)/models:/app/models \
  stt-engine:latest

# Step 2: í˜„ì¬ ë°±ì—”ë“œ í™•ì¸
curl http://localhost:8003/backend/current | jq

# Step 3: ë°°ì¹˜ ì²˜ë¦¬ (Python)
python batch_transcribe.py audio/samples/

# Step 4: ì¤‘ê°„ì— ë°±ì—”ë“œ ë³€ê²½ í•„ìš”ì‹œ
curl -X POST http://localhost:8003/backend/reload \
  -H "Content-Type: application/json" \
  -d '{"backend": "transformers"}'

# Step 5: ë‹¤ì‹œ ì²˜ë¦¬ ê³„ì†
python batch_transcribe.py audio/samples/2/
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©:**
- ì²˜ìŒ ìš”ì²­: ~2.5GB (faster-whisper ë¡œë“œ)
- ë°±ì—”ë“œ ì „í™˜: ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒˆ ë°±ì—”ë“œ ë¡œë“œ (~2-3GB, ì´ì „ ì •ë¦¬ë¨)
- ì´í›„ ìš”ì²­: 0MB ì¶”ê°€ (ì¬ì‚¬ìš©)
- 100ê°œ íŒŒì¼ ì²˜ë¦¬ í›„: ì—¬ì „íˆ ~2.5GB (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ)

### ìš´ì˜ ì‚¬ë¡€ 2: ëŒ€ê·œëª¨ ë°°ì¹˜ (1000+ íŒŒì¼)
```bash
# ë³‘ë ¬ í´ë¼ì´ì–¸íŠ¸ë¡œ ìš”ì²­ (Python)
```

```python
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
import requests
import json

def transcribe_with_api(audio_file):
    try:
        with open(audio_file, "rb") as f:
            response = requests.post(
                "http://localhost:8003/transcribe",
                files={"file": f},
                data={"language": "ko"},
                timeout=300
            )
        if response.status_code == 200:
            result = response.json()
            return {"file": audio_file.name, "text": result.get("text"), "success": True}
        else:
            return {"file": audio_file.name, "success": False, "status": response.status_code}
    except Exception as e:
        return {"file": audio_file.name, "success": False, "error": str(e)}

audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

# 10ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ìš”ì²­ (API ì„œë²„ëŠ” 1ê°œ, í´ë¼ì´ì–¸íŠ¸ë§Œ ë³‘ë ¬)
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(transcribe_with_api, f) for f in audio_files]
    
    for future in futures:
        result = future.result()
        results.append(result)
        status = "âœ…" if result["success"] else "âŒ"
        print(f"{status} {result['file']}")

# ê²°ê³¼ ì €ì¥
with open("transcribed_batch.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… {sum(1 for r in results if r['success'])}/{len(results)} ì™„ë£Œ")
```

**íŠ¹ì§•:**
- ì„œë²„ ë©”ëª¨ë¦¬: ê³ ì • (1ê°œ ëª¨ë¸)
- í´ë¼ì´ì–¸íŠ¸: ë³‘ë ¬ ìš”ì²­ (I/O ëŒ€ê¸° ì¤‘ì— ë‹¤ë¥¸ íŒŒì¼ ì²˜ë¦¬)
- ì†ë„: ~30-50% í–¥ìƒ (ë„¤íŠ¸ì›Œí¬ I/O ë³‘ë ¬í™”)
- ì•ˆì •ì„±: í•œ íŒŒì¼ ì‹¤íŒ¨ â‰  ì „ì²´ ì‹¤íŒ¨

---

## ì„±ëŠ¥ ë¹„êµ (ìµœì¢…)

### ë°©ì‹ë³„ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ | ì†ë„ (100ê°œ) | êµ¬í˜„ | ì•ˆì •ì„± | ì¶”ì²œ |
|------|--------|-------------|------|--------|------|
| **ìˆœì°¨ ì²˜ë¦¬** | 2-3GB | ~50ì´ˆ | â­ | â­â­â­ | ì†Œê·œëª¨ |
| **ë™ì  ì „í™˜** | 2-3GB | ~50ì´ˆ | â­ | â­â­â­ | ìµœì í™” í•„ìš” ì‹œ â­ |
| **API (ìˆœì°¨)** | 2-3GB | ~60ì´ˆ | â­ | â­â­â­ | ìš´ì˜ í™˜ê²½ â­â­ |
| **API (ë³‘ë ¬)** | 2-3GB | ~40ì´ˆ | â­â­ | â­â­â­ | ëŒ€ê·œëª¨ â­â­â­ |

### ë°±ì—”ë“œë³„ ì„±ëŠ¥

| ë°±ì—”ë“œ | 10ì´ˆ ìŒì„± (GPU) | ë©”ëª¨ë¦¬ | ì •í™•ë„ | ì¶”ì²œ |
|--------|-----------------|--------|--------|------|
| faster-whisper | **2-3ì´ˆ** | 2GB | 95% | ğŸ¥‡ ê¸°ë³¸ |
| transformers | 10-15ì´ˆ | 3GB | 95% | ğŸ¥ˆ ì•ˆì •ì„± |
| openai-whisper | 20-30ì´ˆ | 3GB | 95% | ğŸ¥‰ fallback |

---

## ê²°ë¡ 

### âœ… í˜„ì¬ êµ¬ì¡°ì˜ ì¥ì 
1. ì´ˆê¸°í™” ì‹œ ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œë§Œ ë¡œë“œ
2. transcribe() í˜¸ì¶œë§ˆë‹¤ ë©”ëª¨ë¦¬ ì¦ê°€ ì—†ìŒ
3. **ìƒˆë¡œìš´ reload_backend()ë¡œ ëŸ°íƒ€ì„ ë°±ì—”ë“œ ì „í™˜ ê°€ëŠ¥**
4. get_backend_info()ë¡œ í˜„ì¬ ìƒíƒœ í™•ì¸ ê°€ëŠ¥
5. 100ê°œ+ íŒŒì¼ ì²˜ë¦¬ì— ìµœì í™”ë¨

### ğŸ“Š ê¶Œì¥ Batch ì²˜ë¦¬ ë°©ì‹
- **ì†Œê·œëª¨ (< 100ê°œ)**: ìˆœì°¨ ì²˜ë¦¬ (python)
- **ì¤‘ê·œëª¨ (100-1000ê°œ)**: API ì„œë²„ + ë™ì  ì „í™˜
- **ëŒ€ê·œëª¨ (1000+)**: API ì„œë²„ + ë³‘ë ¬ í´ë¼ì´ì–¸íŠ¸
- **ìµœì í™” í•„ìš”**: reload_backend()ë¡œ ë°±ì—”ë“œ ì „í™˜

### ğŸ“š ê´€ë ¨ ë¬¸ì„œ
- [BACKEND_SWITCHING_GUIDE.md](BACKEND_SWITCHING_GUIDE.md) - API ìƒì„¸ ë¬¸ì„œ
- [QUICKSTART.md](QUICKSTART.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

ë” í•„ìš”í•œ ìµœì í™”ê°€ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ğŸš€

