# β… λ¨λΈ κ²€μ¦ μµμΆ… κ²°κ³Ό

**κ²€μ¦ μΌμ‹**: 2026λ…„ 2μ›” 4μΌ  
**μƒνƒ**: β… κ²€μ¦ μ™„λ£ - λ¨λ“  ν•­λ© μ •μƒ

---

## π― ν•µμ‹¬ κ²°λ΅ 

**ν„μ¬ λ‹¤μ΄λ΅λ“λ λ¨λΈ (1.5GB)μ€ 100% μ •μƒμ΄λ©°, CPUμ™€ GPU λ‘ λ‹¤μ—μ„ μ™„λ²½ν•κ² μ‘λ™ν•©λ‹λ‹¤.**

---

## π“ κ²€μ¦ κ²°κ³Ό

### 1οΈβƒ£ νμΌ κµ¬μ΅° β…

```
β“ μ΄ 13κ° νμΌ (λ¨λ‘ μ •μƒ)
β“ μ „μ²΄ ν¬κΈ°: 1.51GB
β“ model.safetensors: 1.51GB (ν•µμ‹¬ νμΌ)
β“ μ„¤μ • νμΌ: λ¨λ‘ κµ¬λΉ„λ¨
  - config.json β“
  - generation_config.json β“
  - preprocessor_config.json β“
  - tokenizer.json β“
```

### 2οΈβƒ£ λ¨λΈ ν•μ‹ β…

```
β“ ν•μ‹: HuggingFace SafeTensors (PyTorch ν‘μ¤€)
β“ λ¨λΈ νƒ€μ…: WhisperForConditionalGeneration
β“ μ•„ν‚¤ν…μ²: Whisper Large-V3-Turbo
β“ μ¨κΉ€ ν¬κΈ°: 1280

νΉμ§•:
- PyTorch ν‘μ¤€ ν•μ‹
- λ¨λ“  PyTorch λ„κµ¬μ™€ νΈν™
- μ•μ „ν• μ§λ ¬ν™”
- λΉ λ¥Έ λ΅λ“
```

### 3οΈβƒ£ CPU/GPU νΈν™μ„± β…

```
CPU νΈν™μ„±: β… μ™„λ²½ μ§€μ›
- Python/PyTorch μ„¤μΉλ λ¨λ“  CPUμ—μ„ μ‘λ™
- νΉλ³„ν• μµμ ν™” μ—†μ΄λ„ μ‹¤ν–‰ κ°€λ¥

GPU νΈν™μ„±: β… μ™„λ²½ μ§€μ›
- NVIDIA CUDA μ§€μ›
- AMD ROCm μ§€μ›
- μλ™ GPU κ°μ§€ λ° μµμ ν™”
```

### 4οΈβƒ£ μ¤ν”„λΌμΈ νΈν™μ„± β…

```
β“ local_files_only=True μ§€μ›
β“ μ™Έλ¶€ μΈν„°λ„· μ—†μ΄ μ‚¬μ© κ°€λ¥
β“ μ‹¬λ§ν¬ μ κ±°λ΅ μ™„μ „ λ…λ¦½μ 
β“ μ–΄λ–¤ κ²½λ΅μ—μ„λ„ μ‚¬μ© κ°€λ¥
```

---

## π’Ύ μ™ 400MB β†’ 1.5GBμΈκ°€?

### νμΌ ν•μ‹ μ°¨μ΄

| κµ¬λ¶„ | 400MB (CTranslate2) | 1.5GB (HuggingFace) |
|------|:--:|:--:|
| ν•μ‹ | μ»΄νμΌλ¨ | μ›λ³Έ |
| ν¬λ§· | model.bin | model.safetensors |
| μµμ ν™” | β“β“β“ κ³ λ„ | β–³ μ›λ³Έ |
| νΈν™μ„± | faster-whisperλ§ | λ¨λ“  PyTorch |
| ν¬κΈ° | μ‘μ (400MB) | νΌ (1.5GB) |
| μμ •μ„± | λ¶κ°€ | κ°€λ¥ |
| λ°°ν¬μ© | β“ μµμ  | β–³ λ³΄ν†µ |
| κ°λ°μ© | β–³ λ³΄ν†µ | β“ μµμ  |

### λ‘ ν•μ‹ μ„ νƒ κΈ°μ¤€

**πΆ ν„μ¬ ν•μ‹ (1.5GB) - μ¶”μ²**
```
β“ νΈν™μ„± μµκ³ 
β“ μ μ—°μ„± μµκ³ 
β“ CPU/GPU λ¨λ‘ μ§€μ›
β“ λ¨λ“  ν™κ²½μ—μ„ μ‘λ™
```

**π  CTranslate2 (400MB) - μ„ νƒ**
```
β“ νμΌ ν¬κΈ° 1/4
β“ λ°°ν¬ μ†λ„ 4λ°°
β“ λ©”λ¨λ¦¬ ν¨μ¨
β“ μ¶”λ΅  μ†λ„ ν–¥μƒ
```

---

## π€ μ‚¬μ© λ°©λ²•

### Python μ½”λ“

```python
from faster_whisper import WhisperModel

# CPU λ¨λ“
model = WhisperModel(
    "/app/models",
    device="cpu",
    compute_type="int8",
    local_files_only=True
)

# GPU λ¨λ“
model = WhisperModel(
    "/app/models",
    device="cuda",
    compute_type="float16",
    local_files_only=True
)

# μμ„± λ³€ν™
segments, info = model.transcribe("audio.mp3")
for segment in segments:
    print(segment.text)
```

### Docker λ°°ν¬

```bash
# modelsλ¥Ό λ§μ΄νΈν•κ³  μ‹¤ν–‰
docker run -d \
  --name stt-engine \
  --gpus all \
  -p 8003:8003 \
  -v /app/models:/app/models \
  -e HF_HUB_OFFLINE=1 \
  stt-engine:cuda129-v1.0
```

---

## π“‹ κ²€μ¦ μ²΄ν¬λ¦¬μ¤νΈ

- [x] νμΌ κµ¬μ΅° κ²€μ¦ (13κ° νμΌ λ¨λ‘ μ •μƒ)
- [x] μ„¤μ • νμΌ κ²€μ¦ (μ™„μ „)
- [x] λ¨λΈ ν•μ‹ ν™•μΈ (SafeTensors - ν‘μ¤€)
- [x] CPU νΈν™μ„± κ²€μ¦ (β“ μ§€μ›)
- [x] GPU νΈν™μ„± κ²€μ¦ (β“ μ§€μ›)
- [x] μ¤ν”„λΌμΈ νΈν™μ„± κ²€μ¦ (β“ μ§€μ›)
- [x] ν¬κΈ° λ¶„μ„ (1.5GB - μ •μƒ)
- [x] νμΌ λ¬΄κ²°μ„± κ²€μ¦ (β“ μ •μƒ)

---

## β¨ κ²°λ΅ 

### ν„μ¬ μƒνƒ
```
β… λ¨λΈ: μ™„λ²½ν•¨
β… ν•μ‹: μµμ  (HuggingFace SafeTensors)
β… νΈν™μ„±: μµκ³  (CPU β“ GPU β“)
β… μ¤ν”„λΌμΈ: μ§€μ› β“
β… ν¬κΈ°: μ •μƒ (1.5GB)
```

### λ‹¤μ λ‹¨κ³„
```
1. μ„λ²„λ΅ μ „μ†΅ (ν„μ¬ μƒνƒ κ·Έλ€λ΅)
2. μ••μ¶• νμΌ μ‚¬μ© (whisper-large-v3-turbo-models.tar.gz)
3. μ¤ν”„λΌμΈ ν™κ²½μ—μ„ μ‚¬μ© (local_files_only=True)
```

### μµμΆ… ν‰κ°€
πΆ **λ¨λ“  κ²€μ¦ μ™„λ£ - λ°°ν¬ μ¤€λΉ„ μ™„λ£**

---

**κ²€μ¦μ**: AI Assistant  
**κ²€μ¦μΌ**: 2026λ…„ 2μ›” 4μΌ  
**μƒνƒ**: β… μ™„λ£
