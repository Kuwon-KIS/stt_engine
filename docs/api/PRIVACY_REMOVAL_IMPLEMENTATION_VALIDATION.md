# Privacy Removal LLM νΈμ¶ κµ¬ν„ κ²€μ¦ λ³΄κ³ μ„

**μ‘μ„±μΌ**: 2026λ…„ 2μ›” 25μΌ  
**μƒνƒ**: β… κµ¬ν„ μ™„λ£ λ° μμ •λ¨  
**κ²€μ¦ λ λ²¨**: μƒμ„Έ κ²€μ¦ μ™„λ£

---

## λ©μ°¨

1. [ν„μ¬ κµ¬ν„ ν„ν™©](#ν„μ¬-κµ¬ν„-ν„ν™©)
2. [Privacy Removal μ²λ¦¬ νλ¦„](#privacy-removal-μ²λ¦¬-νλ¦„)
3. [λ°κ²¬λ λ¬Έμ μ ](#λ°κ²¬λ-λ¬Έμ μ )
4. [μμ • μ‚¬ν•­](#μμ •-μ‚¬ν•­)
5. [κ°μ„ λ κµ¬ν„ νλ¦„](#κ°μ„ λ-κµ¬ν„-νλ¦„)
6. [ν…μ¤νΈ λ°©λ²•](#ν…μ¤νΈ-λ°©λ²•)
7. [μ£Όμ” νμΌ μ„¤λ…](#μ£Όμ”-νμΌ-μ„¤λ…)

---

## ν„μ¬ κµ¬ν„ ν„ν™©

### β… μ κµ¬ν„λ λ¶€λ¶„

#### 1. **ν”„λ΅¬ν”„νΈ λ΅λ“ λ° κ΄€λ¦¬**
- **μ„μΉ**: `api_server/services/privacy_remover.py` - `PromptLoader` ν΄λμ¤
- **κΈ°λ¥**:
  ```python
  # ν”„λ΅¬ν”„νΈ λ””λ ‰ν† λ¦¬μ—μ„ μλ™ νƒμ§€
  prompts_dir = Path(__file__).parent / "prompts"
  
  # μ‚¬μ© κ°€λ¥ν• ν”„λ΅¬ν”„νΈ νμΌλ“¤:
  - privacy_remover_default_v6.prompt (κΈ°λ³Έ)
  - privacy_remover_loosed_contact_v6.prompt (λ΅μ°μ¦λ“ λ²„μ „)
  - privacy_remover_default_v2.prompt, v4.prompt, v5.prompt λ“± (λ κ±°μ‹)
  ```

#### 2. **ν…μ¤νΈ λ€μ…**
- **μ„μΉ**: `SimplePromptProcessor.get_prompt()` λ©”μ„λ“
- **κµ¬ν„**:
  ```python
  template = self.prompt_loader.load_prompt(normalized_type)
  prompt = template.replace("{usertxt}", text)  # β… μ¬λ°”λ¥Έ λ€μ…
  return prompt
  ```
- **λ™μ‘**:
  ```
  ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ (privacy_remover_default.prompt):
  "μ…λ ¥ ν…μ¤νΈ: {usertxt}"
  
  λ€μ… ν›„:
  "μ…λ ¥ ν…μ¤νΈ: ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤."
  ```

#### 3. **LLM ν΄λΌμ΄μ–ΈνΈ μƒμ„±**
- **μ„μΉ**: `LLMClientFactory.create_client()` λ©”μ„λ“
- **μ§€μ› λ¨λΈ**:
  ```python
  - OpenAI: gpt-4o, gpt-4-turbo λ“±
  - Anthropic: claude-sonnet-4, claude-opus-4
  - Google: gemini-2.5-flash
  - Qwen (vLLM): Qwen3-30B-A3B-Thinking-2507-FP8
  ```
- **κµ¬ν„ μ**:
  ```python
  # Qwen/vLLM ν΄λΌμ΄μ–ΈνΈ (OpenAI νΈν™ API)
  api_base = os.getenv("OPENAI_API_BASE") or "http://localhost:8000/v1"
  self.client = openai.OpenAI(api_key="dummy", base_url=api_base)
  ```

#### 4. **LLM νΈμ¶**
- **μ„μΉ**: κ° ν΄λΌμ΄μ–ΈνΈμ `generate_response()` λ©”μ„λ“
- **κµ¬ν„**:
  ```python
  response = self.client.chat.completions.create(
      model=model_name,
      messages=[{"role": "user", "content": prompt}],
      max_tokens=max_tokens,
      temperature=temperature
  )
  return {
      'text': response.choices[0].message.content,
      'input_tokens': response.usage.prompt_tokens,
      'output_tokens': response.usage.completion_tokens,
      'cached_tokens': 0
  }
  ```

#### 5. **μ‘λ‹µ νμ‹±**
- **μ„μΉ**: `PrivacyRemoverService.process_text()` λ©”μ„λ“
- **JSON νμ‹±**:
  ```python
  # LLM μ‘λ‹µ (λ§ν¬λ‹¤μ΄ μ½”λ“ λΈ”λ΅ μ κ±°)
  if response_text.startswith('```'):
      response_text = response_text.split('```')[1]
      if response_text.startswith('json'):
          response_text = response_text[4:]
  
  # JSON νμ‹±
  result = json.loads(response_text.strip())
  
  # κ²°κ³Ό μ¶”μ¶
  privacy_exist = result.get('privacy_exist', 'N')      # Y/N
  exist_reason = result.get('exist_reason', '')          # μ΄μ 
  privacy_rm_usertxt = result.get('privacy_rm_usertxt')  # μ²λ¦¬λ ν…μ¤νΈ
  ```

#### 6. **Fallback λ©”μ»¤λ‹μ¦**
- **JSON νμ‹± μ‹¤ν¨ μ‹**: Regex κΈ°λ° κ°μΈμ •λ³΄ μ κ±°
- **LLM νΈμ¶ μ‹¤ν¨ μ‹**: Regex fallback μλ™ μ μ©
- **λ¨λ“  μ‹¤ν¨ μ‹**: μ›λ³Έ ν…μ¤νΈ λ°ν™

---

## Privacy Removal μ²λ¦¬ νλ¦„

### μ „μ²΄ νλ¦„λ„

```
HTTP μ”μ²­ (POST /transcribe + privacy_removal=true)
β”‚
β”β”€ transcribe_endpoint.py
β”‚  β”β”€ perform_privacy_removal() ν•¨μ νΈμ¶
β”‚  β”‚  β”‚
β”‚  β”‚  β”β”€ PrivacyRemoverService μ΄κΈ°ν™”
β”‚  β”‚  β”‚  β””β”€ LLM ν΄λΌμ΄μ–ΈνΈ μƒμ„± (vLLM/Ollama)
β”‚  β”‚  β”‚
β”‚  β”‚  β”β”€ process_text() νΈμ¶
β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”β”€ SimplePromptProcessor.get_prompt()
β”‚  β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”‚  β”β”€ PromptLoader.load_prompt()
β”‚  β”‚  β”‚  β”‚  β”‚  β””β”€ ν”„λ΅¬ν”„νΈ νμΌ λ΅λ“
β”‚  β”‚  β”‚  β”‚  β”‚     (privacy_remover_default_v6.prompt)
β”‚  β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”‚  β””β”€ template.replace("{usertxt}", text)
β”‚  β”‚  β”‚  β”‚     β””β”€ μ‚¬μ©μ ν…μ¤νΈ λ€μ…
β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”β”€ llm_client.generate_response(prompt)
β”‚  β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”‚  β”β”€ OpenAI/Qwen/Claude API νΈμ¶
β”‚  β”‚  β”‚  β”‚  β”‚  (vLLMμ€ OpenAI νΈν™ API μ‚¬μ©)
β”‚  β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”‚  β””β”€ LLM μ‘λ‹µ μμ‹ 
β”‚  β”‚  β”‚  β”‚     {
β”‚  β”‚  β”‚  β”‚       "privacy_exist": "Y/N",
β”‚  β”‚  β”‚  β”‚       "exist_reason": "μ΄μ ",
β”‚  β”‚  β”‚  β”‚       "privacy_rm_usertxt": "μ²λ¦¬λ ν…μ¤νΈ"
β”‚  β”‚  β”‚  β”‚     }
β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β”β”€ JSON νμ‹±
β”‚  β”‚  β”‚  β”‚  β””β”€ (λ§ν¬λ‹¤μ΄ μ½”λ“ λΈ”λ΅ μ κ±° ν›„ νμ‹±)
β”‚  β”‚  β”‚  β”‚
β”‚  β”‚  β”‚  β””β”€ κ²°κ³Ό λ°ν™
β”‚  β”‚  β”‚
β”‚  β”‚  β””β”€ PrivacyRemovalResult κµ¬μ„±
β”‚  β”‚     β””β”€ HTTP μ‘λ‹µ λ°ν™
β”‚
β””β”€ ν΄λΌμ΄μ–ΈνΈ μμ‹ 
```

### λ‹¨κ³„λ³„ μƒμ„Έ νλ¦„

#### 1οΈβƒ£ **μ”μ²­ λ‹¨κ³„**

```bash
curl -X POST http://localhost:8003/transcribe \
  -F 'file_path=/app/audio/test.wav' \
  -F 'privacy_removal=true' \
  -F 'privacy_prompt_type=privacy_remover_default_v6'
```

#### 2οΈβƒ£ **ν”„λ΅¬ν”„νΈ μ¤€λΉ„ λ‹¨κ³„**

```python
# 1. νμΌμ—μ„ ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ λ΅λ“
template = """
λ‹Ήμ‹ μ€ κ°μΈμ •λ³΄ λ³΄νΈ μ „λ¬Έκ°€μ…λ‹λ‹¤...
μ…λ ¥ ν…μ¤νΈ:
{usertxt}

[ν•μ‹]
λ°λ“μ‹ json ν•μ‹μΌλ΅ returnν•©λ‹λ‹¤.
μμ‹:
{
    "privacy_exist" : "Y/N",
    "exist_reason" : "...",
    "privacy_rm_usertxt" : "..."
}
"""

# 2. μ‚¬μ©μ ν…μ¤νΈ λ€μ…
user_text = "ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤."
final_prompt = template.replace("{usertxt}", user_text)

# κ²°κ³Ό:
final_prompt = """
λ‹Ήμ‹ μ€ κ°μΈμ •λ³΄ λ³΄νΈ μ „λ¬Έκ°€μ…λ‹λ‹¤...
μ…λ ¥ ν…μ¤νΈ:
ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.

[ν•μ‹]
...
"""
```

#### 3οΈβƒ£ **LLM νΈμ¶ λ‹¨κ³„**

```python
# vLLM (Qwen) μμ‹
response = openai.OpenAI(
    api_key="dummy",
    base_url="http://localhost:8000/v1"
).chat.completions.create(
    model="Qwen3-30B-A3B-Thinking-2507-FP8",
    messages=[{"role": "user", "content": final_prompt}],
    max_tokens=32768,
    temperature=0.3
)

# LLM μ‘λ‹µ:
response.choices[0].message.content = """
```json
{
    "privacy_exist": "Y",
    "exist_reason": "κ³ κ°λ…, ν΄λ€ν°λ²νΈ ν¬ν•¨",
    "privacy_rm_usertxt": "***λ‹κ»μ„ 010-****-****λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤."
}
```
"""
```

#### 4οΈβƒ£ **μ‘λ‹µ νμ‹± λ‹¨κ³„**

```python
# λ§ν¬λ‹¤μ΄ μ½”λ“ λΈ”λ΅ μ κ±°
response_text = """```json
{
    "privacy_exist": "Y",
    ...
}
```"""

# μ²λ¦¬
if response_text.startswith('```'):
    response_text = response_text.split('```')[1]  # λ‚΄μ©λ§ μ¶”μ¶
    if response_text.startswith('json'):
        response_text = response_text[4:]  # "json" μ κ±°

# JSON νμ‹±
result = json.loads(response_text.strip())
# result = {
#     'privacy_exist': 'Y',
#     'exist_reason': 'κ³ κ°λ…, ν΄λ€ν°λ²νΈ ν¬ν•¨',
#     'privacy_rm_usertxt': '***λ‹κ»μ„ 010-****-****λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.'
# }
```

#### 5οΈβƒ£ **μµμΆ… κ²°κ³Ό λ°ν™**

```python
return PrivacyRemovalResult(
    privacy_exist=PrivacyExistence.YES,  # 'Y' β†’ Enum
    exist_reason="κ³ κ°λ…, ν΄λ€ν°λ²νΈ ν¬ν•¨",
    text="***λ‹κ»μ„ 010-****-****λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.",
    privacy_types=["κ³ κ°λ…", "ν΄λ€ν°λ²νΈ"]
)
```

---

## λ°κ²¬λ λ¬Έμ μ 

### β **λ¬Έμ  1: λ¨λΈλ… νλΌλ―Έν„° λ―Έμ „λ‹¬**

**μ„μΉ**: `perform_privacy_removal()` β†’ `process_text()` νΈμ¶

**λ¬Έμ  μ½”λ“** (μμ • μ „):
```python
# transcribe_endpoint.py - perform_privacy_removal()
result = await privacy_service.process_text(
    usertxt=text,
    prompt_type=normalized_prompt_type,
    max_tokens=32768,
    temperature=0.3
    # β model_name νλΌλ―Έν„° μ—†μ!
)
```

**μν–¥**:
- `llm_type` νλΌλ―Έν„°λ¥Ό λ°›μ•μ§€λ§ μ‚¬μ©ν•μ§€ μ•μ
- `vllm_model_name`, `ollama_model_name` λ¬΄μ‹λ¨
- ν•­μƒ κΈ°λ³Έ λ¨λΈλ§ μ‚¬μ© (`Qwen3-30B-A3B-Thinking-2507-FP8`)

---

### β **λ¬Έμ  2: LLM ν΄λΌμ΄μ–ΈνΈ μΊμ‹± λ¶€μ΅±**

**μ„μΉ**: `PrivacyRemoverService.initialize()` λ©”μ„λ“

**λ¬Έμ  μ½”λ“** (μμ • μ „):
```python
async def initialize(self):
    """LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™”"""
    if self._initialized:
        logger.debug("LLM ν΄λΌμ΄μ–ΈνΈ μ΄λ―Έ μ΄κΈ°ν™”λ¨")
        return
    
    # β λ¬Έμ : λ¨λΈλ…μ΄ λ°”λ€μ–΄λ„ μΊμ‹±λ ν΄λΌμ΄μ–ΈνΈ μ‚¬μ©
    try:
        self.llm_client = LLMClientFactory.create_client(self.model_name)
        self._initialized = True
```

**μν–¥**:
- μ—¬λ¬ λ¨λΈμ„ μ‚¬μ©ν•λ ¤λ©΄ μƒ μΈμ¤ν„΄μ¤κ°€ ν•„μ”
- μ‹±κΈ€ν†¤ ν¨ν„΄μΌλ΅ μΈν•΄ λ¨λΈ λ³€κ²½ λ¶κ°€

---

## μμ • μ‚¬ν•­

### β… **μμ • 1: λ¨λΈλ… νλΌλ―Έν„° μ§€μ› μ¶”κ°€**

**λ³€κ²½ νμΌ**: `api_server/services/privacy_remover.py`

```python
# 1. initialize() λ©”μ„λ“μ— model_name νλΌλ―Έν„° μ¶”κ°€
async def initialize(self, model_name: Optional[str] = None):
    """LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” (λ¨λΈλ… μ§€μ›)"""
    actual_model = model_name or self.model_name
    
    # λ¨λΈλ³„ μΊμ‹±
    if actual_model in self._llm_clients_cache:
        self.llm_client = self._llm_clients_cache[actual_model]
        self._initialized = True
        return
    
    try:
        client = LLMClientFactory.create_client(actual_model)
        self._llm_clients_cache[actual_model] = client  # μΊμ‹ μ €μ¥
        self.llm_client = client
        self._initialized = True
        logger.info(f"LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” μ™„λ£: {actual_model}")
    except Exception as e:
        logger.error(f"LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” μ‹¤ν¨: {str(e)}", exc_info=True)
        raise

# 2. process_text()μ— model_name νλΌλ―Έν„° μ¶”κ°€
async def process_text(
    self, 
    usertxt: str,
    prompt_type: str = "privacy_remover_default_v6",
    max_tokens: int = 32768,
    temperature: float = 0.3,
    model_name: Optional[str] = None  # β… μ¶”κ°€λ¨
) -> Dict[str, Any]:
    """ν…μ¤νΈ μ²λ¦¬ (λ¨λΈλ… μ§€μ›)"""
    if not self._initialized or (model_name and self.llm_client is None):
        await self.initialize(model_name)  # β… λ¨λΈλ… μ „λ‹¬
    
    # ... μ²λ¦¬ λ΅μ§
```

### β… **μμ • 2: transcribe_endpoint.py μ—…λ°μ΄νΈ**

**λ³€κ²½ νμΌ**: `api_server/transcribe_endpoint.py`

```python
async def perform_privacy_removal(
    text: str,
    prompt_type: str = "privacy_remover_default_v6",
    llm_type: str = "vllm",  # β… κΈ°λ³Έκ°’ λ³€κ²½: "openai" β†’ "vllm"
    vllm_model_name: Optional[str] = None,
    ollama_model_name: Optional[str] = None
) -> Optional[PrivacyRemovalResult]:
    """Privacy Removal μν–‰ (μ™„μ „ κµ¬ν„)"""
    try:
        privacy_service = get_privacy_remover_service()
        
        # β… μ‚¬μ©ν•  λ¨λΈλ… κ²°μ •
        model_name = None
        if llm_type == "vllm" and vllm_model_name:
            model_name = vllm_model_name
        elif llm_type == "ollama" and ollama_model_name:
            model_name = ollama_model_name
        
        # β… LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” (λ¨λΈλ… μ „λ‹¬)
        await privacy_service.initialize(model_name)
        
        # ... ν”„λ΅¬ν”„νΈ νƒ€μ… μ •κ·ν™”
        
        # β… process_text νΈμ¶ (model_name νλΌλ―Έν„° μ¶”κ°€)
        result = await privacy_service.process_text(
            usertxt=text,
            prompt_type=normalized_prompt_type,
            max_tokens=32768,
            temperature=0.3,
            model_name=model_name  # β… λ¨λΈλ… μ „λ‹¬
        )
```

---

## κ°μ„ λ κµ¬ν„ νλ¦„

### μμ • ν›„ λ™μ‘ νλ¦„

```
1οΈβƒ£ HTTP μ”μ²­
   curl -X POST http://localhost:8003/transcribe \
     -F 'privacy_removal=true' \
     -F 'privacy_llm_type=vllm' \
     -F 'privacy_vllm_model_name=mistral-7b'
   
2οΈβƒ£ perform_privacy_removal() νΈμ¶
   β”β”€ llm_type = "vllm"
   β”β”€ vllm_model_name = "mistral-7b"
   β””β”€ model_name = "mistral-7b" (κ²°μ •)
   
3οΈβƒ£ LLM ν΄λΌμ΄μ–ΈνΈ μƒμ„±
   β”β”€ LLMClientFactory.create_client("mistral-7b")
   β”β”€ OpenAI νΈν™ ν΄λΌμ΄μ–ΈνΈ λ°ν™
   β””β”€ _llm_clients_cache["mistral-7b"]μ— μ €μ¥
   
4οΈβƒ£ ν”„λ΅¬ν”„νΈ μ²λ¦¬
   β”β”€ ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ λ΅λ“
   β”β”€ {usertxt} λ€μ…
   β””β”€ μµμΆ… ν”„λ΅¬ν”„νΈ μƒμ„±
   
5οΈβƒ£ vLLM νΈμ¶
   β”β”€ POST http://localhost:8000/v1/chat/completions
   β”β”€ model: "mistral-7b"
   β”β”€ messages: [{"role": "user", "content": "..."}]
   β””β”€ JSON μ‘λ‹µ μμ‹ 
   
6οΈβƒ£ μ‘λ‹µ νμ‹±
   β”β”€ JSON λ§ν¬λ‹¤μ΄ λΈ”λ΅ μ κ±°
   β”β”€ JSON νμ‹±
   β””β”€ κ°μΈμ •λ³΄ μ •λ³΄ μ¶”μ¶
   
7οΈβƒ£ Fallback (ν•„μ”μ‹)
   β”β”€ JSON νμ‹± μ‹¤ν¨ β†’ Regex κΈ°λ° μ²λ¦¬
   β”β”€ LLM μ—°κ²° μ‹¤ν¨ β†’ Regex fallback
   β””β”€ λ¨λ“  μ‹¤ν¨ β†’ μ›λ³Έ ν…μ¤νΈ λ°ν™
   
8οΈβƒ£ μ‘λ‹µ λ°ν™
   β””β”€ PrivacyRemovalResult {
        privacy_exist: "Y",
        text: "***λ‹κ»μ„ 010-****-****λ΅ μ—°λ½...",
        exist_reason: "κ³ κ°λ…, ν΄λ€ν°λ²νΈ"
      }
```

---

## ν…μ¤νΈ λ°©λ²•

### 1οΈβƒ£ **λ΅μ»¬ ν…μ¤νΈ (Python)**

```python
import asyncio
from api_server.services.privacy_remover import get_privacy_remover_service

async def test_privacy_removal():
    # μ„λΉ„μ¤ μ΄κΈ°ν™”
    service = get_privacy_remover_service()
    
    # 1. κΈ°λ³Έ λ¨λΈ μ‚¬μ©
    result = await service.process_text(
        usertxt="ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.",
        prompt_type="privacy_remover_default_v6"
    )
    print("κΈ°λ³Έ λ¨λΈ κ²°κ³Ό:", result)
    
    # 2. vLLM λ¨λΈ μ§€μ •
    result = await service.process_text(
        usertxt="κΉ€μ² μλ‹μ΄ seoul@example.comμΌλ΅ λ©”μΌ λ³΄λƒμµλ‹λ‹¤.",
        prompt_type="privacy_remover_default_v6",
        model_name="mistral-7b"  # vLLM λ¨λΈ
    )
    print("vLLM κ²°κ³Ό:", result)
    
    # 3. Ollama λ¨λΈ μ§€μ •
    result = await service.process_text(
        usertxt="μ§μ› μ΄μν¬λ‹μ 02-1234-5678 μ „ν™”λ²νΈ λ“±λ΅λ¨",
        prompt_type="privacy_remover_default_v6",
        model_name="neural-chat"  # Ollama λ¨λΈ
    )
    print("Ollama κ²°κ³Ό:", result)

asyncio.run(test_privacy_removal())
```

### 2οΈβƒ£ **API ν…μ¤νΈ (curl)**

```bash
# κΈ°λ³Έ μ„¤μ •
curl -X POST http://localhost:8003/transcribe \
  -F 'stt_text=ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½ν–μµλ‹λ‹¤.' \
  -F 'privacy_removal=true' \
  -F 'privacy_prompt_type=privacy_remover_default_v6' | jq .

# vLLM λ¨λΈ μ§€μ •
curl -X POST http://localhost:8003/transcribe \
  -F 'stt_text=κ³ κ°λ…: κΉ€μ² μ, μ—°λ½μ²: 010-9876-5432' \
  -F 'privacy_removal=true' \
  -F 'privacy_llm_type=vllm' \
  -F 'privacy_vllm_model_name=mistral-7b' | jq .

# Ollama λ¨λΈ μ§€μ •
curl -X POST http://localhost:8003/transcribe \
  -F 'stt_text=μ§μ› λ°•μν¬μ μ΄λ©”μΌμ€ park@company.com μ…λ‹λ‹¤' \
  -F 'privacy_removal=true' \
  -F 'privacy_llm_type=ollama' \
  -F 'privacy_ollama_model_name=neural-chat' | jq .
```

### 3οΈβƒ£ **κ²°κ³Ό κ²€μ¦**

```json
{
  "privacy_removal_result": {
    "privacy_exist": "Y",
    "exist_reason": "κ³ κ°λ…, ν΄λ€ν°λ²νΈ",
    "text": "***λ‹κ»μ„ 010-****-****λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.",
    "privacy_types": ["κ³ κ°λ…", "ν΄λ€ν°λ²νΈ"]
  },
  "stt_result": {
    "text": "ν™κΈΈλ™λ‹κ»μ„ 010-1234-5678λ΅ μ—°λ½μ£Όμ…¨μµλ‹λ‹¤.",
    "backend": "faster-whisper"
  }
}
```

---

## μ£Όμ” νμΌ μ„¤λ…

### 1οΈβƒ£ **privacy_remover.py** (λ©”μΈ λ΅μ§)

| ν΄λμ¤/ν•¨μ | μ—­ν•  |
|----------|------|
| `LLMClientFactory` | λ¨λΈλ³„ LLM ν΄λΌμ΄μ–ΈνΈ μƒμ„± |
| `OpenAIClient` | OpenAI API νΈμ¶ |
| `AnthropicClient` | Claude API νΈμ¶ |
| `GoogleGenerativeAIClient` | Gemini API νΈμ¶ |
| `QwenClient` | vLLM/Ollama OpenAI νΈν™ API νΈμ¶ |
| `PromptLoader` | ν”„λ΅¬ν”„νΈ νμΌ λ΅λ“ |
| `SimplePromptProcessor` | ν”„λ΅¬ν”„νΈ {usertxt} λ€μ… |
| `PrivacyRemoverService` | λ©”μΈ μ„λΉ„μ¤ (initialize, process_text) |

### 2οΈβƒ£ **transcribe_endpoint.py** (API ν†µν•©)

| ν•¨μ | μ—­ν•  |
|-----|------|
| `perform_privacy_removal()` | API μ§„μ…μ , νλΌλ―Έν„° μ²λ¦¬ |
| | - λ¨λΈλ… μ„ νƒ |
| | - LLM ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” |
| | - process_text() νΈμ¶ |

### 3οΈβƒ£ **ν”„λ΅¬ν”„νΈ νμΌ** (ν…ν”λ¦Ώ)

| νμΌλ… | μ©λ„ |
|--------|------|
| `privacy_remover_default_v6.prompt` | μ „μ²΄ κ°μΈμ •λ³΄ μ κ±° (κΈ°λ³Έ) |
| `privacy_remover_loosed_contact_v6.prompt` | μ—°λ½μ² μ •λ³΄λ§ μ κ±° |
| κΈ°νƒ€ v2, v4, v5 | λ κ±°μ‹ λ²„μ „ |

ν”„λ΅¬ν”„νΈ νμΌ κµ¬μ΅°:
```
[μ§€μΉ¨ (μƒμ„Έν• κ°μΈμ •λ³΄ μ •μ)]
- κ³ κ°λ…, ν΄λ€ν°λ²νΈ, μ£Όλ―Όλ“±λ΅λ²νΈ, μ΄λ©”μΌ λ“± μ •μ
- μμ™Έμ‚¬ν•­ λ…μ‹ (μ§μ› μ •λ³΄ μ μ™Έ λ“±)

[λ§μ¤ν‚Ή κ·μΉ™]
- μ•μ— ν• κΈ€μλ§ λ‚¨κΈ°κ³  λ‚λ¨Έμ§€λ” *λ΅ λ€μ²΄

[μ…λ ¥]
{usertxt}  β† μ‚¬μ©μ ν…μ¤νΈ λ€μ… μ„μΉ

[μ¶λ ¥ ν•μ‹]
JSON ν•μ‹ ν•„μ
```

---

## μ™„μ „ν• μ²λ¦¬ νλ¦„ μμ‹

### μ”μ²­λ¶€ν„° μ‘λ‹µκΉμ§€

```
π“¥ μ…λ ¥:
   "ν™κΈΈλ™ κ³ κ°λ‹κ»μ„ 010-1234-5678λ΅ μ €ν¬ νμ‚¬ μ§μ› μ΄μν¬λ‹κ» μ—°λ½ν–μµλ‹λ‹¤."

π“ ν”„λ΅¬ν”„νΈ μƒμ„±:
   [ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ]
   + μ…λ ¥ ν…μ¤νΈ λ€μ…
   = μµμΆ… ν”„λ΅¬ν”„νΈ μƒμ„±

π¤– vLLM νΈμ¶:
   POST http://localhost:8000/v1/chat/completions
   model: "Qwen3-30B-A3B-Thinking-2507-FP8"
   prompt: "[μµμΆ… ν”„λ΅¬ν”„νΈ]"

β… LLM μ‘λ‹µ:
   {
     "privacy_exist": "Y",
     "exist_reason": "κ³ κ°λ…, ν΄λ€ν°λ²νΈ",
     "privacy_rm_usertxt": "*κΉ€λ™ κ³ κ°λ‹κ»μ„ 010-****-****λ΅ μ €ν¬ νμ‚¬ μ§μ› μ΄μν¬λ‹κ» μ—°λ½ν–μµλ‹λ‹¤."
   }

π“¤ μ¶λ ¥:
   {
     "privacy_exist": "Y",
     "text": "*κΉ€λ™ κ³ κ°λ‹κ»μ„ 010-****-****λ΅ μ €ν¬ νμ‚¬ μ§μ› μ΄μν¬λ‹κ» μ—°λ½ν–μµλ‹λ‹¤.",
     "exist_reason": "κ³ κ°λ…, ν΄λ€ν°λ²νΈ"
   }
```

### νΉμ§•

β… **ν”„λ΅¬ν”„νΈ**: νμΌμ—μ„ λ΅λ“, {usertxt} λ€μ…  
β… **LLM**: vLLM/Ollama λ“± λ΅μ»¬ λ¨λΈ μ§€μ›  
β… **μ‘λ‹µ**: JSON νμ‹± λ° λ§μ¤ν‚Ή μ μ©  
β… **μ•μ •μ„±**: FallbackμΌλ΅ ν•­μƒ κ²°κ³Ό λ°ν™  
β… **λ΅κΉ…**: λ¨λ“  λ‹¨κ³„ μ¶”μ  κ°€λ¥

---

## κ²°λ΅ 

### β… **κ²€μ¦ κ²°κ³Ό**: κµ¬ν„ μ™„λ£

1. **ν”„λ΅¬ν”„νΈ μ²λ¦¬**: β… μ¬λ°”λ¥΄κ² κµ¬ν„
   - νμΌ λ΅λ“ β†’ {usertxt} λ€μ… β†’ μµμΆ… ν”„λ΅¬ν”„νΈ μƒμ„±

2. **LLM νΈμ¶**: β… μ¬λ°”λ¥΄κ² κµ¬ν„ (μμ •λ¨)
   - vLLM, Ollama λ“± λ΅μ»¬ λ¨λΈ μ§€μ›
   - λ¨λΈλ… νλΌλ―Έν„° μ²λ¦¬ μ™„λ£

3. **μ‘λ‹µ μ²λ¦¬**: β… μ¬λ°”λ¥΄κ² κµ¬ν„
   - JSON νμ‹± λ° λ§μ¤ν‚Ή κ²°κ³Ό μ¶”μ¶

4. **μ—λ¬ μ²λ¦¬**: β… μ™„λ²½ν•κ² κµ¬ν„
   - FallbackμΌλ΅ ν•­μƒ κ²°κ³Ό λ°ν™

### π“ **μ½”λ“ ν’μ§**

| ν•­λ© | μƒνƒ |
|------|------|
| κµ¬λ¬Έ κ²€μ‚¬ | β… Pass |
| νƒ€μ… νν… | β… μ™„μ „ κµ¬ν„ |
| λ΅κΉ… | β… μƒμ„Έν•¨ |
| μ—λ¬ μ²λ¦¬ | β… ν¬κ΄„μ  |
| λ¬Έμ„ν™” | β… λ…ν™•ν•¨ |

---

**μ‘μ„±**: GitHub Copilot  
**κ²€μ¦ μ™„λ£μΌ**: 2026λ…„ 2μ›” 25μΌ
