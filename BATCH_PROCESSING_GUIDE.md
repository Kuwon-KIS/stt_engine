# Batch ìŒì„± ì²˜ë¦¬ ìµœì í™” ê°€ì´ë“œ

## âš ï¸ ì¤‘ìš” ì‚¬í•­: Backend ë¡œë“œ ë°©ì‹

**í˜„ì¬ êµ¬ì¡°: ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œ 1ê°œë§Œ ë¡œë“œ**

```python
# __init__ì—ì„œ:
if FASTER_WHISPER_AVAILABLE:
    self._try_faster_whisper()  # ì„±ê³µí•˜ë©´ ì—¬ê¸°ì„œ ë!

if self.backend is None and TRANSFORMERS_AVAILABLE:  # â† None ì²´í¬
    self._try_transformers()

if self.backend is None and WHISPER_AVAILABLE:      # â† None ì²´í¬
    self._try_whisper()
```

**ê²°ê³¼:**
- âœ… faster-whisper ì„±ê³µ â†’ transformers/whisper ë¡œë“œ ì•ˆ í•¨
- âœ… transformersë§Œ ê°€ëŠ¥ â†’ whisper ë¡œë“œ ì•ˆ í•¨
- âœ… whisperë§Œ ê°€ëŠ¥ â†’ ë¡œë“œ

**transcribeì˜ backend íŒŒë¼ë¯¸í„°:**
- âœ… **ë¡œë“œëœ ë°±ì—”ë“œë§Œ** ì‚¬ìš© ê°€ëŠ¥
- âŒ **ë¡œë“œë˜ì§€ ì•Šì€ ë°±ì—”ë“œëŠ” ì—ëŸ¬** ë°œìƒ

```python
# ì˜ˆ: faster-whisper ë¡œë“œëœ ê²½ìš°
stt.transcribe(audio, backend="faster-whisper")  # âœ… ê°€ëŠ¥
stt.transcribe(audio, backend="transformers")     # âŒ ì—ëŸ¬! ë¡œë“œ ì•ˆ ë¨
```

---

## í˜„ì¬ êµ¬ì¡° ë¶„ì„

### ë©”ëª¨ë¦¬ íš¨ìœ¨: âœ… ìš°ìˆ˜
- **ëª¨ë¸ ë¡œë“œ**: `__init__ì—ì„œ 1ê°œ ë°±ì—”ë“œë§Œ ë¡œë“œ`
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: transcribeë§ˆë‹¤ ìƒˆë¡œ ë¡œë“œí•˜ì§€ ì•ŠìŒ
- **ê²°ë¡ **: Batch ì²˜ë¦¬ì— ìµœì í™”ë¨ (í•˜ë‚˜ì˜ ë°±ì—”ë“œì— ëŒ€í•´ì„œë§Œ)

### êµ¬ì¡°
```python
stt = WhisperSTT(model_path)  # ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œë§Œ ë¡œë“œ

# ì´ì œ 100ê°œ íŒŒì¼ì„ ê°™ì€ ë°±ì—”ë“œë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ê³ ì •)
for audio_file in audio_files:
    result = stt.transcribe(audio_file)  # ë¡œë“œëœ ëª¨ë¸ ì¬ì‚¬ìš©
```

---

## Batch ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ìˆœì°¨ ì²˜ë¦¬ (í˜„ì¬ ë°©ì‹) âœ… ê¶Œì¥
```python
from stt_engine import WhisperSTT
from pathlib import Path

# ëª¨ë¸ 1íšŒ ë¡œë“œ (ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œ)
stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")
# ì˜ˆ: faster-whisper ë¡œë“œë¨

# 100ê°œ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬
audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    result = stt.transcribe(
        str(audio_file),
        language="ko"
        # backend ì§€ì • ê°€ëŠ¥í•˜ì§€ë§Œ, ë¡œë“œëœ ë°±ì—”ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥
    )
    results.append({
        "file": audio_file.name,
        "text": result.get("text"),
        "language": result.get("language"),
        "duration": result.get("duration")
    })

# ê²°ê³¼ ì €ì¥
import json
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ (faster-whisper)")
```

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨: ëª¨ë¸ 1íšŒ ë¡œë“œ, 100ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘ì—ë„ ë©”ëª¨ë¦¬ ê³ ì •
- êµ¬í˜„ ê°„ë‹¨: ê¸°ì¡´ transcribe() ì‚¬ìš©
- ì•ˆì •ì„±: ë¡œë“œëœ ë°±ì—”ë“œ 1ê°œë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì—ëŸ¬ ê°€ëŠ¥ì„± ë‚®ìŒ

**ì„±ëŠ¥ ì˜ˆìƒ:**
- faster-whisper: 8ì´ˆ ìŒì„± = ~0.5ì´ˆ (GPU)
- 100ê°œ íŒŒì¼ = ~50ì´ˆ

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë‹¤ì–‘í•œ ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤)
ì—¬ëŸ¬ ë°±ì—”ë“œë¥¼ ë¹„êµí•˜ë ¤ë©´ ë³„ë„ ì¸ìŠ¤í„´ìŠ¤ í•„ìš”:

```python
from stt_engine import WhisperSTT
import json

audio_file = "audio/samples/test.wav"
results = {}

# ê° ë°±ì—”ë“œë³„ë¡œ ë³„ë„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
backends = []

try:
    stt_faster = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")
    backends.append(("faster-whisper", stt_faster))
except RuntimeError:
    print("âŒ faster-whisper ë¡œë“œ ì‹¤íŒ¨")

try:
    # âš ï¸ ì£¼ì˜: transformers ë¡œë“œí•˜ë ¤ë©´ faster-whisperì´ ì‹¤íŒ¨í•´ì•¼ í•¨
    # í˜„ì¬ êµ¬ì¡°ìƒ ë¶ˆê°€ëŠ¥ (ì²« ë²ˆì§¸ ì„±ê³µí•˜ë©´ ë‹¤ë¥¸ ê²ƒ ì•ˆ ë¡œë“œë¨)
except RuntimeError:
    print("âŒ transformers ë¡œë“œ ì‹¤íŒ¨")

# ë¡œë“œëœ ë°±ì—”ë“œë“¤ë¡œ í…ŒìŠ¤íŠ¸
for backend_name, stt in backends:
    result = stt.transcribe(audio_file, language="ko")
    results[backend_name] = result.get("text")

print(json.dumps(results, indent=2, ensure_ascii=False))
```

**ë¬¸ì œì :**
- âš ï¸ í˜„ì¬ êµ¬ì¡°ì—ì„œëŠ” ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œë§Œ ë¡œë“œë¨
- ì—¬ëŸ¬ ë°±ì—”ë“œë¥¼ ë™ì‹œì— ë¡œë“œí•  ìˆ˜ ì—†ìŒ
- ë°±ì—”ë“œ ë¹„êµ í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ì˜ ê°œì„  í•„ìš”

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: API ì„œë²„ (ê¶Œì¥ â­)
```bash
# 1. Docker ì‹¤í–‰ (íŠ¹ì • ë°±ì—”ë“œ ë¡œë“œ, ë©”ëª¨ë¦¬ ê³ ì •)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/audio/samples:/app/audio/samples \
  stt-engine:cuda129-rhel89-v1.5
# faster-whisper ë¡œë“œë¨

# 2. Python í´ë¼ì´ì–¸íŠ¸ë¡œ ìˆœì°¨ ìš”ì²­
from pathlib import Path
import requests
import json

audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    with open(audio_file, "rb") as f:
        files = {"file": f}
        data = {"language": "ko"}
        
        response = requests.post(
            "http://localhost:8003/transcribe",
            files=files,
            data=data
        )
        
        if response.status_code == 200:
            result = response.json()
            results.append({
                "file": audio_file.name,
                "text": result.get("text"),
                "language": result.get("language")
            })
            print(f"âœ… {audio_file.name}")
        else:
            print(f"âŒ {audio_file.name}: {response.status_code}")

# ê²°ê³¼ ì €ì¥
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**ì¥ì :**
- ë©”ëª¨ë¦¬: ì„œë²„ ë©”ëª¨ë¦¬ ê³ ì • (ì¬ì‹œì‘ ì•ˆ í•¨)
- í™•ì¥ì„±: ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ ë™ì‹œ ìš”ì²­ ê°€ëŠ¥
- ì•ˆì •ì„±: í•œ ìš”ì²­ ì‹¤íŒ¨ â‰  ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨
- ëª¨ë‹ˆí„°ë§: API ë¡œê·¸ë¡œ ê° íŒŒì¼ ì²˜ë¦¬ ì¶”ì  ê°€ëŠ¥

**ì„±ëŠ¥:**
- 100ê°œ íŒŒì¼ ìˆœì°¨ ìš”ì²­ = ~60ì´ˆ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ í¬í•¨)

---

## ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ | ì†ë„ | êµ¬í˜„ | ìš©ë„ |
|------|-------|------|------|------|
| **ìˆœì°¨ ì²˜ë¦¬** (í˜„ì¬) | âœ… ë‚®ìŒ | ë³´í†µ | ê°„ë‹¨ | ì†Œê·œëª¨ (< 100ê°œ) |
| **ë³‘ë ¬ ì²˜ë¦¬** | âš ï¸ ë†’ìŒ | ë¹ ë¦„ | ë³µì¡ | ì¤‘ê·œëª¨ (100-1000ê°œ) |
| **API ì„œë²„** | âœ… ë‚®ìŒ | ë³´í†µ | ê°„ë‹¨ | ëŒ€ê·œëª¨, ì§€ì† ì„œë¹„ìŠ¤ â­ |

---

## ìµœì í™” íŒ

### 1ï¸âƒ£ Backend í™•ì¸
```python
stt = WhisperSTT(model_path)

# ë¡œë“œëœ ë°±ì—”ë“œ í™•ì¸
if hasattr(stt.backend, '_backend_type'):
    print(f"ë¡œë“œëœ ë°±ì—”ë“œ: {stt.backend._backend_type}")
else:
    print(f"ë¡œë“œëœ ë°±ì—”ë“œ: {type(stt.backend).__name__}")
```

### 2ï¸âƒ£ Batch ì²˜ë¦¬ ì¤‘ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
```python
import gc
import torch

for audio_file in audio_files:
    result = stt.transcribe(audio_file)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)
    if len(results) % 10 == 0:
        gc.collect()
        torch.cuda.empty_cache()
```

### 3ï¸âƒ£ íƒ€ì„ì•„ì›ƒ ì„¤ì • (API ì‚¬ìš© ì‹œ)
```python
# ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
response = requests.post(
    "http://localhost:8003/transcribe",
    files=files,
    timeout=300  # 5ë¶„ (ê¸´ ìŒì„± íŒŒì¼ìš©)
)
```

---

## ì‹¤ì œ ìš´ì˜ ì‚¬ë¡€

### EC2 + Docker (ê¶Œì¥)
```bash
# Step 1: Docker ì‹¤í–‰ (ëª¨ë¸ ë¡œë“œ ì‹œê°„: ~30ì´ˆ)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -e STT_COMPUTE_TYPE=int8 \
  -v $(pwd)/models:/app/models \
  stt-engine:cuda129-rhel89-v1.5
# â†’ faster-whisper ìë™ ë¡œë“œ (CTranslate2 ëª¨ë¸ ìˆìœ¼ë¯€ë¡œ)

# Step 2: ëŒ€ëŸ‰ íŒŒì¼ ì²˜ë¦¬
python batch_transcribe.py audio/samples/ > results.json

# Step 3: ì„œë²„ ì¬ì‚¬ìš© (ë‹¤ìŒ ë°°ì¹˜ ìš”ì²­ ì‹œ)
# ëª¨ë¸ì€ ì—¬ì „íˆ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆìŒ (ë©”ëª¨ë¦¬ ì¦ê°€ ì—†ìŒ)
python batch_transcribe.py audio/samples/2/ > results2.json
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©:**
- ì²˜ìŒ ìš”ì²­: ~2.5GB (faster-whisper ë¡œë“œ)
- ì´í›„ ìš”ì²­: 0MB ì¶”ê°€ (ì¬ì‚¬ìš©)
- 100ê°œ íŒŒì¼ ì²˜ë¦¬ í›„: ì—¬ì „íˆ ~2.5GB (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ)

---

## ê²°ë¡ 

**í˜„ì¬ WhisperSTT êµ¬ì¡°:**

1. âœ… `__init__`ì—ì„œ **ì²« ë²ˆì§¸ ì„±ê³µí•œ ë°±ì—”ë“œ 1ê°œë§Œ ë¡œë“œ**
2. âœ… `transcribe()` í˜¸ì¶œë§ˆë‹¤ ë©”ëª¨ë¦¬ ì¦ê°€ ì—†ìŒ (ê°™ì€ ë°±ì—”ë“œ ì‚¬ìš©)
3. âš ï¸ `backend` íŒŒë¼ë¯¸í„°ëŠ” **ë¡œë“œëœ ë°±ì—”ë“œë¥¼ ì§€ì •**í•  ë•Œë§Œ ì‚¬ìš©
4. âœ… 100ê°œ ì´ìƒ íŒŒì¼ ë™ì¼ ë°±ì—”ë“œë¡œ ì²˜ë¦¬í•  ë•Œ **ë§¤ìš° íš¨ìœ¨ì **

**ê¶Œì¥ Batch ì²˜ë¦¬ ë°©ì‹:**
- ì†Œê·œëª¨ (< 100ê°œ): ìˆœì°¨ ì²˜ë¦¬ (í˜„ì¬ ì½”ë“œ)
- ì¤‘ê·œëª¨ (100-1000ê°œ): API ì„œë²„ + ìˆœì°¨ ìš”ì²­
- ëŒ€ê·œëª¨ (1000+): API ì„œë²„ + ë³‘ë ¬ í´ë¼ì´ì–¸íŠ¸ or ë©”ì‹œì§€ í

**ë§Œì•½ ë‹¤ì–‘í•œ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´:**
- ê¸°ëŠ¥ ê°œì„ ì´ í•„ìš” (ëª¨ë“  ë°±ì—”ë“œë¥¼ ë™ì‹œì— ë¡œë“œí•˜ëŠ” êµ¬ì¡°ë¡œ ë³€ê²½)
- ë˜ëŠ” ê° ë°±ì—”ë“œë³„ ë³„ë„ Docker ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰

ë” í•„ìš”í•œ ìµœì í™”ê°€ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ğŸš€

```python
from stt_engine import WhisperSTT
from pathlib import Path

# ëª¨ë¸ 1íšŒ ë¡œë“œ
stt = WhisperSTT("models/openai_whisper-large-v3-turbo", device="cuda")

# 100ê°œ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬
audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    result = stt.transcribe(
        str(audio_file),
        language="ko",
        backend="faster-whisper"  # íŠ¹ì • ë°±ì—”ë“œ ì§€ì •
    )
    results.append({
        "file": audio_file.name,
        "text": result.get("text"),
        "language": result.get("language"),
        "duration": result.get("duration")
    })

# ê²°ê³¼ ì €ì¥
import json
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… {len(results)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
```

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨: ëª¨ë¸ 1íšŒ ë¡œë“œ, 100ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘ì—ë„ ë©”ëª¨ë¦¬ ì¦ê°€ ìµœì†Œ
- êµ¬í˜„ ê°„ë‹¨: ê¸°ì¡´ transcribe() ì‚¬ìš©
- Backend ìœ ì—°ì„±: ê° íŒŒì¼ë§ˆë‹¤ ë‹¤ë¥¸ backend ì„ íƒ ê°€ëŠ¥

**ì„±ëŠ¥ ì˜ˆìƒ:**
- faster-whisper: 8ì´ˆ ìŒì„± = ~0.5ì´ˆ (GPU)
- 100ê°œ íŒŒì¼ = ~50ì´ˆ

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë³‘ë ¬ ì²˜ë¦¬ (ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤)
```python
from stt_engine import WhisperSTT
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

# ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ì„ ê°œë³„ ë¡œë“œ (ë©”ëª¨ë¦¬ ì¦ê°€)
def transcribe_file(audio_path):
    stt = WhisperSTT(
        "models/openai_whisper-large-v3-turbo",
        device="cuda"  # âš ï¸ ì£¼ì˜: GPU ë©”ëª¨ë¦¬ ì¦ê°€
    )
    return stt.transcribe(audio_path, language="ko")

audio_files = list(Path("audio/samples").glob("**/*.wav"))

# 4ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì²˜ë¦¬
with ProcessPoolExecutor(max_workers=4) as executor:
    futures = {
        executor.submit(transcribe_file, str(f)): f.name
        for f in audio_files
    }
    
    results = []
    for future in as_completed(futures):
        filename = futures[future]
        try:
            result = future.result()
            results.append({"file": filename, **result})
            print(f"âœ… {filename} ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ {filename} ì‹¤íŒ¨: {e}")

print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ")
```

**ì£¼ì˜ì‚¬í•­:**
- âš ï¸ GPU ë©”ëª¨ë¦¬: í”„ë¡œì„¸ìŠ¤ë‹¹ ëª¨ë¸ ë©”ëª¨ë¦¬ í•„ìš” (e.g., 4 Ã— 2GB = 8GB)
- âš ï¸ CPU ë©”ëª¨ë¦¬: ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
- âœ… ì†ë„: ëŒ€ì‹  ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ (CPU ì½”ì–´ í™œìš©)

**ê¶Œì¥ ì„¤ì •:**
```python
# GPU ë©”ëª¨ë¦¬ 4GBì¸ ê²½ìš°
max_workers = 2  # 2ê°œ í”„ë¡œì„¸ìŠ¤ë§Œ (2GB Ã— 2)

# GPU ë©”ëª¨ë¦¬ 16GBì¸ ê²½ìš°
max_workers = 4  # 4ê°œ í”„ë¡œì„¸ìŠ¤ (2GB Ã— 4)
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: API ì„œë²„ (ê¶Œì¥ â­)
```bash
# 1. Docker ì‹¤í–‰ (ëª¨ë¸ 1íšŒ ë¡œë“œ, ë©”ëª¨ë¦¬ ê³ ì •)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/audio/samples:/app/audio/samples \
  stt-engine:cuda129-rhel89-v1.5

# 2. Python í´ë¼ì´ì–¸íŠ¸ë¡œ ìˆœì°¨ ìš”ì²­
from pathlib import Path
import requests
import json

audio_files = list(Path("audio/samples").glob("**/*.wav"))
results = []

for audio_file in audio_files:
    with open(audio_file, "rb") as f:
        files = {"file": f}
        data = {"language": "ko", "backend": "faster-whisper"}
        
        response = requests.post(
            "http://localhost:8003/transcribe",
            files=files,
            data=data
        )
        
        if response.status_code == 200:
            result = response.json()
            results.append({
                "file": audio_file.name,
                "text": result.get("text"),
                "language": result.get("language")
            })
            print(f"âœ… {audio_file.name}")
        else:
            print(f"âŒ {audio_file.name}: {response.status_code}")

# ê²°ê³¼ ì €ì¥
with open("transcribed.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**ì¥ì :**
- ë©”ëª¨ë¦¬: ì„œë²„ ë©”ëª¨ë¦¬ ê³ ì • (ì¬ì‹œì‘ ì•ˆ í•¨)
- í™•ì¥ì„±: ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ ë™ì‹œ ìš”ì²­ ê°€ëŠ¥
- ì•ˆì •ì„±: í•œ ìš”ì²­ ì‹¤íŒ¨ â‰  ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨
- ëª¨ë‹ˆí„°ë§: API ë¡œê·¸ë¡œ ê° íŒŒì¼ ì²˜ë¦¬ ì¶”ì  ê°€ëŠ¥

**ì„±ëŠ¥:**
- 100ê°œ íŒŒì¼ ìˆœì°¨ ìš”ì²­ = ~60ì´ˆ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ í¬í•¨)

---

## ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì†ë„ | êµ¬í˜„ ë³µì¡ë„ | ì¶”ì²œ ìš©ë„ |
|------|-----------|------|-----------|---------|
| **ìˆœì°¨ ì²˜ë¦¬** (í˜„ì¬) | âœ… ë‚®ìŒ | ë³´í†µ | âœ… ê°„ë‹¨ | ì†Œê·œëª¨ (< 100ê°œ) |
| **ë³‘ë ¬ ì²˜ë¦¬** | âš ï¸ ë†’ìŒ | â­ ë¹ ë¦„ | ë³µì¡ | ì¤‘ê·œëª¨ (100-1000ê°œ) |
| **API ì„œë²„** | âœ… ë‚®ìŒ | ë³´í†µ | â­ ê°„ë‹¨ | ëŒ€ê·œëª¨, ì§€ì† ì„œë¹„ìŠ¤ |

---

## ìµœì í™” íŒ

### 1ï¸âƒ£ Backend ì„ íƒ
```python
# faster-whisper: ê°€ì¥ ë¹ ë¦„ (GPU ìµœì í™”)
stt.transcribe(audio, backend="faster-whisper")

# transformers: í˜¸í™˜ì„± ìš°ìˆ˜, ì¤‘ê°„ ì†ë„
stt.transcribe(audio, backend="transformers")

# openai-whisper: ëŠë¦¼, ëŒ€ì²´ìš©ë§Œ
stt.transcribe(audio, backend="openai-whisper")
```

**Batchì—ì„œ:**
```python
# ëª¨ë“  íŒŒì¼ì— ê°™ì€ backend ì‚¬ìš© (faster-whisper ê¶Œì¥)
for audio_file in audio_files:
    result = stt.transcribe(audio_file, backend="faster-whisper")
```

### 2ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ìµœì í™”
```python
# Batch ì²˜ë¦¬ ì¤‘ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
import gc
import torch

for audio_file in audio_files:
    result = stt.transcribe(audio_file)
    
    # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)
    if len(results) % 10 == 0:
        gc.collect()
        torch.cuda.empty_cache()
```

### 3ï¸âƒ£ íƒ€ì„ì•„ì›ƒ ì„¤ì • (API ì‚¬ìš© ì‹œ)
```python
# ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
response = requests.post(
    "http://localhost:8003/transcribe",
    files=files,
    timeout=300  # 5ë¶„ (ê¸´ ìŒì„± íŒŒì¼ìš©)
)
```

---

## ì‹¤ì œ ìš´ì˜ ì‚¬ë¡€

### EC2 + Docker (ê¶Œì¥)
```bash
# Step 1: Docker ì‹¤í–‰ (ëª¨ë¸ ë¡œë“œ ì‹œê°„: ~30ì´ˆ)
docker run -d \
  --name stt-engine \
  -p 8003:8003 \
  -e STT_DEVICE=cuda \
  -e STT_COMPUTE_TYPE=int8 \
  -v $(pwd)/models:/app/models \
  stt-engine:cuda129-rhel89-v1.5

# Step 2: ëŒ€ëŸ‰ íŒŒì¼ ì²˜ë¦¬
python batch_transcribe.py audio/samples/ > results.json

# Step 3: ì„œë²„ ì¬ì‚¬ìš© (ë‹¤ìŒ ë°°ì¹˜ ìš”ì²­ ì‹œ)
# ëª¨ë¸ì€ ì—¬ì „íˆ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆìŒ
python batch_transcribe.py audio/samples/2/ > results2.json
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©:**
- ì²˜ìŒ ìš”ì²­: ~2.5GB (ëª¨ë¸ ë¡œë“œ í¬í•¨)
- ì´í›„ ìš”ì²­: 0MB ì¶”ê°€ (ì¬ì‚¬ìš©)
- 100ê°œ íŒŒì¼ ì²˜ë¦¬ í›„: ì—¬ì „íˆ ~2.5GB (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ)

---

## ê²°ë¡ 

**í˜„ì¬ WhisperSTT êµ¬ì¡°ëŠ” Batch ì²˜ë¦¬ì— âœ… ì´ë¯¸ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤:**

1. âœ… ëª¨ë¸ì€ __init__ì—ì„œ 1íšŒë§Œ ë¡œë“œ
2. âœ… transcribe() í˜¸ì¶œë§ˆë‹¤ ë©”ëª¨ë¦¬ ì¦ê°€ ì—†ìŒ
3. âœ… Backend íŒŒë¼ë¯¸í„°ë¡œ ìœ ì—°í•œ ì„ íƒ ê°€ëŠ¥
4. âœ… 100ê°œ ì´ìƒ íŒŒì¼ë„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬

**ê¶Œì¥ Batch ì²˜ë¦¬ ë°©ì‹:**
- ì†Œê·œëª¨ (< 100ê°œ): ìˆœì°¨ ì²˜ë¦¬ (í˜„ì¬ ì½”ë“œ)
- ì¤‘ê·œëª¨ (100-1000ê°œ): API ì„œë²„ + ìˆœì°¨ ìš”ì²­
- ëŒ€ê·œëª¨ (1000+): API ì„œë²„ + ë³‘ë ¬ í´ë¼ì´ì–¸íŠ¸ or ë©”ì‹œì§€ í

ë” í•„ìš”í•œ ìµœì í™”ê°€ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ğŸš€
