#!/usr/bin/env python3
"""
Simpler CTranslate2 ëª¨ë¸ ë³€í™˜ (faster-whisper ë‚´ì¥ ë°©ì‹)
"""

import sys
import shutil
from pathlib import Path

print("=" * 80)
print("ğŸš€ ê°„ë‹¨í•œ ëª¨ë¸ ë³€í™˜ (faster-whisper ìºì‹œ í™œìš©)")
print("=" * 80)
print()

try:
    from faster_whisper import WhisperModel
    import ctranslate2
    
    print(f"âœ… faster-whisper ë²„ì „: {WhisperModel.__module__}")
    print(f"âœ… CTranslate2 ë²„ì „: {ctranslate2.__version__}")
    print()
    
except ImportError as e:
    print(f"âŒ íŒ¨í‚¤ì§€ import ì‹¤íŒ¨: {e}")
    print()
    print("ğŸ“¦ ì„¤ì¹˜ í•„ìš”: pip install faster-whisper ctranslate2 torch")
    sys.exit(1)

model_path = Path(__file__).parent / "models" / "openai_whisper-large-v3-turbo"

print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")
print()

# Step 1: ëª¨ë¸ íŒŒì¼ í™•ì¸
print("1ï¸âƒ£  ëª¨ë¸ íŒŒì¼ í™•ì¸...")
print("-" * 80)

model_safetensors = model_path / "model.safetensors"
if not model_safetensors.exists():
    print(f"âŒ model.safetensorsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_safetensors}")
    sys.exit(1)

size_gb = model_safetensors.stat().st_size / (1024 ** 3)
print(f"âœ… ëª¨ë¸ íŒŒì¼: {model_safetensors.name} ({size_gb:.2f}GB)")
print()

# Step 2: faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ (ìë™ ë³€í™˜)
print("2ï¸âƒ£  faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
print("-" * 80)
print("   (ì´ ê³¼ì •ì€ 2-5ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
print()

try:
    print("   ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = WhisperModel(
        str(model_path),
        device="cpu",
        compute_type="default",
        local_files_only=True,
    )
    print("   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # ëª¨ë¸ì´ ë¡œë“œë˜ë©´ CTranslate2ê°€ ìë™ìœ¼ë¡œ model.binì„ ìƒì„±
    
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print()
    print("   ì›ì¸ ë¶„ì„:")
    print("   - model.safetensors í¬ë§·ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ")
    print("   - faster-whisper ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ")
    sys.exit(1)

print()

# Step 3: model.bin í™•ì¸
print("3ï¸âƒ£  ë³€í™˜ëœ ëª¨ë¸ íŒŒì¼ í™•ì¸...")
print("-" * 80)

model_bin = model_path / "model.bin"
if model_bin.exists():
    size_gb = model_bin.stat().st_size / (1024 ** 3)
    print(f"âœ… model.bin ìƒì„±ë¨: {size_gb:.2f}GB")
else:
    # CTranslate2 ìºì‹œ ìœ„ì¹˜ í™•ì¸
    cache_dir = Path.home() / ".cache" / "ct2models"
    if cache_dir.exists():
        print(f"âš ï¸  ëª¨ë¸ì´ ìºì‹œì— ì €ì¥ë¨: {cache_dir}")
        print()
        print("   ìºì‹œëœ ëª¨ë¸ì„ í”„ë¡œì íŠ¸ë¡œ ë³µì‚¬ ì¤‘...")
        
        # ìºì‹œì—ì„œ ëª¨ë¸ ì°¾ê¸°
        for cached_model in cache_dir.glob("whisper-*"):
            if (cached_model / "model.bin").exists():
                print(f"   ì°¾ì€ ìºì‹œ ëª¨ë¸: {cached_model.name}")
                shutil.copy2(cached_model / "model.bin", model_bin)
                print(f"   âœ… ë³µì‚¬ ì™„ë£Œ: {model_bin}")
                break
    else:
        print(f"âŒ model.binì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)

print()

# Step 4: ìµœì¢… í™•ì¸
print("4ï¸âƒ£  ìµœì¢… íŒŒì¼ ëª©ë¡...")
print("-" * 80)

for f in sorted(model_path.glob("*")):
    if f.is_file():
        size = f.stat().st_size
        if size > 1024 ** 3:
            size_str = f"{size / (1024**3):.2f}GB"
        elif size > 1024 ** 2:
            size_str = f"{size / (1024**2):.1f}MB"
        else:
            size_str = f"{size / 1024:.1f}KB"
        print(f"   - {f.name:40s} {size_str:>10s}")

print()
print("=" * 80)
print("âœ… ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
print("=" * 80)
print()
print("ğŸ“ ë‹¤ìŒ ë‹¨ê³„: Linux ì„œë²„ë¡œ model.bin ì „ì†¡")
print()
