# Whisper STT API - ìŒì„± ì¸ì‹ ì—”ì§„

OpenAIì˜ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” REST API ì„œë²„ì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **Whisper ê¸°ë°˜ STT**: OpenAIì˜ whisper-large-v3-turbo ëª¨ë¸ ì‚¬ìš©
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›
- **GPU ìµœì í™”**: CUDA GPUë¥¼ í™œìš©í•œ ë¹ ë¥¸ ì²˜ë¦¬
- **Docker í™˜ê²½**: ì»¨í…Œì´ë„ˆí™”ëœ ê°„ë‹¨í•œ ë°°í¬
- **FastAPI ì„œë²„**: REST APIë¥¼ í†µí•œ ì‰¬ìš´ ì ‘ê·¼

**ì°¸ê³ **: vLLM í…ìŠ¤íŠ¸ ì²˜ë¦¬ëŠ” ë³„ë„ë¡œ ë°°í¬ëœ ì„œë²„ì—ì„œ ë‹´ë‹¹í•©ë‹ˆë‹¤.

## ğŸ“‹ ì¤€ë¹„ ì‚¬í•­

- Python 3.11+
- CUDA 12.4+ (GPU ì‚¬ìš© ì‹œ, CUDA 11.8 í˜¸í™˜ë„ ê°€ëŠ¥)
- Docker & Docker Compose
- ìµœì†Œ 4GB VRAM (ê¶Œì¥ 8GB ì´ìƒ)

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Docker)

### 1ë‹¨ê³„: ì´ë¯¸ì§€ ë¹Œë“œ
```bash
docker build -f docker/Dockerfile.gpu -t whisper-stt:latest .
```

### 2ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```bash
docker-compose -f docker/docker-compose.yml up -d
```

### 3ë‹¨ê³„: ìƒíƒœ í™•ì¸
```bash
curl http://localhost:8001/health
```

### 4ë‹¨ê³„: ìŒì„± ì¸ì‹
```bash
curl -X POST -F "file=@audio.wav" -F "language=ko" \
  http://localhost:8001/transcribe
```

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8001/health
```

### ìŒì„± ì¸ì‹ (STT)
```bash
curl -X POST \
  -F "file=@audio.wav" \
  -F "language=ko" \
  http://localhost:8001/transcribe
```

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "success": true,
  "text": "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì¸ê³µì§€ëŠ¥ ìŒì„±ì¸ì‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
  "language": "ko"
}
```

## ğŸ”„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (vLLM)

STTë¡œ ë³€í™˜í•œ í…ìŠ¤íŠ¸ë¥¼ ë³„ë„ë¡œ ë°°í¬ëœ vLLM ì„œë²„ë¡œ ë³´ë‚´ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤:

```bash
# vLLM API í˜¸ì¶œ ì˜ˆì‹œ
curl -X POST http://your-vllm-server:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "prompt": "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì¸ê³µì§€ëŠ¥ ìŒì„±ì¸ì‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
    "max_tokens": 100
  }'
```

### ìŒì„± íŒŒì¼ ë³€í™˜ ë° vLLM ì²˜ë¦¬
```bash
curl -X POST -F "file=@audio.wav" \
  -F "instruction=ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:" \
  http://localhost:8001/transcribe-and-process
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
stt_engine/
â”œâ”€â”€ download_model.py          # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ stt_engine.py             # STT í•µì‹¬ ëª¨ë“ˆ
â”œâ”€â”€ vllm_client.py            # vLLM í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ api_server.py             # FastAPI ì„œë²„
â”œâ”€â”€ Dockerfile                # Docker ì´ë¯¸ì§€ ì •ì˜
â”œâ”€â”€ docker-compose.yml        # Docker Compose ì„¤ì •
â”œâ”€â”€ requirements.txt          # Python ì˜ì¡´ì„±
â”œâ”€â”€ .env.example              # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ
â”œâ”€â”€ README.md                 # ì´ íŒŒì¼
â”œâ”€â”€ models/                   # Whisper ëª¨ë¸ ì €ì¥ ìœ„ì¹˜
â”œâ”€â”€ audio/                    # í…ŒìŠ¤íŠ¸ ìŒì„± íŒŒì¼ ìœ„ì¹˜
â””â”€â”€ logs/                     # ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### GPU ì‚¬ìš© ì„¤ì •

#### docker-compose.ymlì—ì„œ GPU í™œì„±í™”:
```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

#### í™˜ê²½ ë³€ìˆ˜ì—ì„œ GPU ì„¤ì •:
```bash
WHISPER_DEVICE=cuda
```

### vLLM ëª¨ë¸ ë³€ê²½

docker-compose.ymlì—ì„œ ëª¨ë¸ ì´ë¦„ ë³€ê²½:
```yaml
environment:
  - MODEL_NAME=meta-llama/Llama-2-13b-hf  # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ë¡œì»¬ í…ŒìŠ¤íŠ¸
```bash
# STT ì—”ì§„ í…ŒìŠ¤íŠ¸
python stt_engine.py

# vLLM ì—°ê²° í…ŒìŠ¤íŠ¸
python vllm_client.py
```

### Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker-compose exec stt-engine python stt_engine.py
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸
```bash
# ìµœê·¼ 100ì¤„ ë¡œê·¸
tail -100f logs/*.log

# íŠ¹ì • ë¡œê·¸ ë³´ê¸°
docker-compose logs -f stt-engine
```

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
docker stats stt-engine vllm-server
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**: ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ì´ ìƒë‹¹íˆ í¼ (ìˆ˜ GB)ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **GPU ë©”ëª¨ë¦¬**: GPUë¥¼ ì‚¬ìš©í•  ê²½ìš° ì¶©ë¶„í•œ VRAMì´ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 8GB ê¶Œì¥).
3. **vLLM ì„œë²„**: STTì™€ vLLMì„ í•¨ê»˜ ì‚¬ìš©í•˜ë ¤ë©´ vLLM ì„œë²„ê°€ ë°˜ë“œì‹œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# Hugging Face í† í° ì„¤ì •
export HUGGINGFACE_HUB_TOKEN=your_token_here
python download_model.py
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# CPU ëª¨ë“œë¡œ ì‹¤í–‰
export WHISPER_DEVICE=cpu
python stt_engine.py
```

### vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```bash
# vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
curl http://localhost:8000/health

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart vllm-server
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ğŸ‘¥ ê¸°ì—¬

ì´ìŠˆ ë° í’€ ë¦¬í€˜ìŠ¤íŠ¸ëŠ” ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!
