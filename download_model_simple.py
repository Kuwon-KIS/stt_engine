#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ë¡œì»¬ ë° ì˜¤í”„ë¼ì¸ ë°°í¬ìš©)

ëª©ì : 
  - Hugging Faceì—ì„œ openai/whisper-large-v3-turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  - ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë¡œì»¬ ì €ì¥
  - ì™¸ë¶€ ì¸í„°ë„·ì´ ì—†ëŠ” Linux ì„œë²„ë¡œ ì´ë™ ê°€ëŠ¥í•˜ê²Œ ì¤€ë¹„

ì‚¬ìš©:
  python download_model_simple.py
"""

import os
import sys
import ssl
from pathlib import Path

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°ìš©)
ssl._create_default_https_context = ssl._create_unverified_context

print("ğŸ”„ Whisper Large-V3-Turbo ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
print("=" * 60)

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"
models_dir.mkdir(parents=True, exist_ok=True)

# HuggingFace í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œì»¬ ìºì‹œ ì‚¬ìš©)
os.environ["HF_HOME"] = str(models_dir / ".cache")
os.environ["HF_HUB_CACHE"] = str(models_dir / ".cache" / "hub")
os.environ["TRANSFORMERS_CACHE"] = str(models_dir / ".cache")

print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {models_dir}")
print(f"ğŸ—‚ï¸  ìºì‹œ ê²½ë¡œ: {models_dir / '.cache'}")
print()

try:
    from faster_whisper import WhisperModel
    
    print(f"ï¿½ ëª¨ë¸ëª…: openai/whisper-large-v3-turbo")
    print(f"â³ faster_whisperë¡œ ë‹¤ìš´ë¡œë“œ ì¤‘ (ì•½ 1.3GB)...")
    print()
    
    # faster_whisperì—ì„œ Hugging Face repo ID ì§ì ‘ ì‚¬ìš©
    # download_rootë¥¼ ì§€ì •í•˜ë©´ models/ í´ë” ì§ì ‘ ì‚¬ìš©
    model = WhisperModel(
        "openai/whisper-large-v3-turbo",
        device="cpu",
        compute_type="int8",
        download_root=str(models_dir),
        local_files_only=False  # ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ
    )
    
    print()
    print("=" * 60)
    print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì™„ë£Œ!")
    print("=" * 60)
    # íŒŒì¼ ê²€ì¦ ë° í†µê³„
    import subprocess
    
    result = subprocess.run(f"find {models_dir} -type f ! -path '*/.*' ! -name '.DS_Store'", 
                          shell=True, capture_output=True, text=True)
    files = [f for f in result.stdout.strip().split('\n') if f and not f.startswith('.')]
    
    print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ìˆ˜: {len(files)}")
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    REQUIRED_FILES = [
        "config.json",
        "model.safetensors",
        "generation_config.json",
        "preprocessor_config.json",
        "tokenizer.json",
    ]
    
    print("\nâœ“ í•„ìˆ˜ íŒŒì¼ í™•ì¸:")
    all_found = True
    for req_file in REQUIRED_FILES:
        if (models_dir / req_file).exists():
            size = (models_dir / req_file).stat().st_size / (1024**3)
            print(f"  âœ“ {req_file} ({size:.2f}GB)")
        else:
            print(f"  âœ— {req_file} (MISSING)")
            all_found = False
    
    if not all_found:
        print("\nâš ï¸  ì¼ë¶€ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤!")
        sys.exit(1)
    
    result = subprocess.run(f"du -sh {models_dir}", 
                          shell=True, capture_output=True, text=True)
    print(f"\nğŸ“ ì´ í¬ê¸°: {result.stdout.strip()}")
    
    print("\nğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
    result = subprocess.run(f"ls -lh {models_dir}/", 
                          shell=True, capture_output=True, text=True)
    print(result.stdout)
    
    print("\nâœ… faster_whisper ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ëª¨ë¸ ê²€ì¦: python validate_model.py")
    print("  2. ëª¨ë¸ ì••ì¶•: bash scripts/compress-model.sh")
    print("  3. ì„œë²„ ì „ì†¡: scp whisper-large-v3-turbo-models.tar.gz user@server:/path/")
    print("  4. ì„œë²„ì—ì„œ ì••ì¶• í’€ê¸°: tar -xzf whisper-large-v3-turbo-models.tar.gz")
    print("\nğŸ’¡ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ëª¨ë¸ ì‚¬ìš©:")
    print(f"  - ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©: local_files_only=True")
    print(f"  - í™˜ê²½ë³€ìˆ˜: HF_HOME={models_dir / '.cache'}")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", file=sys.stderr)
    import traceback
    traceback.print_exc()
    sys.exit(1)
