# vLLM ì—°ë™ ì™„ë²½ ê°€ì´ë“œ - ìµœì¢… ìš”ì•½

## ğŸ¯ ë‹¹ì‹ ì˜ ìƒí™©

> "vllmì„ dockerë¡œ ë„ì› ëŠ”ë°, ê·¸ëŸ¬ë©´ ì—¬ê¸°ì˜ endpointëŠ” ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼í•˜ëŠ”ê±°ì•¼? ìŒì„± íŒŒì¼ì„ ì €ê¸°ë¡œ ë³´ë‚´ì„œ ì²˜ë¦¬í•˜ë ¤ë©´ ì–´ë–¤ ì‹ìœ¼ë¡œ í•´ì•¼í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜"

## âœ… ë‹µë³€

### 1ï¸âƒ£ Endpoint ì„¤ì • ë°©ë²•

#### A. ë¡œì»¬ í…ŒìŠ¤íŠ¸ (ê°€ì¥ ê°„ë‹¨)
```bash
# í„°ë¯¸ë„ 1: vLLM ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# í„°ë¯¸ë„ 2: STT Engine ì„¤ì •
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# ê²°ê³¼: ëª¨ë“  í†µì‹ ì´ http://localhost í†µí•´ ìë™ ì—°ê²°
```

**ì½”ë“œ**: `.env` íŒŒì¼ì´ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ìë™ ì„¤ì •
```env
VLLM_API_URL="http://localhost:8000"
```

---

#### B. Docker Compose (ê¶Œì¥)
```bash
# í•œ ëª…ë ¹ìœ¼ë¡œ ëª¨ë‘ ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d

# íŠ¹ì§•:
# - STT + vLLMì´ ìë™ìœ¼ë¡œ ì—°ê²°
# - ë‚´ë¶€ Docker ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© (http://vllm:8000)
# - GPU ìë™ í• ë‹¹
```

**ì½”ë“œ**: ìë™ìœ¼ë¡œ `http://vllm:8000`ìœ¼ë¡œ ì„¤ì •ë¨

---

#### C. ì›ê²© GPU ì„œë²„
```bash
# GPU ì„œë²„ì—ì„œ
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# ë¡œì»¬ì—ì„œ
export VLLM_API_URL="http://192.168.1.100:8000"
python api_server.py
```

**ì½”ë“œ**: `.env` íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜
```env
VLLM_API_URL="http://192.168.1.100:8000"
```

---

### 2ï¸âƒ£ ìŒì„± íŒŒì¼ ì²˜ë¦¬ ë°©ì‹

#### ë°©ë²• A: API ì§ì ‘ í˜¸ì¶œ (ê°€ì¥ ê°„ë‹¨)

**ìŒì„± íŒŒì¼ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ**:
```bash
curl -X POST "http://localhost:8001/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=ko"
```

**ì‘ë‹µ**:
```json
{
  "success": true,
  "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤."
}
```

---

**ìŒì„± íŒŒì¼ â†’ í…ìŠ¤íŠ¸ â†’ vLLM ì²˜ë¦¬ (ì™„ì „ ìë™)**:
```bash
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio.mp3" \
  -F "language=ko" \
  -F "instruction=ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:"
```

**ì‘ë‹µ**:
```json
{
  "success": true,
  "stt_result": {
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤."
  },
  "vllm_result": {
    "summary": "ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ì¸ì‚¬ë§"
  }
}
```

---

#### ë°©ë²• B: Python í´ë¼ì´ì–¸íŠ¸

```python
import requests

# ìŒì„± íŒŒì¼ ì¤€ë¹„
with open("audio.mp3", "rb") as f:
    files = {"file": f}
    data = {
        "language": "ko",
        "instruction": "ì´ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:"
    }
    
    # STT + vLLM í•œ ë²ˆì— ì²˜ë¦¬
    response = requests.post(
        "http://localhost:8001/transcribe-and-process",
        files=files,
        data=data
    )
    
    result = response.json()
    print(f"ìŒì„± ì¸ì‹: {result['stt_result']['text']}")
    print(f"vLLM ë¶„ì„: {result['vllm_result']}")
```

---

### 3ï¸âƒ£ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ìŒì„± íŒŒì¼    â”‚ (audio.mp3)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (HTTP POST)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STT Engine         â”‚ (Port 8001)
â”‚ â€¢ Whisper ëª¨ë¸       â”‚
â”‚ â€¢ ìŒì„± â†’ í…ìŠ¤íŠ¸      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (ì¸ì‹ëœ í…ìŠ¤íŠ¸)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   vLLM Server        â”‚ (Port 8000)
â”‚ â€¢ LLM ëª¨ë¸           â”‚
â”‚ â€¢ í…ìŠ¤íŠ¸ ì²˜ë¦¬/ìš”ì•½   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (ì²˜ë¦¬ ê²°ê³¼)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ìµœì¢… ê²°ê³¼                    â”‚
â”‚ {                           â”‚
â”‚   "text": "...",            â”‚
â”‚   "summary": "...",         â”‚
â”‚   "emotion": "...",         â”‚
â”‚   ...                       â”‚
â”‚ }                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ 3ê°€ì§€ í™˜ê²½ë³„ ì„¤ì • ê°€ì´ë“œ

### í™˜ê²½ 1: ë¡œì»¬ (macOS)
```bash
# 1. vLLM ì‹¤í–‰
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# 2. STT Engine ì‹¤í–‰
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# 3. í…ŒìŠ¤íŠ¸
python test_vllm_integration.py --test-vllm audio.mp3
```

**Endpoint**: `http://localhost:8000`

---

### í™˜ê²½ 2: Docker Compose (ë¡œì»¬)
```bash
# 1. ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d

# 2. í…ŒìŠ¤íŠ¸
curl http://localhost:8001/health
curl http://localhost:8000/health

# 3. ìŒì„± íŒŒì¼ ì²˜ë¦¬
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio.mp3" \
  -F "language=ko"
```

**Endpoint**: `http://vllm:8000` (ìë™ ì„¤ì •)

---

### í™˜ê²½ 3: ì›ê²© GPU ì„œë²„
```bash
# ì„œë²„ì—ì„œ vLLM ì‹¤í–‰
ssh gpu-server
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# ë¡œì»¬ì—ì„œ STT Engine ì„¤ì •
export VLLM_API_URL="http://192.168.1.100:8000"
python api_server.py

# í…ŒìŠ¤íŠ¸
curl "http://localhost:8001/health"
```

**Endpoint**: `http://192.168.1.100:8000`

---

## ğŸ“‹ ë¹ ë¥¸ ì‹œì‘ (ì„ íƒí•˜ì„¸ìš”)

### Option 1ï¸âƒ£: ê°€ì¥ ë¹ ë¥¸ ë°©ë²• (Docker Compose)
```bash
cd /Users/a113211/workspace/stt_engine

# 1ë‹¨ê³„: ì‹œì‘
docker-compose -f docker-compose.vllm.yml up -d

# 2ë‹¨ê³„: í™•ì¸ (30ì´ˆ ëŒ€ê¸°)
sleep 30
curl http://localhost:8001/health

# 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8001/transcribe-and-process" \
  -F "file=@audio_samples/test.mp3" \
  -F "language=ko"
```

---

### Option 2ï¸âƒ£: ìˆ˜ë™ ì„¤ì • (ë” ìì„¸í•œ ì œì–´)
```bash
# í„°ë¯¸ë„ 1: vLLM
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf

# í„°ë¯¸ë„ 2: STT Engine
cd /Users/a113211/workspace/stt_engine
export VLLM_API_URL="http://localhost:8000"
python api_server.py

# í„°ë¯¸ë„ 3: í…ŒìŠ¤íŠ¸
python test_vllm_integration.py --test-vllm audio_samples/test.mp3 \
  --instruction "ì´ ìŒì„±ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:"
```

---

## ğŸ”§ í•µì‹¬ ì„¤ì • íŒŒì¼ 3ê°œ

### 1. `.env` - í™˜ê²½ ë³€ìˆ˜
```env
# vLLM ì—°ê²° ì„¤ì •
VLLM_API_URL="http://localhost:8000"    # â† ì—¬ê¸°ë§Œ ë³€ê²½!
VLLM_MODEL_NAME="meta-llama/Llama-2-7b-hf"
VLLM_TIMEOUT=60
```

### 2. `docker-compose.vllm.yml` - ìë™ ì„¤ì •
```yaml
services:
  vllm:
    ports: ["8000:8000"]
  
  whisper-api:
    environment:
      - VLLM_API_URL=http://vllm:8000  # â† ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •
```

### 3. `test_vllm_integration.py` - í†µí•© í…ŒìŠ¤íŠ¸
```bash
python test_vllm_integration.py --check-health
python test_vllm_integration.py --test-vllm audio.mp3
```

---

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

- **[VLLM_QUICKSTART.md](VLLM_QUICKSTART.md)** - 5ë¶„ ì‹œì‘ ê°€ì´ë“œ â­
- **[VLLM_SETUP.md](VLLM_SETUP.md)** - ì™„ë²½í•œ ì„¤ì • ë§¤ë‰´ì–¼
- **[docker-compose.vllm.yml](docker-compose.vllm.yml)** - Docker Compose ì„¤ì •
- **[test_vllm_integration.py](test_vllm_integration.py)** - í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

---

## â“ FAQ

### Q1: vLLM endpointë¥¼ ë°”ê¾¸ë ¤ë©´?
```bash
export VLLM_API_URL="http://192.168.1.100:8000"
python api_server.py
```

### Q2: Docker Composeì—ì„œ ìë™ìœ¼ë¡œ ì—°ê²°ë˜ë‚˜ìš”?
ë„¤, `VLLM_API_URL=http://vllm:8000`ìœ¼ë¡œ ìë™ ì„¤ì •ë©ë‹ˆë‹¤.

### Q3: ì›ê²© GPU ì„œë²„ì—ì„œëŠ”?
```bash
export VLLM_API_URL="http://gpu-server-ip:8000"
```

### Q4: ëª¨ë¸ ë³€ê²½í•˜ë ¤ë©´?
```bash
# .env ë˜ëŠ” docker-compose.vllm.yml ìˆ˜ì •
VLLM_MODEL_NAME="mistralai/Mistral-7B-v0.1"

# ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker-compose -f docker-compose.vllm.yml restart vllm
```

### Q5: ë°°ì¹˜ ì²˜ë¦¬ëŠ”?
```bash
python test_vllm_integration.py --batch audio_samples/
```

---

## ğŸ“ ì‘ë™ ì›ë¦¬

```python
# 1. ìŒì„± íŒŒì¼ì„ ë°›ìŒ
# api_server.pyì˜ /transcribe-and-process ì—”ë“œí¬ì¸íŠ¸

# 2. Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜
stt_result = stt.transcribe("audio.mp3", language="ko")
# "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤."

# 3. vLLMìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
# vllm_client.pyì˜ generate() ë©”ì„œë“œ
vllm_result = vllm_client.generate(
    prompt="ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”: ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ìŒì„±ì…ë‹ˆë‹¤."
)
# "ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ì¸ì‚¬ë§"

# 4. ìµœì¢… ê²°ê³¼ ë°˜í™˜
return {
    "stt_result": stt_result,
    "vllm_result": vllm_result
}
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] vLLM Docker ì´ë¯¸ì§€ í™•ì¸
- [ ] VLLM_API_URL ì„¤ì •
- [ ] docker-compose.vllm.yml ì¤€ë¹„
- [ ] STT Engine í—¬ìŠ¤ ì²´í¬
- [ ] vLLM í—¬ìŠ¤ ì²´í¬
- [ ] ìŒì„± íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
- [ ] ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ** â†’ VLLM_QUICKSTART.md ë”°ë¼í•˜ê¸°
2. **ì›ê²© ì„œë²„ ë°°í¬** â†’ ëª¨ë¸ì„ GPU ì„œë²„ë¡œ ì´ì „
3. **í”„ë¡œë•ì…˜ ì„¤ì •** â†’ vLLM ì„±ëŠ¥ ìµœì í™”
4. **ëª¨ë‹ˆí„°ë§** â†’ ë¡œê·¸ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

---

**ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”?** [VLLM_SETUP.md](VLLM_SETUP.md)ì˜ "ë¬¸ì œ í•´ê²°" ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”!
