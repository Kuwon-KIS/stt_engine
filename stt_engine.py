#!/usr/bin/env python3
"""
STT ëª¨ë“ˆ - 3ê°€ì§€ ë°±ì—”ë“œ ìë™ ì„ íƒ

ìš°ì„ ìˆœìœ„:
1. faster-whisper + CTranslate2 (ê°€ì¥ ë¹ ë¦„, ê¶Œì¥)
2. transformers WhisperForConditionalGeneration (HF ëª¨ë¸ ì§ì ‘ ì§€ì›)
3. OpenAI Whisper (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›, ëŒ€ì²´ìš©)

ì§€ì› ëª¨ë¸ í˜•ì‹:
- CTranslate2: .tar.gz (ë³€í™˜ë¨)
- transformers: Hugging Face í˜•ì‹ (PyTorch/SafeTensors)
- OpenAI Whisper: ê³µì‹ ëª¨ë¸ëª…ë§Œ (tiny, base, small, medium, large)
"""

import os
from pathlib import Path
from typing import Optional, Dict
import tarfile

# ì„¸ ê°€ì§€ ë°±ì—”ë“œ ì‹œë„
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

try:
    from transformers import WhisperProcessor, WhisperForConditionalGeneration
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import openai_whisper as whisper
    WHISPER_AVAILABLE = True
except ImportError:
    try:
        import whisper
        WHISPER_AVAILABLE = True
    except ImportError:
        WHISPER_AVAILABLE = False

if not (FASTER_WHISPER_AVAILABLE or TRANSFORMERS_AVAILABLE or WHISPER_AVAILABLE):
    raise ImportError(
        "ë‹¤ìŒ ì¤‘ ìµœì†Œ í•˜ë‚˜ì˜ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n"
        "  1. faster-whisper + ctranslate2 (ê¶Œì¥)\n"
        "  2. transformers (HF ëª¨ë¸ ì§€ì›)\n"
        "  3. openai-whisper / whisper (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›)"
    )


def diagnose_faster_whisper_model(model_path: str) -> dict:
    """
    faster-whisper ëª¨ë¸ ìƒì„¸ ì§„ë‹¨ (ë””ë²„ê¹…ìš©)
    
    CTranslate2 ëª¨ë¸ì€ ë‹¤ìŒ íŒŒì¼ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
    - model.bin (CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬)
    - config.json (ëª¨ë¸ ì„¤ì •)
    - vocabulary.json (ë˜ëŠ” tokens.json) - í† í¬ë‚˜ì´ì € ì •ë³´
    - shared_vocabulary.json (ì„ íƒì‚¬í•­)
    
    Returns:
        {
            'valid': bool,
            'errors': [list of errors],
            'warnings': [list of warnings],
            'files': {detailed file structure},
            'model_bin_size_mb': float
        }
    """
    model_dir = Path(model_path)
    ct_model_dir = model_dir / "ctranslate2_model"
    
    diagnosis = {
        'valid': True,
        'errors': [],
        'warnings': [],
        'files': {},
        'model_bin_size_mb': None
    }
    
    # 1. ctranslate2_model í´ë” ì¡´ì¬ í™•ì¸
    if not ct_model_dir.exists():
        diagnosis['errors'].append(f"ctranslate2_model í´ë” ì—†ìŒ: {ct_model_dir}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 2. ctranslate2_model ë‚´ ëª¨ë“  íŒŒì¼ ë‚˜ì—´
    try:
        ct_files = list(ct_model_dir.rglob("*"))
        diagnosis['files']['total_count'] = len(ct_files)
        diagnosis['files']['list'] = []
        
        for file_path in sorted(ct_files)[:30]:  # ì²˜ìŒ 30ê°œ
            if file_path.is_file():
                size_kb = file_path.stat().st_size / 1024
                diagnosis['files']['list'].append({
                    'name': file_path.name,
                    'relative_path': str(file_path.relative_to(ct_model_dir)),
                    'size_kb': size_kb
                })
    except Exception as e:
        diagnosis['errors'].append(f"íŒŒì¼ ë‚˜ì—´ ì‹¤íŒ¨: {e}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 3. í•„ìˆ˜ íŒŒì¼ í™•ì¸ (CTranslate2 í¬ë§·)
    critical_files = {
        'model.bin': 'CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬',
        'config.json': 'Whisper ëª¨ë¸ ì„¤ì •'
    }
    
    for file_name, description in critical_files.items():
        file_path = ct_model_dir / file_name
        if not file_path.exists():
            diagnosis['errors'].append(f"ëˆ„ë½: {file_name} ({description})")
            diagnosis['valid'] = False
        else:
            size_kb = file_path.stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{file_name}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB (ì†ìƒ ê°€ëŠ¥ì„±)")
    
    # 4. í† í¬ë‚˜ì´ì € íŒŒì¼ í™•ì¸ (vocabulary.json ë˜ëŠ” tokens.json)
    # CTranslate2ëŠ” OpenAI Whisperì˜ tokenizer.jsonì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    vocab_files = ['vocabulary.json', 'tokens.json', 'tokenizer.json']
    has_vocab = False
    for vocab_file in vocab_files:
        if (ct_model_dir / vocab_file).exists():
            has_vocab = True
            size_kb = (ct_model_dir / vocab_file).stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{vocab_file}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB")
            break
    
    if not has_vocab:
        diagnosis['warnings'].append(f"í† í¬ë‚˜ì´ì € íŒŒì¼ ì—†ìŒ (vocabulary.json, tokens.json, tokenizer.json ì¤‘ í•˜ë‚˜ í•„ìš”)")
    
    # 5. model.bin ìƒì„¸ ê²€ì‚¬
    model_bin = ct_model_dir / "model.bin"
    if model_bin.exists():
        size_bytes = model_bin.stat().st_size
        size_mb = size_bytes / (1024 * 1024)
        diagnosis['model_bin_size_mb'] = size_mb
        
        if size_mb < 100:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° ì‘ìŒ: {size_mb:.1f}MB (ì†ìƒ ë˜ëŠ” ë³€í™˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
            diagnosis['valid'] = False
        
        if size_mb > 5000:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° í¼: {size_mb:.1f}MB (ì–‘ìí™” í™•ì¸ í•„ìš”)")
    
    return diagnosis


def validate_faster_whisper_model(model_path: str) -> bool:
    """
    faster-whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦
    diagnose_faster_whisper_modelì˜ ê°„ë‹¨í•œ ë˜í¼
    """
    diagnosis = diagnose_faster_whisper_model(model_path)
    
    print(f"   ğŸ“‚ faster-whisper ëª¨ë¸ ê²€ì¦: {model_path}")
    
    if diagnosis['files']['total_count'] > 0:
        print(f"   âœ“ ctranslate2_model í´ë” ìˆìŒ ({diagnosis['files']['total_count']}ê°œ íŒŒì¼)")
    
    if diagnosis['model_bin_size_mb']:
        print(f"   âœ“ model.bin: {diagnosis['model_bin_size_mb']:.1f}MB")
    
    for warning in diagnosis['warnings']:
        print(f"   âš ï¸  {warning}")
    
    for error in diagnosis['errors']:
        print(f"   âŒ {error}")
    
    return diagnosis['valid']


def validate_whisper_model(model_path: str) -> bool:
    """
    OpenAI Whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦ (PyTorch ëª¨ë¸ í˜•ì‹)
    
    ì£¼ì˜: OpenAI WhisperëŠ” ê³µì‹ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:
    - tiny, base, small, medium, large
    
    "large-v3", "large-v3-turbo" ê°™ì€ ë³€í˜•ì€ huggingfaceì—ì„œë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ
    ìš´ì˜ì„œë²„ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.
    
    Args:
        model_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ì°¸ê³ ìš©)
    
    Returns:
        True if ìœ íš¨, False otherwise
    """
    model_dir = Path(model_path)
    
    if not model_dir.exists():
        print(f"   âš ï¸  ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        return False
    
    # pytorch_model.bin ë˜ëŠ” model.safetensors ì¤‘ í•˜ë‚˜ í•„ìš”
    has_pytorch = (model_dir / "pytorch_model.bin").exists()
    has_safetensors = (model_dir / "model.safetensors").exists()
    
    if not (has_pytorch or has_safetensors):
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”")
        return False
    
    # config.json, tokens.json í•„ìˆ˜
    required_files = ["config.json", "tokenizer.json"]
    missing_files = []
    
    for file in required_files:
        file_path = model_dir / file
        if not file_path.exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {', '.join(missing_files)}")
        return False
    
    print(f"   âœ“ Whisper ëª¨ë¸ êµ¬ì¡° ìœ íš¨")
    return True


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì˜ˆ: "models")
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ (models/openai_whisper-large-v3-turbo)
    
    Raises:
        RuntimeError: ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨
        FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                # ì•ˆì „ì„± ê²€ì‚¬: tar ë©¤ë²„ ê²€ì¦
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        raise RuntimeError(f"ë³´ì•ˆ ìœ„í—˜: ì˜ëª»ëœ ê²½ë¡œ {member.name}")
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except tarfile.TarError as e:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ tar íŒŒì¼: {e}")
            raise RuntimeError(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}") from e
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """faster-whisper / OpenAI Whisper ìë™ ì„ íƒ STT í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device: str = "cpu", compute_type: str = "float16"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: "models")
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', ë˜ëŠ” 'auto')
            compute_type: ê³„ì‚° íƒ€ì… ('float32', 'float16', 'int8')
        
        Raises:
            FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
            RuntimeError: ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨
        
        ì§€ì› ëª¨ë¸ í˜•ì‹:
        - CTranslate2: ctranslate2_model/ í´ë” (model.bin)
        - transformers: PyTorch/SafeTensors (pytorch_model.bin ë˜ëŠ” model.safetensors)
        - OpenAI Whisper: ê³µì‹ ëª¨ë¸ëª…ë§Œ (tiny, base, small, medium, large)
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        models_dir = str(Path(model_path).parent)
        self.model_path = str(auto_extract_model_if_needed(models_dir))
        
        # ëª¨ë¸ ê²½ë¡œ ìœ íš¨ì„± ìµœì¢… í™•ì¸
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                                  f"ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
                                  f"1. ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ê°€? (download_model_hf.py ì‹¤í–‰)\n"
                                  f"2. ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ê°€? (ê¸°ë³¸ê°’: models/openai_whisper-large-v3-turbo)\n"
                                  f"3. ìš´ì˜ì„œë²„ì¸ê°€? (ì˜¤í”„ë¼ì¸ ë°°í¬ì¸ ê²½ìš° ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ì‹œì¼œì•¼ í•¨)")
        
        self.device = device if device != "auto" else ("cuda" if self._is_cuda_available() else "cpu")
        self.compute_type = compute_type
        self.backend = None
        
        print(f"\nğŸ“Š STT ëª¨ë¸ ë¡œë“œ ì‹œì‘")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ:")
        print(f"     - faster-whisper: {FASTER_WHISPER_AVAILABLE}")
        print(f"     - transformers: {TRANSFORMERS_AVAILABLE}")
        print(f"     - openai-whisper: {WHISPER_AVAILABLE}\n")
        
        # 1ï¸âƒ£ faster-whisper ì‹œë„ (CTranslate2 ëª¨ë¸, ê°€ì¥ ë¹ ë¦„)
        if FASTER_WHISPER_AVAILABLE:
            self._try_faster_whisper()
        
        # 2ï¸âƒ£ transformers ì‹œë„ (PyTorch/HF ëª¨ë¸)
        if self.backend is None and TRANSFORMERS_AVAILABLE:
            self._try_transformers()
        
        # 3ï¸âƒ£ OpenAI Whisper ì‹œë„ (ê³µì‹ ëª¨ë¸ë§Œ)
        if self.backend is None and WHISPER_AVAILABLE:
            self._try_whisper()
        
        # ëª¨ë‘ ì‹¤íŒ¨
        if self.backend is None:
            available = []
            if FASTER_WHISPER_AVAILABLE:
                available.append("faster-whisper")
            if TRANSFORMERS_AVAILABLE:
                available.append("transformers")
            if WHISPER_AVAILABLE:
                available.append("openai-whisper")
            
            raise RuntimeError(
                f"âŒ ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨ (ì‚¬ìš© ê°€ëŠ¥: {', '.join(available)})\n\n"
                "ğŸ”§ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"1. ëª¨ë¸ ê²½ë¡œ: {self.model_path}\n"
                f"2. í•„ìš”í•œ íŒŒì¼:\n"
                f"   - CTranslate2: {self.model_path}/ctranslate2_model/model.bin\n"
                f"   - PyTorch: {self.model_path}/pytorch_model.bin ë˜ëŠ” model.safetensors\n"
                f"3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: python download_model_hf.py\n"
                f"4. ë¡œì»¬ ìºì‹œì—ì„œ ë³µì‚¬: cp -r ~/.cache/huggingface/hub/models--openai--whisper-large-v3-turbo/snapshots/*/  {self.model_path}"
            )
        

        # faster-whisper ë¡œë“œ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬
        if self.backend is None:
            raise RuntimeError(
                "âŒ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\n\n"
                "ğŸ”§ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"1. ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸:\n"
                f"   ê²½ë¡œ: {self.model_path}\n"
                f"   í•„ìˆ˜: {self.model_path}/ctranslate2_model/model.bin (1.5GB+)\n\n"
                f"2. CTranslate2 ë³€í™˜ ì™„ë£Œ í™•ì¸:\n"
                f"   ls -lh {self.model_path}/ctranslate2_model/\n"
                f"   model.bin (1.5GB), config.json (2.2KB), vocabulary.json\n\n"
                "3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë³€í™˜:\n"
                "   python download_model_hf.py  # ~30-45ë¶„\n\n"
                "4. Docker ë§ˆìš´íŠ¸ í™•ì¸ (ìš´ì˜ì„œë²„):\n"
                "   docker exec stt-engine ls -lh /app/models/ctranslate2_model/"
            )
    
    def _try_faster_whisper(self):
        """faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©, ìƒì„¸ ì§„ë‹¨ í¬í•¨)"""
        try:
            print(f"ğŸ”„ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device}, compute: {self.compute_type})")
            
            # ëª¨ë¸ êµ¬ì¡° ìƒì„¸ ì§„ë‹¨
            diagnosis = diagnose_faster_whisper_model(self.model_path)
            
            if not diagnosis['valid']:
                print(f"\n   âŒ ëª¨ë¸ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨:")
                for error in diagnosis['errors']:
                    print(f"      - {error}")
                
                # CTranslate2 ë³€í™˜ ê°€ì´ë“œ
                if "tokenizer.json" in str(diagnosis['errors']):
                    print(f"\n   ğŸ’¡ CTranslate2 ë³€í™˜ ì •ë³´:")
                    print(f"      OpenAI Whisperì˜ tokenizer.jsonì€ CTranslate2ë¡œ ë³€í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print(f"      ëŒ€ì‹  ë‹¤ìŒ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
                    print(f"      - vocabulary.json")
                    print(f"      - tokens.json")
                    print(f"      - tokenizer.json (ì›ë³¸ ë³´ì¡´ëœ ê²½ìš°)")
                
                return
            
            # ê²½ê³  í™•ì¸
            if diagnosis['warnings']:
                print(f"\n   âš ï¸  ì£¼ì˜ì‚¬í•­:")
                for warning in diagnosis['warnings']:
                    print(f"      - {warning}")
            
            # íŒŒì¼ ëª©ë¡ ì¶œë ¥
            if diagnosis['files']['list']:
                print(f"\n   ğŸ“‚ CTranslate2 ëª¨ë¸ íŒŒì¼ ({diagnosis['files']['total_count']}ê°œ):")
                for file_info in diagnosis['files']['list'][:10]:
                    print(f"      âœ“ {file_info['name']} ({file_info['size_kb']:.1f}KB)")
                if len(diagnosis['files']['list']) > 10:
                    print(f"      ... ì™¸ {len(diagnosis['files']['list']) - 10}ê°œ")
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„ - CTranslate2 ëª¨ë¸ ì„œë¸Œë””ë ‰í† ë¦¬ ì‚¬ìš©
            # faster-whisperëŠ” ëª¨ë¸.binê³¼ tokenizer íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•¨
            ct2_model_dir = Path(self.model_path) / "ctranslate2_model"
            
            print(f"\n   ğŸ“¦ faster-whisper WhisperModel ë¡œë“œ ì¤‘...")
            print(f"   ğŸ“ ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
            
            self.model = WhisperModel(
                str(ct2_model_dir),
                device=self.device,
                compute_type=self.compute_type,
                num_workers=4,
                cpu_threads=4,
                download_root=None,
                local_files_only=True
            )
            
            self.backend = self.model  # ì‹¤ì œ ëª¨ë¸ ê°ì²´ë¥¼ backendì— ì €ì¥
            print(f"âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
        except FileNotFoundError as e:
            print(f"\n   âŒ faster-whisper: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"      ì—ëŸ¬: {e}")
            print(f"      ê²½ë¡œ: {self.model_path}")
            print(f"\n   ğŸ’¡ í•´ê²° ë°©ë²•:")
            print(f"      1. download_model_hf.py ì‹¤í–‰ ìƒíƒœ í™•ì¸")
            print(f"      2. CTranslate2 ë³€í™˜ ì™„ë£Œ ì—¬ë¶€ í™•ì¸")
            print(f"      3. {self.model_path}/ctranslate2_model/model.bin íŒŒì¼ í¬ê¸° í™•ì¸ (100MB ì´ìƒ)")
        except Exception as e:
            error_str = str(e)
            print(f"\n   âŒ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
            print(f"      ë©”ì‹œì§€: {error_str[:200]}")
            
            # ì•Œë ¤ì§„ ì—ëŸ¬ ì§„ë‹¨
            if "vocabulary" in error_str.lower() or "token" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: í† í¬ë‚˜ì´ì €/ì–´íœ˜ ì˜¤ë¥˜")
                print(f"      - CTranslate2 ë³€í™˜ì´ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
                print(f"      - í•„ìš”í•œ íŒŒì¼: vocabulary.json, tokens.json ë“±")
                print(f"      - download_model_hf.pyì˜ CTranslate2 ë³€í™˜ ë¡œê·¸ í™•ì¸")
            elif "model.bin" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: model.bin ë¡œë“œ ì˜¤ë¥˜")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 1: model.bin íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¶ˆì™„ì „í•¨")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 2: CTranslate2 ë³€í™˜ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŒ")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 3: config.jsonì´ ì†ìƒë¨ (2.2KB ì´ìƒì¸ì§€ í™•ì¸)")
                print(f"\n   ğŸ”§ í•´ê²° ë°©ë²•:")
                print(f"      1. EC2ì—ì„œ ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ:")
                print(f"         rm -rf models/openai_whisper-large-v3-turbo")
                print(f"         python3 download_model_hf.py")
                print(f"      2. Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ:")
                print(f"         bash scripts/build-server-image.sh")
                print(f"      3. ì»¨í…Œì´ë„ˆ ì¬ì‹¤í–‰ (ëª¨ë¸ ë§ˆìš´íŠ¸)")
                print(f"         docker run -v $(pwd)/models:/app/models ...")
            elif "not found" in error_str.lower() or "no such file" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜")
                print(f"      - ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
                print(f"      - ctranslate2_model í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸")
                print(f"      - model.bin íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸")
                print(f"\n   ğŸ”§ í•´ê²° ë°©ë²•:")
                print(f"      1. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í™•ì¸:")
                print(f"         docker exec -it <container> ls -lh /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/")
                print(f"      2. ë§ˆìš´íŠ¸ í™•ì¸:")
                print(f"         docker inspect <container> | grep -A 5 'Mounts'")
            else:
                print(f"\n   ğŸ’¡ ìƒì„¸ ì§„ë‹¨ì„ ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
                print(f"      1. {self.model_path}/ctranslate2_model/ í´ë” ì¡´ì¬")
                print(f"      2. model.bin íŒŒì¼ í¬ê¸° (100MB ì´ìƒ, ì¼ë°˜ì ìœ¼ë¡œ 1.5GB)")
                print(f"      3. config.json íŒŒì¼ í¬ê¸° (2.2KB ì´ìƒ)")
                print(f"      4. vocabulary.json íŒŒì¼ ì¡´ì¬")
                print(f"\n   ğŸ“‹ ë””ë²„ê·¸ ëª…ë ¹ì–´:")
                print(f"      docker exec -it <container> bash")
                print(f"      ls -lh /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/")
                print(f"      file /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/config.json")

    
    def _try_transformers(self):
        """
        transformers WhisperForConditionalGenerationìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„
        
        íŠ¹ì§•:
        - Hugging Face ëª¨ë¸ ì§ì ‘ ì§€ì› (PyTorch/SafeTensors)
        - large-v3-turbo í¬í•¨ ëª¨ë“  HF Whisper ëª¨ë¸ ì§€ì›
        - GPU ê°€ì† ê°€ëŠ¥
        - ë” ëŠë¦¼ (faster-whisper ëŒ€ë¹„ 2-3ë°°)
        
        ì§€ì› íŒŒì¼:
        - pytorch_model.bin (PyTorch í˜•ì‹)
        - model.safetensors (SafeTensors í˜•ì‹, ë” ë¹ ë¦„)
        - config.json, tokenizer.json ë“±
        """
        try:
            print(f"ğŸ”„ transformersë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device})")
            
            from transformers import WhisperProcessor, WhisperForConditionalGeneration
            import torch
            
            model_path = Path(self.model_path)
            
            # PyTorch ëª¨ë¸ íŒŒì¼ í™•ì¸
            has_pytorch = (model_path / "pytorch_model.bin").exists()
            has_safetensors = (model_path / "model.safetensors").exists()
            
            if not (has_pytorch or has_safetensors):
                print(f"   âš ï¸  PyTorch ëª¨ë¸ íŒŒì¼ ì—†ìŒ (pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”)")
                return
            
            # ë¡œì»¬ ìºì‹œì—ì„œ ë¡œë“œ (HF í—ˆë¸Œ ì ‘ê·¼ ë°©ì§€)
            processor = WhisperProcessor.from_pretrained(str(model_path), local_files_only=True)
            model = WhisperForConditionalGeneration.from_pretrained(str(model_path), local_files_only=True)
            
            # GPUë¡œ ì´ë™
            if self.device == "cuda" and torch.cuda.is_available():
                model = model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œ
            model.eval()
            
            self.backend = type('TransformersBackend', (), {
                'processor': processor,
                'model': model,
                'device': self.device,
                'transcribe': self._transcribe_with_transformers
            })()
            
            print(f"   âœ… transformers ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"      íƒ€ì…: WhisperForConditionalGeneration")
            print(f"      íŒŒì¼: {'SafeTensors' if has_safetensors else 'PyTorch'}")
            
        except FileNotFoundError as e:
            print(f"   âš ï¸  ë¡œì»¬ ìºì‹œ ì‹¤íŒ¨: {e}")
            print(f"      ì‹œë„: Hugging Face í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ...")
            
            try:
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                import torch
                
                processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3-turbo")
                model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3-turbo")
                
                if self.device == "cuda" and torch.cuda.is_available():
                    model = model.to(self.device)
                
                model.eval()
                
                self.backend = type('TransformersBackend', (), {
                    'processor': processor,
                    'model': model,
                    'device': self.device,
                    'transcribe': self._transcribe_with_transformers
                })()
                
                print(f"   âœ… HF í—ˆë¸Œì—ì„œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                
            except Exception as e2:
                print(f"   âŒ HF í—ˆë¸Œ ë¡œë“œ ì‹¤íŒ¨: {type(e2).__name__}")
                
        except Exception as e:
            print(f"   âŒ transformers ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
            print(f"      ì—ëŸ¬: {str(e)[:150]}")
    
    def _transcribe_with_transformers(self, audio_path: str, language: Optional[str] = None) -> Dict:
        """
        transformersë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹
        """
        import librosa
        import torch
        
        try:
            # ìŒì„± ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # í”„ë¡œì„¸ì‹±
            input_features = self.backend.processor(audio, sampling_rate=16000, return_tensors="pt").input_features
            
            if self.device == "cuda":
                input_features = input_features.to(self.device)
            
            # ì¶”ë¡ 
            with torch.no_grad():
                predicted_ids = self.backend.model.generate(input_features)
            
            # ë””ì½”ë”©
            transcription = self.backend.processor.batch_decode(predicted_ids, skip_special_tokens=True)
            
            return {
                "text": transcription[0] if transcription else "",
                "language": language or "auto",
                "backend": "transformers"
            }
        
        except Exception as e:
            return {
                "text": "",
                "error": f"transformers transcription failed: {e}",
                "backend": "transformers"
            }

    
    def _try_whisper(self):
        """
        OpenAI Whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„
        
        âš ï¸ ê³µì‹ ëª¨ë¸ë§Œ ì§€ì› (tiny, base, small, medium, large)
        large-v3-turboëŠ” OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹ˆë¯€ë¡œ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
        """
        try:
            print(f"ğŸ”„ OpenAI Whisper ì‹œë„... (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›)")
            
            import whisper
            
            # OpenAI Whisper ì§€ì› ëª¨ë¸ í™•ì¸
            supported_models = whisper.available_models()
            
            model_path = Path(self.model_path)
            model_name = model_path.name
            
            # large-v3-turboëŠ” OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜
            if "turbo" in model_name.lower() or "v3-turbo" in model_name.lower():
                print(f"   âš ï¸  '{model_name}'ì€(ëŠ”) OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜")
                print(f"       ì§€ì› ëª¨ë¸: {supported_models}")
                return
            
            # ê³µì‹ ëª¨ë¸ì¸ì§€ í™•ì¸
            if not any(m in model_name for m in ['tiny', 'base', 'small', 'medium', 'large']):
                print(f"   âš ï¸  '{model_name}'ì€(ëŠ”) ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜")
                print(f"       ì§€ì› ëª¨ë¸: {supported_models}")
                return
            
            # ë¡œë“œ ì‹œë„
            model = whisper.load_model("large", device=self.device)
            
            self.backend = type('WhisperBackend', (), {
                'model': model,
                'device': self.device,
                'transcribe': self._transcribe_with_whisper
            })()
            
            print(f"   âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (large)")
            
        except Exception as e:
            print(f"   âŒ OpenAI Whisper ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
    
    def _transcribe_with_whisper(self, audio_path: str, language: Optional[str] = None) -> Dict:
        """OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹"""
        try:
            result = self.backend.model.transcribe(audio_path, language=language)
            return {
                "text": result.get("text", ""),
                "language": result.get("language", ""),
                "backend": "whisper"
            }
        except Exception as e:
            return {
                "text": "",
                "error": f"whisper transcription failed: {e}",
                "backend": "whisper"
            }
    
    @staticmethod
    def _explain_openai_whisper_limitations():
        """
        OpenAI Whisperì˜ ì•„í‚¤í…ì²˜ ì œí•œì‚¬í•­ ì„¤ëª…
        (ì´ ë©”ì„œë“œëŠ” ì°¸ê³  ëª©ì ìœ¼ë¡œë§Œ ìœ ì§€ë¨)
        
        âš ï¸ OpenAI Whisper.load_model()ì˜ ì œí•œì‚¬í•­:
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        - ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›: tiny, base, small, medium, large, turbo
        - ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì› ì•ˆí•¨: large-v3, large-v3-turbo ë“±
        - ë¡œì»¬ PyTorch ëª¨ë¸ ì§ì ‘ ë¡œë“œ ë¶ˆê°€
        - ëª¨ë¸ëª… hardcoding: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ëª¨ë¸ ê±°ë¶€
        
        ë”°ë¼ì„œ faster-whisper + CTranslate2ê°€ ìœ ì¼í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.
        """
        print("\nâŒ OpenAI Whisper ì§€ì› ë¶ˆê°€ (ì•„í‚¤í…ì²˜ ì œí•œ):")
        print("â”" * 60)
        print("OpenAI Whisper.load_model()ì€ ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:")
        print("  âœ“ tiny.en, tiny, base.en, base")
        print("  âœ“ small.en, small, medium.en, medium")
        print("  âœ“ large, turbo (ì¼ë¶€)")
        print("\nHugging Face ì»¤ìŠ¤í…€ ëª¨ë¸ ë¯¸ì§€ì›:")
        print("  âœ— large-v3, large-v3-turbo (ì´ í”„ë¡œì íŠ¸ì˜ ëª¨ë¸)")
        print("  âœ— ë¡œì»¬ PyTorch ëª¨ë¸ ì§ì ‘ ë¡œë“œ ë¶ˆê°€")
        print("\nğŸ’¡ ì†”ë£¨ì…˜: faster-whisper + CTranslate2 ë˜ëŠ” transformers ì‚¬ìš©")
        print("â”" * 60)

    
    @staticmethod
    def _is_cuda_available() -> bool:
        """CUDA ê°€ìš©ì„± í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def transcribe(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
            **kwargs: ì¶”ê°€ ì˜µì…˜
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            
            # ë°±ì—”ë“œ íƒ€ì… í™•ì¸ ë° ì²˜ë¦¬
            if self.backend is None:
                raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # ë°±ì—”ë“œ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
            backend_type = type(self.backend).__name__
            print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ë°±ì—”ë“œ: {backend_type} (ê°ì²´: {self.backend})")
            
            if backend_type == 'WhisperModel':
                # faster-whisper
                return self._transcribe_faster_whisper(audio_path, language, **kwargs)
            elif backend_type == 'TransformersBackend':
                # transformers
                return self._transcribe_with_transformers(audio_path, language)
            elif backend_type == 'WhisperBackend':
                # OpenAI Whisper
                return self._transcribe_with_whisper(audio_path, language)
            elif backend_type == 'str':
                # ë¬¸ìì—´ì´ ì €ì¥ëœ ê²½ìš° (ë²„ê·¸)
                raise RuntimeError(f"âŒ ë²„ê·¸: backendê°€ ë¬¸ìì—´ë¡œ ì €ì¥ë¨: {self.backend}")
            else:
                # ì œë„¤ë¦­ ë°±ì—”ë“œ ê°ì²´ ì²˜ë¦¬
                if hasattr(self.backend, 'transcribe'):
                    result = self.backend.transcribe(audio_path, language)
                    return result
                else:
                    raise RuntimeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ: {backend_type} (value: {self.backend})")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_path": audio_path
            }
    
    def _transcribe_faster_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """faster-whisper (WhisperModel)ë¡œ ë³€í™˜"""
        segments, info = self.backend.transcribe(
            audio_path,
            language=language,
            beam_size=kwargs.get("beam_size", 5),
            best_of=kwargs.get("best_of", 5),
            patience=kwargs.get("patience", 1),
            temperature=kwargs.get("temperature", 0)
        )
        
        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
        text = "".join([segment.text for segment in segments])
        detected_language = info.language if info else language or "unknown"
        
        return {
            "success": True,
            "text": text.strip(),
            "audio_path": audio_path,
            "language": detected_language,
            "duration": info.duration if info else None,
            "backend": "faster-whisper"
        }
    
    def _transcribe_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """OpenAI Whisperë¡œ ë³€í™˜"""
        result = self.model.transcribe(
            audio_path,
            language=language
        )
        
        text = result.get("text", "").strip()
        
        return {
            "success": True,
            "text": text,
            "audio_path": audio_path,
            "language": language or "unknown",
            "duration": None,
            "backend": "whisper"
        }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ë””ë²„ê¹…ìš©, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    ì°¸ê³ : FastAPI ì„œë¹„ìŠ¤ (api_server.py)ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•ŒëŠ”
         ì´ í•¨ìˆ˜ê°€ ì•„ë‹Œ WhisperSTT í´ë˜ìŠ¤ë¥¼ ì§ì ‘ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # STT ì´ˆê¸°í™”
    stt = WhisperSTT(
        model_path,
        device=device,
        compute_type="float16"
    )
    
    print(f"\nğŸ“Š ì‚¬ìš© ë°±ì—”ë“œ: {stt.backend}\n")
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg", "*.m4a")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
            if result.get("duration"):
                print(f"â±ï¸  ìŒì„± ê¸¸ì´: {result['duration']:.1f}ì´ˆ")
            print(f"ğŸ”§ ì‚¬ìš© ë°±ì—”ë“œ: {result.get('backend', 'unknown')}")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ğŸ”´ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


# ============================================================================
# ì£¼ì˜: ì´ íŒŒì¼ì€ api_server.pyì˜ FastAPI ì„œë¹„ìŠ¤ì—ì„œ importë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.
# api_server.py:
#   from stt_engine import WhisperSTT
#   stt = WhisperSTT(model_path=..., device=...)
#   result = stt.transcribe(audio_path)
#
# ë”°ë¼ì„œ ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
# ë§Œì•½ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:
#   python stt_engine.py  (ë‹¨, audio/ ë””ë ‰í† ë¦¬ì— ìŒì„± íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
# ============================================================================
