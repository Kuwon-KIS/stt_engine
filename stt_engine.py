#!/usr/bin/env python3
"""
STT ëª¨ë“ˆ - faster-whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜
ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„ì™€ ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ìµœì í™”ë¨
"""

import os
from pathlib import Path
from typing import Optional, Dict
import tarfile
from faster_whisper import WhisperModel


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì˜ˆ: "models")
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ (models/openai_whisper-large-v3-turbo)
    
    Raises:
        RuntimeError: ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨
        FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                # ì•ˆì „ì„± ê²€ì‚¬: tar ë©¤ë²„ ê²€ì¦
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        raise RuntimeError(f"ë³´ì•ˆ ìœ„í—˜: ì˜ëª»ëœ ê²½ë¡œ {member.name}")
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except tarfile.TarError as e:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ tar íŒŒì¼: {e}")
            raise RuntimeError(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}") from e
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """faster-whisperë¥¼ ì‚¬ìš©í•œ STT í´ë˜ìŠ¤ (3-4ë°° ë¹ ë¥¸ ì¶”ë¡ )"""
    
    def __init__(self, model_path: str, device: str = "cpu", compute_type: str = "float16"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: "models/openai_whisper-large-v3-turbo")
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', ë˜ëŠ” 'auto')
            compute_type: ê³„ì‚° íƒ€ì… ('float32', 'float16', 'int8')
                        - float16: ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ê¶Œì¥)
                        - float32: ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
                        - int8: ê°€ì¥ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (VRAM <2GB)
        
        Raises:
            FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
            RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        models_dir = str(Path(model_path).parent)
        self.model_path = str(auto_extract_model_if_needed(models_dir))
        
        self.device = device if device != "auto" else ("cuda" if self._is_cuda_available() else "cpu")
        self.compute_type = compute_type
        
        print(f"ğŸ”„ faster-whisper ëª¨ë¸ ë¡œë“œ ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device}, compute: {compute_type})")
        
        # faster-whisper ëª¨ë¸ ë¡œë“œ
        # model_size_or_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ë¡œì»¬) ë˜ëŠ” ëª¨ë¸ ì´ë¦„ (tiny, base, small, medium, large)
        try:
            self.model = WhisperModel(
                self.model_path,
                device=self.device,
                compute_type=self.compute_type,
                num_workers=4,
                cpu_threads=4,
                download_root=None
            )
            print(f"âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except FileNotFoundError:
            print(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            print(f"ğŸ’¡ ë‹¤ìŒ ê²½ë¡œì— ëª¨ë¸ì„ ë°°ì¹˜í•˜ì„¸ìš”: {self.model_path}")
            raise
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}") from e
    
    @staticmethod
    def _is_cuda_available() -> bool:
        """CUDA ê°€ìš©ì„± í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def transcribe(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko' for Korean, 'en' for English)
                     Noneì´ë©´ ìë™ ê°ì§€
            **kwargs: ì¶”ê°€ ì˜µì…˜
                - beam_size: ë¹” ì„œì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 5, ë²”ìœ„: 1-30)
                - best_of: ìƒ˜í”Œë§ ìµœì í™” (ê¸°ë³¸ê°’: 5)
                - patience: ì¡°ê¸° ì¢…ë£Œ patience (ê¸°ë³¸ê°’: 1)
                - temperature: ì˜¨ë„ (ê¸°ë³¸ê°’: 0)
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            
            # faster-whisper transcribe (ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì²˜ë¦¬)
            # language: ì–¸ì–´ í† í° ì„¤ì • (ëª…ì‹œí•˜ë©´ ë” ë¹ ë¦„)
            segments, info = self.model.transcribe(
                audio_path,
                language=language,
                beam_size=kwargs.get("beam_size", 5),
                best_of=kwargs.get("best_of", 5),
                patience=kwargs.get("patience", 1),
                temperature=kwargs.get("temperature", 0),
                verbose=False
            )
            
            # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
            text = "".join([segment.text for segment in segments])
            detected_language = info.language if info else language or "unknown"
            
            return {
                "success": True,
                "text": text.strip(),
                "audio_path": audio_path,
                "language": detected_language,
                "duration": info.duration if info else None
            }
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_path": audio_path
            }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    """
    # STT ì´ˆê¸°í™” (float16ìœ¼ë¡œ ìµœì í™”, VRAM 3-4GB)
    stt = WhisperSTT(
        model_path,
        device=device,
        compute_type="float16"  # ë¹ ë¥´ê³  íš¨ìœ¨ì 
    )
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg", "*.m4a")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
            if result.get("duration"):
                print(f"â±ï¸  ìŒì„± ê¸¸ì´: {result['duration']:.1f}ì´ˆ")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ğŸ”´ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


if __name__ == "__main__":
    import sys
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = str(Path(__file__).parent / "models" / "openai_whisper-large-v3-turbo")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda"  # faster-whisperëŠ” CUDA ìë™ìœ¼ë¡œ ì¸ì‹
    
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_stt(model_path, audio_dir="audio", device=device)
