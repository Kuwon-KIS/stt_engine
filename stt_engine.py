#!/usr/bin/env python3
"""
STT ëª¨ë“ˆ - faster-whisper / OpenAI Whisper ìë™ ì„ íƒ
faster-whisper ìš°ì„  ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ OpenAI Whisperë¡œ í´ë°±
"""

import os
from pathlib import Path
from typing import Optional, Dict
import tarfile

# ë‘ ê°€ì§€ ë°±ì—”ë“œ ì‹œë„
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

try:
    import openai_whisper as whisper
    WHISPER_AVAILABLE = True
except ImportError:
    try:
        import whisper
        WHISPER_AVAILABLE = True
    except ImportError:
        WHISPER_AVAILABLE = False

if not FASTER_WHISPER_AVAILABLE and not WHISPER_AVAILABLE:
    raise ImportError("faster-whisper ë˜ëŠ” openai-whisper(whisper) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤")


def validate_faster_whisper_model(model_path: str) -> bool:
    """
    faster-whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦ (CTranslate2 ëª¨ë¸ í˜•ì‹)
    faster-whisperëŠ” model_path ë‚´ì—ì„œ ctranslate2_model í´ë”ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    í•„ìˆ˜ í´ë”: model_path/ctranslate2_model/
    í•„ìˆ˜ íŒŒì¼: model.bin, config.json, vocabulary.json, tokenizer.json ë“±
    
    Args:
        model_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ì˜ˆ: /app/models/openai_whisper-large-v3-turbo)
    
    Returns:
        True if ìœ íš¨, False otherwise
    """
    model_dir = Path(model_path)
    ct_model_dir = model_dir / "ctranslate2_model"
    
    print(f"   ğŸ“‚ faster-whisper ëª¨ë¸ ê²€ì¦: {model_path}")
    
    # ctranslate2_model í´ë” í™•ì¸
    if not ct_model_dir.exists():
        print(f"   âš ï¸  ctranslate2_model í´ë” ì—†ìŒ: {ct_model_dir}")
        return False
    
    # ctranslate2_model ë‚´ íŒŒì¼ í™•ì¸
    ct_files = list(ct_model_dir.glob("*"))
    if not ct_files:
        print(f"   âš ï¸  ctranslate2_model í´ë”ê°€ ë¹„ì–´ìˆìŒ: {ct_model_dir}")
        return False
    
    print(f"   âœ“ ctranslate2_model í´ë” ìˆìŒ ({len(ct_files)}ê°œ íŒŒì¼)")
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
    critical_files = ["model.bin"]
    missing_critical = []
    
    for file in critical_files:
        file_path = ct_model_dir / file
        if not file_path.exists():
            missing_critical.append(file)
    
    if missing_critical:
        print(f"   âš ï¸  í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {', '.join(missing_critical)}")
        return False
    
    # model.bin íŒŒì¼ í¬ê¸° í™•ì¸ (ì†ìƒ ì—¬ë¶€ íŒë‹¨)
    model_bin = ct_model_dir / "model.bin"
    size_mb = model_bin.stat().st_size / (1024 * 1024)
    print(f"   âœ“ model.bin ìˆìŒ ({size_mb:.1f} MB)")
    
    if size_mb < 100:  # 100MB ë¯¸ë§Œì´ë©´ ì†ìƒ ê°€ëŠ¥ì„±
        print(f"   âš ï¸  ê²½ê³ : model.bin íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ ({size_mb:.1f} MB) - ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŒ")
        return False
    
    print(f"   âœ“ faster-whisper ëª¨ë¸ êµ¬ì¡° ìœ íš¨")
    return True


def validate_whisper_model(model_path: str) -> bool:
    """
    OpenAI Whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦ (PyTorch ëª¨ë¸ í˜•ì‹)
    
    ì£¼ì˜: OpenAI WhisperëŠ” ê³µì‹ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:
    - tiny, base, small, medium, large
    
    "large-v3", "large-v3-turbo" ê°™ì€ ë³€í˜•ì€ huggingfaceì—ì„œë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ
    ìš´ì˜ì„œë²„ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.
    
    Args:
        model_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ì°¸ê³ ìš©)
    
    Returns:
        True if ìœ íš¨, False otherwise
    """
    model_dir = Path(model_path)
    
    if not model_dir.exists():
        print(f"   âš ï¸  ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        return False
    
    # pytorch_model.bin ë˜ëŠ” model.safetensors ì¤‘ í•˜ë‚˜ í•„ìš”
    has_pytorch = (model_dir / "pytorch_model.bin").exists()
    has_safetensors = (model_dir / "model.safetensors").exists()
    
    if not (has_pytorch or has_safetensors):
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”")
        return False
    
    # config.json, tokens.json í•„ìˆ˜
    required_files = ["config.json", "tokenizer.json"]
    missing_files = []
    
    for file in required_files:
        file_path = model_dir / file
        if not file_path.exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {', '.join(missing_files)}")
        return False
    
    print(f"   âœ“ Whisper ëª¨ë¸ êµ¬ì¡° ìœ íš¨")
    return True


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì˜ˆ: "models")
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ (models/openai_whisper-large-v3-turbo)
    
    Raises:
        RuntimeError: ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨
        FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                # ì•ˆì „ì„± ê²€ì‚¬: tar ë©¤ë²„ ê²€ì¦
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        raise RuntimeError(f"ë³´ì•ˆ ìœ„í—˜: ì˜ëª»ëœ ê²½ë¡œ {member.name}")
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except tarfile.TarError as e:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ tar íŒŒì¼: {e}")
            raise RuntimeError(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}") from e
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """faster-whisper / OpenAI Whisper ìë™ ì„ íƒ STT í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device: str = "cpu", compute_type: str = "float16"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: "models")
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', ë˜ëŠ” 'auto')
            compute_type: ê³„ì‚° íƒ€ì… (faster-whisperìš©, 'float32', 'float16', 'int8')
        
        Raises:
            FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
            RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        models_dir = str(Path(model_path).parent)
        self.model_path = str(auto_extract_model_if_needed(models_dir))
        
        # ëª¨ë¸ ê²½ë¡œ ìœ íš¨ì„± ìµœì¢… í™•ì¸
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                                  f"ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
                                  f"1. ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ê°€? (download_model_hf.py ì‹¤í–‰)\n"
                                  f"2. ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ê°€? (ê¸°ë³¸ê°’: models/openai_whisper-large-v3-turbo)\n"
                                  f"3. ìš´ì˜ì„œë²„ì¸ê°€? (ì˜¤í”„ë¼ì¸ ë°°í¬ì¸ ê²½ìš° ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ì‹œì¼œì•¼ í•¨)")
        
        self.device = device if device != "auto" else ("cuda" if self._is_cuda_available() else "cpu")
        self.compute_type = compute_type
        self.backend = None
        
        print(f"\nğŸ“Š ëª¨ë¸ ë¡œë“œ ì‹œì‘")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ: faster-whisper={FASTER_WHISPER_AVAILABLE}, whisper={WHISPER_AVAILABLE}\n")
        
        # faster-whisper ë¨¼ì € ì‹œë„
        if FASTER_WHISPER_AVAILABLE:
            self._try_faster_whisper()
        
        # faster-whisper ì‹¤íŒ¨í•˜ë©´ OpenAI Whisper ì‹œë„
        if self.backend is None and WHISPER_AVAILABLE:
            self._try_whisper()
        
        # ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬
        if self.backend is None:
            raise RuntimeError(
                "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ë‘ ë°±ì—”ë“œ ëª¨ë‘ ì‹¤íŒ¨\n\n"
                "ğŸ”§ ìš´ì˜ì„œë²„(ì˜¤í”„ë¼ì¸) ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                "1. faster-whisper ëª¨ë¸ (ì¶”ì²œ):\n"
                f"   ê²½ë¡œ: {self.model_path}\n"
                f"   í•„ìˆ˜: {self.model_path}/ctranslate2_model/model.bin\n"
                "   ê²€ì¦: ëª¨ë¸ íŒŒì¼ í¬ê¸° 100MB ì´ìƒì¸ì§€ í™•ì¸\n\n"
                "2. OpenAI Whisper (ëŒ€ì²´):\n"
                "   ì§€ì› ëª¨ë¸: tiny, base, small, medium, large\n"
                "   ì£¼ì˜: large-v3-turboëŠ” ìš´ì˜ì„œë²„ì—ì„œ ë¶ˆê°€ëŠ¥\n"
                "   ëŒ€ì‹  'large' ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (ìë™ ë‹¤ìš´ë¡œë“œ)\n\n"
                "3. ëª¨ë¸ íŒŒì¼ í™•ì¸:\n"
                f"   faster-whisper: find {self.model_path} -name 'model.bin'\n"
                f"   íŒŒì¼ì´ ì—†ê±°ë‚˜ 100MB ë¯¸ë§Œì´ë©´ ì†ìƒë¨"
            )
    
    def _try_faster_whisper(self):
        """faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©)"""
        try:
            print(f"ğŸ”„ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device}, compute: {self.compute_type})")
            
            # ëª¨ë¸ êµ¬ì¡° ë¨¼ì € ê²€ì¦
            if not validate_faster_whisper_model(self.model_path):
                print(f"   â†’ faster-whisper ëª¨ë¸ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨")
                return
            
            # ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë„ë¡ ë¡œë“œ
            self.model = WhisperModel(
                self.model_path,
                device=self.device,
                compute_type=self.compute_type,
                num_workers=4,
                cpu_threads=4,
                download_root=None,
                local_files_only=True  # ğŸ”’ ìš´ì˜ì„œë²„ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°©ì§€
            )
            
            self.backend = "faster-whisper"
            print(f"âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
        except FileNotFoundError as e:
            print(f"âš ï¸  faster-whisper: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - {e}")
            print(f"   â†’ OpenAI Whisperë¡œ í´ë°± ì‹œë„...")
        except Exception as e:
            print(f"âš ï¸  faster-whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   â†’ OpenAI Whisperë¡œ í´ë°± ì‹œë„...")
    
    def _try_whisper(self):
        """OpenAI Whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ì˜¤í”„ë¼ì¸ í™˜ê²½ ê³ ë ¤)"""
        try:
            print(f"ğŸ”„ OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device})")
            
            model_path = Path(self.model_path)
            
            # ìš´ì˜ì„œë²„ ì˜¤í”„ë¼ì¸ í™˜ê²½: ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì§€ì› ì—†ìŒ
            # OpenAI WhisperëŠ” ê³µì‹ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë§Œ ì§€ì›:
            # tiny, base, small, medium, large
            #
            # "large-v3-turbo" ê°™ì€ ì»¤ìŠ¤í…€ ëª¨ë¸ì€ huggingfaceì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
            # ë”°ë¼ì„œ ìš´ì˜ì„œë²„ì—ì„œëŠ” ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ê³µì‹ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
            
            available_models = ["tiny", "base", "small", "medium", "large"]
            
            print(f"   ğŸ“ OpenAI Whisper ê³µì‹ ì§€ì› ëª¨ë¸: {', '.join(available_models)}")
            print(f"   âš ï¸  ì£¼ì˜: large-v3-turbo ê°™ì€ ì»¤ìŠ¤í…€ ëª¨ë¸ì€ ìš´ì˜ì„œë²„ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            print(f"   â†’ ëŒ€ì‹  'large' ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤")
            
            # ê³µì‹ ëª¨ë¸ 'large' ì‚¬ìš©
            self.model = whisper.load_model(
                "large",
                device=self.device,
                in_memory=False,
                download_root=None
            )
            
            self.backend = "whisper"
            print(f"âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ëª¨ë¸: large)")
            
        except FileNotFoundError as e:
            print(f"âŒ OpenAI Whisper: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - {e}")
            print(f"   ğŸ’¡ íŒ: ìš´ì˜ì„œë²„ì—ì„œ ì»¤ìŠ¤í…€ ëª¨ë¸(large-v3-turbo)ì„ ì‚¬ìš©í•˜ë ¤ë©´")
            print(f"        ëª¨ë¸ì„ Docker ì´ë¯¸ì§€ì— í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤")
        except Exception as e:
            print(f"âŒ OpenAI Whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def _is_cuda_available() -> bool:
        """CUDA ê°€ìš©ì„± í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def transcribe(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
            **kwargs: ì¶”ê°€ ì˜µì…˜
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            
            # ë°±ì—”ë“œë³„ ì²˜ë¦¬
            if self.backend == "faster-whisper":
                return self._transcribe_faster_whisper(audio_path, language, **kwargs)
            elif self.backend == "whisper":
                return self._transcribe_whisper(audio_path, language, **kwargs)
            else:
                raise RuntimeError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°±ì—”ë“œ: {self.backend}")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_path": audio_path
            }
    
    def _transcribe_faster_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """faster-whisperë¡œ ë³€í™˜"""
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            beam_size=kwargs.get("beam_size", 5),
            best_of=kwargs.get("best_of", 5),
            patience=kwargs.get("patience", 1),
            temperature=kwargs.get("temperature", 0),
            verbose=False
        )
        
        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
        text = "".join([segment.text for segment in segments])
        detected_language = info.language if info else language or "unknown"
        
        return {
            "success": True,
            "text": text.strip(),
            "audio_path": audio_path,
            "language": detected_language,
            "duration": info.duration if info else None,
            "backend": "faster-whisper"
        }
    
    def _transcribe_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """OpenAI Whisperë¡œ ë³€í™˜"""
        result = self.model.transcribe(
            audio_path,
            language=language
        )
        
        text = result.get("text", "").strip()
        
        return {
            "success": True,
            "text": text,
            "audio_path": audio_path,
            "language": language or "unknown",
            "duration": None,
            "backend": "whisper"
        }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ë””ë²„ê¹…ìš©, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    ì°¸ê³ : FastAPI ì„œë¹„ìŠ¤ (api_server.py)ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•ŒëŠ”
         ì´ í•¨ìˆ˜ê°€ ì•„ë‹Œ WhisperSTT í´ë˜ìŠ¤ë¥¼ ì§ì ‘ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # STT ì´ˆê¸°í™”
    stt = WhisperSTT(
        model_path,
        device=device,
        compute_type="float16"
    )
    
    print(f"\nğŸ“Š ì‚¬ìš© ë°±ì—”ë“œ: {stt.backend}\n")
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg", "*.m4a")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
            if result.get("duration"):
                print(f"â±ï¸  ìŒì„± ê¸¸ì´: {result['duration']:.1f}ì´ˆ")
            print(f"ğŸ”§ ì‚¬ìš© ë°±ì—”ë“œ: {result.get('backend', 'unknown')}")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ğŸ”´ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


# ============================================================================
# ì£¼ì˜: ì´ íŒŒì¼ì€ api_server.pyì˜ FastAPI ì„œë¹„ìŠ¤ì—ì„œ importë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.
# api_server.py:
#   from stt_engine import WhisperSTT
#   stt = WhisperSTT(model_path=..., device=...)
#   result = stt.transcribe(audio_path)
#
# ë”°ë¼ì„œ ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
# ë§Œì•½ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:
#   python stt_engine.py  (ë‹¨, audio/ ë””ë ‰í† ë¦¬ì— ìŒì„± íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
# ============================================================================
