#!/usr/bin/env python3
"""
STT í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ
ìŒì„± íŒŒì¼ì„ ë°›ì•„ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
from pathlib import Path
from typing import Optional, Dict
import torch
import torchaudio
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import tarfile


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """Whisper ëª¨ë¸ì„ ì‚¬ìš©í•œ STT í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu' ë˜ëŠ” 'cuda')
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        model_path = str(auto_extract_model_if_needed(
            Path(model_path).parent
        ))
        
        self.device = device
        self.model_path = model_path
        
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (ë””ë°”ì´ìŠ¤: {device})")
        
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(model_path)
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path)
        self.model.to(device)
        self.model.eval()
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def transcribe(self, audio_path: str, language: Optional[str] = None) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko' for Korean, 'en' for English)
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ìŒì„± íŒŒì¼ ë¡œë“œ
            print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ: {audio_path}")
            audio, sr = torchaudio.load(audio_path)
            
            # ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ 16kHzê°€ ì•„ë‹ˆë©´ ë¦¬ìƒ˜í”Œë§
            if sr != 16000:
                print(f"ğŸ”„ ìƒ˜í”Œë§ ë ˆì´íŠ¸ ë³€í™˜: {sr}Hz -> 16000Hz")
                resampler = torchaudio.transforms.Resample(sr, 16000)
                audio = resampler(audio)
            
            # ëª¨ë…¸ë¡œ ë³€í™˜
            if audio.shape[0] > 1:
                audio = audio.mean(dim=0, keepdim=True)
            
            # í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì²˜ë¦¬
            inputs = self.processor(
                audio.squeeze().numpy(),
                sampling_rate=16000,
                return_tensors="pt"
            )
            
            # ëª¨ë¸ë¡œ ì¶”ë¡ 
            with torch.no_grad():
                predicted_ids = self.model.generate(
                    inputs["input_features"].to(self.device),
                    language=language
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            transcription = self.processor.batch_decode(
                predicted_ids,
                skip_special_tokens=True
            )
            
            return {
                "success": True,
                "text": transcription[0],
                "audio_path": audio_path,
                "language": language
            }
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_path": audio_path
            }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    """
    # STT ì´ˆê¸°í™”
    stt = WhisperSTT(model_path, device=device)
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ì˜¤ë¥˜: {result['error']}")


if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = Path(__file__).parent / "models" / "openai_whisper-large-v3-turbo"
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # STT í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_stt(str(model_path), device=device)
