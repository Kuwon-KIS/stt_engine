#!/usr/bin/env python3
"""
STT ëª¨ë“ˆ - faster-whisper / OpenAI Whisper ìë™ ì„ íƒ
faster-whisper ìš°ì„  ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ OpenAI Whisperë¡œ í´ë°±
"""

import os
from pathlib import Path
from typing import Optional, Dict
import tarfile

# ë‘ ê°€ì§€ ë°±ì—”ë“œ ì‹œë„
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

try:
    import openai_whisper as whisper
    WHISPER_AVAILABLE = True
except ImportError:
    try:
        import whisper
        WHISPER_AVAILABLE = True
    except ImportError:
        WHISPER_AVAILABLE = False

if not FASTER_WHISPER_AVAILABLE and not WHISPER_AVAILABLE:
    raise ImportError("faster-whisper ë˜ëŠ” openai-whisper(whisper) íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤")


def diagnose_faster_whisper_model(model_path: str) -> dict:
    """
    faster-whisper ëª¨ë¸ ìƒì„¸ ì§„ë‹¨ (ë””ë²„ê¹…ìš©)
    
    CTranslate2 ëª¨ë¸ì€ ë‹¤ìŒ íŒŒì¼ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
    - model.bin (CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬)
    - config.json (ëª¨ë¸ ì„¤ì •)
    - vocabulary.json (ë˜ëŠ” tokens.json) - í† í¬ë‚˜ì´ì € ì •ë³´
    - shared_vocabulary.json (ì„ íƒì‚¬í•­)
    
    Returns:
        {
            'valid': bool,
            'errors': [list of errors],
            'warnings': [list of warnings],
            'files': {detailed file structure},
            'model_bin_size_mb': float
        }
    """
    model_dir = Path(model_path)
    ct_model_dir = model_dir / "ctranslate2_model"
    
    diagnosis = {
        'valid': True,
        'errors': [],
        'warnings': [],
        'files': {},
        'model_bin_size_mb': None
    }
    
    # 1. ctranslate2_model í´ë” ì¡´ì¬ í™•ì¸
    if not ct_model_dir.exists():
        diagnosis['errors'].append(f"ctranslate2_model í´ë” ì—†ìŒ: {ct_model_dir}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 2. ctranslate2_model ë‚´ ëª¨ë“  íŒŒì¼ ë‚˜ì—´
    try:
        ct_files = list(ct_model_dir.rglob("*"))
        diagnosis['files']['total_count'] = len(ct_files)
        diagnosis['files']['list'] = []
        
        for file_path in sorted(ct_files)[:30]:  # ì²˜ìŒ 30ê°œ
            if file_path.is_file():
                size_kb = file_path.stat().st_size / 1024
                diagnosis['files']['list'].append({
                    'name': file_path.name,
                    'relative_path': str(file_path.relative_to(ct_model_dir)),
                    'size_kb': size_kb
                })
    except Exception as e:
        diagnosis['errors'].append(f"íŒŒì¼ ë‚˜ì—´ ì‹¤íŒ¨: {e}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 3. í•„ìˆ˜ íŒŒì¼ í™•ì¸ (CTranslate2 í¬ë§·)
    critical_files = {
        'model.bin': 'CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬',
        'config.json': 'Whisper ëª¨ë¸ ì„¤ì •'
    }
    
    for file_name, description in critical_files.items():
        file_path = ct_model_dir / file_name
        if not file_path.exists():
            diagnosis['errors'].append(f"ëˆ„ë½: {file_name} ({description})")
            diagnosis['valid'] = False
        else:
            size_kb = file_path.stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{file_name}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB (ì†ìƒ ê°€ëŠ¥ì„±)")
    
    # 4. í† í¬ë‚˜ì´ì € íŒŒì¼ í™•ì¸ (vocabulary.json ë˜ëŠ” tokens.json)
    # CTranslate2ëŠ” OpenAI Whisperì˜ tokenizer.jsonì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    vocab_files = ['vocabulary.json', 'tokens.json', 'tokenizer.json']
    has_vocab = False
    for vocab_file in vocab_files:
        if (ct_model_dir / vocab_file).exists():
            has_vocab = True
            size_kb = (ct_model_dir / vocab_file).stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{vocab_file}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB")
            break
    
    if not has_vocab:
        diagnosis['warnings'].append(f"í† í¬ë‚˜ì´ì € íŒŒì¼ ì—†ìŒ (vocabulary.json, tokens.json, tokenizer.json ì¤‘ í•˜ë‚˜ í•„ìš”)")
    
    # 5. model.bin ìƒì„¸ ê²€ì‚¬
    model_bin = ct_model_dir / "model.bin"
    if model_bin.exists():
        size_bytes = model_bin.stat().st_size
        size_mb = size_bytes / (1024 * 1024)
        diagnosis['model_bin_size_mb'] = size_mb
        
        if size_mb < 100:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° ì‘ìŒ: {size_mb:.1f}MB (ì†ìƒ ë˜ëŠ” ë³€í™˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
            diagnosis['valid'] = False
        
        if size_mb > 5000:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° í¼: {size_mb:.1f}MB (ì–‘ìí™” í™•ì¸ í•„ìš”)")
    
    return diagnosis


def validate_faster_whisper_model(model_path: str) -> bool:
    """
    faster-whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦
    diagnose_faster_whisper_modelì˜ ê°„ë‹¨í•œ ë˜í¼
    """
    diagnosis = diagnose_faster_whisper_model(model_path)
    
    print(f"   ğŸ“‚ faster-whisper ëª¨ë¸ ê²€ì¦: {model_path}")
    
    if diagnosis['files']['total_count'] > 0:
        print(f"   âœ“ ctranslate2_model í´ë” ìˆìŒ ({diagnosis['files']['total_count']}ê°œ íŒŒì¼)")
    
    if diagnosis['model_bin_size_mb']:
        print(f"   âœ“ model.bin: {diagnosis['model_bin_size_mb']:.1f}MB")
    
    for warning in diagnosis['warnings']:
        print(f"   âš ï¸  {warning}")
    
    for error in diagnosis['errors']:
        print(f"   âŒ {error}")
    
    return diagnosis['valid']


def validate_whisper_model(model_path: str) -> bool:
    """
    OpenAI Whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦ (PyTorch ëª¨ë¸ í˜•ì‹)
    
    ì£¼ì˜: OpenAI WhisperëŠ” ê³µì‹ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:
    - tiny, base, small, medium, large
    
    "large-v3", "large-v3-turbo" ê°™ì€ ë³€í˜•ì€ huggingfaceì—ì„œë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ
    ìš´ì˜ì„œë²„ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.
    
    Args:
        model_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ì°¸ê³ ìš©)
    
    Returns:
        True if ìœ íš¨, False otherwise
    """
    model_dir = Path(model_path)
    
    if not model_dir.exists():
        print(f"   âš ï¸  ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        return False
    
    # pytorch_model.bin ë˜ëŠ” model.safetensors ì¤‘ í•˜ë‚˜ í•„ìš”
    has_pytorch = (model_dir / "pytorch_model.bin").exists()
    has_safetensors = (model_dir / "model.safetensors").exists()
    
    if not (has_pytorch or has_safetensors):
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”")
        return False
    
    # config.json, tokens.json í•„ìˆ˜
    required_files = ["config.json", "tokenizer.json"]
    missing_files = []
    
    for file in required_files:
        file_path = model_dir / file
        if not file_path.exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {', '.join(missing_files)}")
        return False
    
    print(f"   âœ“ Whisper ëª¨ë¸ êµ¬ì¡° ìœ íš¨")
    return True


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì˜ˆ: "models")
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ (models/openai_whisper-large-v3-turbo)
    
    Raises:
        RuntimeError: ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨
        FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                # ì•ˆì „ì„± ê²€ì‚¬: tar ë©¤ë²„ ê²€ì¦
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        raise RuntimeError(f"ë³´ì•ˆ ìœ„í—˜: ì˜ëª»ëœ ê²½ë¡œ {member.name}")
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except tarfile.TarError as e:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ tar íŒŒì¼: {e}")
            raise RuntimeError(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}") from e
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """faster-whisper / OpenAI Whisper ìë™ ì„ íƒ STT í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device: str = "cpu", compute_type: str = "float16"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: "models")
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', ë˜ëŠ” 'auto')
            compute_type: ê³„ì‚° íƒ€ì… (faster-whisperìš©, 'float32', 'float16', 'int8')
        
        Raises:
            FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
            RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        models_dir = str(Path(model_path).parent)
        self.model_path = str(auto_extract_model_if_needed(models_dir))
        
        # ëª¨ë¸ ê²½ë¡œ ìœ íš¨ì„± ìµœì¢… í™•ì¸
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                                  f"ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
                                  f"1. ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ê°€? (download_model_hf.py ì‹¤í–‰)\n"
                                  f"2. ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ê°€? (ê¸°ë³¸ê°’: models/openai_whisper-large-v3-turbo)\n"
                                  f"3. ìš´ì˜ì„œë²„ì¸ê°€? (ì˜¤í”„ë¼ì¸ ë°°í¬ì¸ ê²½ìš° ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ì‹œì¼œì•¼ í•¨)")
        
        self.device = device if device != "auto" else ("cuda" if self._is_cuda_available() else "cpu")
        self.compute_type = compute_type
        self.backend = None
        
        print(f"\nğŸ“Š ëª¨ë¸ ë¡œë“œ ì‹œì‘")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ: faster-whisper={FASTER_WHISPER_AVAILABLE}, whisper={WHISPER_AVAILABLE}\n")
        
        # faster-whisper ë¨¼ì € ì‹œë„
        if FASTER_WHISPER_AVAILABLE:
            self._try_faster_whisper()
        
        # faster-whisper ì‹¤íŒ¨í•˜ë©´ OpenAI Whisper ì‹œë„
        if self.backend is None and WHISPER_AVAILABLE:
            self._try_whisper()
        
        # ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬
        if self.backend is None:
            raise RuntimeError(
                "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ë‘ ë°±ì—”ë“œ ëª¨ë‘ ì‹¤íŒ¨\n\n"
                "ğŸ”§ ìš´ì˜ì„œë²„ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                "1. faster-whisper ëª¨ë¸:\n"
                f"   ê²½ë¡œ: {self.model_path}\n"
                f"   í•„ìˆ˜: {self.model_path}/ctranslate2_model/model.bin\n\n"
                "2. OpenAI Whisper ëª¨ë¸ (large-v3-turbo):\n"
                "   ê²½ë¡œ: /app/models (Docker ë§ˆìš´íŠ¸ í¬ì¸íŠ¸)\n"
                "   êµ¬ì¡°: pytorch_model.bin, config.json, tokenizer.json\n\n"
                "3. ëª¨ë¸ ë°°í¬ ë°©ë²•:\n"
                "   a) ë¡œì»¬ì—ì„œ ë‹¤ìš´ë¡œë“œ: python download_model_hf.py\n"
                "   b) ìš´ì˜ì„œë²„ë¡œ ë³µì‚¬: rsync -av models/ server:/models/\n"
                "   c) Docker ì‹¤í–‰: docker run -v /models:/app/models stt-engine"
            )
    
    def _try_faster_whisper(self):
        """faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©, ìƒì„¸ ì§„ë‹¨ í¬í•¨)"""
        try:
            print(f"ğŸ”„ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device}, compute: {self.compute_type})")
            
            # ëª¨ë¸ êµ¬ì¡° ìƒì„¸ ì§„ë‹¨
            diagnosis = diagnose_faster_whisper_model(self.model_path)
            
            if not diagnosis['valid']:
                print(f"\n   âŒ ëª¨ë¸ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨:")
                for error in diagnosis['errors']:
                    print(f"      - {error}")
                
                # CTranslate2 ë³€í™˜ ê°€ì´ë“œ
                if "tokenizer.json" in str(diagnosis['errors']):
                    print(f"\n   ğŸ’¡ CTranslate2 ë³€í™˜ ì •ë³´:")
                    print(f"      OpenAI Whisperì˜ tokenizer.jsonì€ CTranslate2ë¡œ ë³€í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print(f"      ëŒ€ì‹  ë‹¤ìŒ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
                    print(f"      - vocabulary.json")
                    print(f"      - tokens.json")
                    print(f"      - tokenizer.json (ì›ë³¸ ë³´ì¡´ëœ ê²½ìš°)")
                
                return
            
            # ê²½ê³  í™•ì¸
            if diagnosis['warnings']:
                print(f"\n   âš ï¸  ì£¼ì˜ì‚¬í•­:")
                for warning in diagnosis['warnings']:
                    print(f"      - {warning}")
            
            # íŒŒì¼ ëª©ë¡ ì¶œë ¥
            if diagnosis['files']['list']:
                print(f"\n   ğŸ“‚ CTranslate2 ëª¨ë¸ íŒŒì¼ ({diagnosis['files']['total_count']}ê°œ):")
                for file_info in diagnosis['files']['list'][:10]:
                    print(f"      âœ“ {file_info['name']} ({file_info['size_kb']:.1f}KB)")
                if len(diagnosis['files']['list']) > 10:
                    print(f"      ... ì™¸ {len(diagnosis['files']['list']) - 10}ê°œ")
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            print(f"\n   ğŸ“¦ faster-whisper WhisperModel ë¡œë“œ ì¤‘...")
            self.model = WhisperModel(
                self.model_path,
                device=self.device,
                compute_type=self.compute_type,
                num_workers=4,
                cpu_threads=4,
                download_root=None,
                local_files_only=True
            )
            
            self.backend = "faster-whisper"
            print(f"âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
        except FileNotFoundError as e:
            print(f"\n   âŒ faster-whisper: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"      ì—ëŸ¬: {e}")
            print(f"      ê²½ë¡œ: {self.model_path}")
            print(f"\n   ğŸ’¡ í•´ê²° ë°©ë²•:")
            print(f"      1. download_model_hf.py ì‹¤í–‰ ìƒíƒœ í™•ì¸")
            print(f"      2. CTranslate2 ë³€í™˜ ì™„ë£Œ ì—¬ë¶€ í™•ì¸")
            print(f"      3. {self.model_path}/ctranslate2_model/model.bin íŒŒì¼ í¬ê¸° í™•ì¸ (100MB ì´ìƒ)")
        except Exception as e:
            error_str = str(e)
            print(f"\n   âŒ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
            print(f"      ë©”ì‹œì§€: {error_str[:200]}")
            
            # ì•Œë ¤ì§„ ì—ëŸ¬ ì§„ë‹¨
            if "vocabulary" in error_str.lower() or "token" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: í† í¬ë‚˜ì´ì €/ì–´íœ˜ ì˜¤ë¥˜")
                print(f"      - CTranslate2 ë³€í™˜ì´ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
                print(f"      - í•„ìš”í•œ íŒŒì¼: vocabulary.json, tokens.json ë“±")
                print(f"      - download_model_hf.pyì˜ CTranslate2 ë³€í™˜ ë¡œê·¸ í™•ì¸")
            elif "model.bin" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: model.bin ë¡œë“œ ì˜¤ë¥˜")
                print(f"      - model.bin íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ê°€ëŠ¥ì„±")
                print(f"      - CTranslate2 ë³€í™˜ ì¬ì‹¤í–‰ í•„ìš”")
            elif "not found" in error_str.lower() or "no such file" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜")
                print(f"      - ëª¨ë¸ ê²½ë¡œ í™•ì¸: {self.model_path}")
                print(f"      - ctranslate2_model í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸")
            else:
                print(f"\n   ğŸ’¡ ìƒì„¸ ì§„ë‹¨ì„ ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
                print(f"      1. {self.model_path}/ctranslate2_model/ í´ë”")
                print(f"      2. model.bin íŒŒì¼ (100MB ì´ìƒ)")
                print(f"      3. config.json íŒŒì¼")
                print(f"      4. vocabulary.json ë˜ëŠ” tokens.json íŒŒì¼")

    
    def _try_whisper(self):
        """
        OpenAI Whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ë¡œì»¬ ê²½ë¡œë§Œ ì‚¬ìš©)
        
        ìš´ì˜ì„œë²„ ë°°í¬:
        - ëª¨ë¸ì€ ë³„ë„ ë³¼ë¥¨ìœ¼ë¡œ ê´€ë¦¬ (Dockerì™€ ë¶„ë¦¬)
        - ì‹¤í–‰: docker run -v /models/large-v3-turbo:/app/models
        - ëª¨ë¸ íŒŒì¼ì€ Docker ì´ë¯¸ì§€ì— í¬í•¨ë˜ì§€ ì•ŠìŒ
        """
        try:
            print(f"ğŸ”„ OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device})")
            
            model_path = Path(self.model_path)
            
            # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸
            if not model_path.exists():
                print(f"   âš ï¸  ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
                print(f"   ğŸ’¡ í™•ì¸ì‚¬í•­:")
                print(f"      1. ìš´ì˜ì„œë²„ì— ëª¨ë¸ì´ ìˆëŠ”ê°€?")
                print(f"      2. Docker ì‹¤í–‰ ì‹œ -v ì˜µì…˜ìœ¼ë¡œ ë§ˆìš´íŠ¸í–ˆëŠ”ê°€?")
                print(f"         ì˜ˆ: docker run -v /models/large-v3-turbo:/app/models ...")
                return
            
            # ë¡œì»¬ ê²½ë¡œì—ì„œ PyTorch ëª¨ë¸ ì§ì ‘ ë¡œë“œ
            print(f"   ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ: {model_path}")
            
            self.model = whisper.load_model(
                str(model_path),
                device=self.device,
                in_memory=False,
                download_root=None  # ğŸ”’ ë‹¤ìš´ë¡œë“œ ë°©ì§€
            )
            
            self.backend = "whisper"
            print(f"âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
        except FileNotFoundError as e:
            print(f"âŒ OpenAI Whisper: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - {e}")
        except Exception as e:
            print(f"âŒ OpenAI Whisper ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def _is_cuda_available() -> bool:
        """CUDA ê°€ìš©ì„± í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def transcribe(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
            **kwargs: ì¶”ê°€ ì˜µì…˜
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            
            # ë°±ì—”ë“œë³„ ì²˜ë¦¬
            if self.backend == "faster-whisper":
                return self._transcribe_faster_whisper(audio_path, language, **kwargs)
            elif self.backend == "whisper":
                return self._transcribe_whisper(audio_path, language, **kwargs)
            else:
                raise RuntimeError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°±ì—”ë“œ: {self.backend}")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "audio_path": audio_path
            }
    
    def _transcribe_faster_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """faster-whisperë¡œ ë³€í™˜"""
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            beam_size=kwargs.get("beam_size", 5),
            best_of=kwargs.get("best_of", 5),
            patience=kwargs.get("patience", 1),
            temperature=kwargs.get("temperature", 0),
            verbose=False
        )
        
        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
        text = "".join([segment.text for segment in segments])
        detected_language = info.language if info else language or "unknown"
        
        return {
            "success": True,
            "text": text.strip(),
            "audio_path": audio_path,
            "language": detected_language,
            "duration": info.duration if info else None,
            "backend": "faster-whisper"
        }
    
    def _transcribe_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """OpenAI Whisperë¡œ ë³€í™˜"""
        result = self.model.transcribe(
            audio_path,
            language=language
        )
        
        text = result.get("text", "").strip()
        
        return {
            "success": True,
            "text": text,
            "audio_path": audio_path,
            "language": language or "unknown",
            "duration": None,
            "backend": "whisper"
        }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ë””ë²„ê¹…ìš©, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    ì°¸ê³ : FastAPI ì„œë¹„ìŠ¤ (api_server.py)ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•ŒëŠ”
         ì´ í•¨ìˆ˜ê°€ ì•„ë‹Œ WhisperSTT í´ë˜ìŠ¤ë¥¼ ì§ì ‘ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # STT ì´ˆê¸°í™”
    stt = WhisperSTT(
        model_path,
        device=device,
        compute_type="float16"
    )
    
    print(f"\nğŸ“Š ì‚¬ìš© ë°±ì—”ë“œ: {stt.backend}\n")
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg", "*.m4a")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
            if result.get("duration"):
                print(f"â±ï¸  ìŒì„± ê¸¸ì´: {result['duration']:.1f}ì´ˆ")
            print(f"ğŸ”§ ì‚¬ìš© ë°±ì—”ë“œ: {result.get('backend', 'unknown')}")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ğŸ”´ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


# ============================================================================
# ì£¼ì˜: ì´ íŒŒì¼ì€ api_server.pyì˜ FastAPI ì„œë¹„ìŠ¤ì—ì„œ importë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.
# api_server.py:
#   from stt_engine import WhisperSTT
#   stt = WhisperSTT(model_path=..., device=...)
#   result = stt.transcribe(audio_path)
#
# ë”°ë¼ì„œ ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
# ë§Œì•½ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:
#   python stt_engine.py  (ë‹¨, audio/ ë””ë ‰í† ë¦¬ì— ìŒì„± íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
# ============================================================================
