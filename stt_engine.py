#!/usr/bin/env python3
"""
STT ëª¨ë“ˆ - 3ê°€ì§€ ë°±ì—”ë“œ ìë™ ì„ íƒ

ìš°ì„ ìˆœìœ„:
1. faster-whisper + CTranslate2 (ê°€ì¥ ë¹ ë¦„, ê¶Œì¥)
2. transformers WhisperForConditionalGeneration (HF ëª¨ë¸ ì§ì ‘ ì§€ì›)
3. OpenAI Whisper (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›, ëŒ€ì²´ìš©)

ì§€ì› ëª¨ë¸ í˜•ì‹:
- CTranslate2: .tar.gz (ë³€í™˜ë¨)
- transformers: Hugging Face í˜•ì‹ (PyTorch/SafeTensors)
- OpenAI Whisper: ê³µì‹ ëª¨ë¸ëª…ë§Œ (tiny, base, small, medium, large)
"""

import os
from pathlib import Path
from typing import Optional, Dict
import tarfile
import logging
import json
import librosa
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì„¸ ê°€ì§€ ë°±ì—”ë“œ ì‹œë„
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

try:
    from transformers import WhisperProcessor, WhisperForConditionalGeneration
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import openai_whisper as whisper
    WHISPER_AVAILABLE = True
except ImportError:
    try:
        import whisper
        WHISPER_AVAILABLE = True
    except ImportError:
        WHISPER_AVAILABLE = False

if not (FASTER_WHISPER_AVAILABLE or TRANSFORMERS_AVAILABLE or WHISPER_AVAILABLE):
    raise ImportError(
        "ë‹¤ìŒ ì¤‘ ìµœì†Œ í•˜ë‚˜ì˜ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n"
        "  1. faster-whisper + ctranslate2 (ê¶Œì¥)\n"
        "  2. transformers (HF ëª¨ë¸ ì§€ì›)\n"
        "  3. openai-whisper / whisper (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›)"
    )


def diagnose_faster_whisper_model(model_path: str) -> dict:
    """
    faster-whisper ëª¨ë¸ ìƒì„¸ ì§„ë‹¨ (ë””ë²„ê¹…ìš©)
    
    CTranslate2 ëª¨ë¸ì€ ë‹¤ìŒ íŒŒì¼ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
    - model.bin (CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬)
    - config.json (ëª¨ë¸ ì„¤ì •)
    - vocabulary.json (ë˜ëŠ” tokens.json) - í† í¬ë‚˜ì´ì € ì •ë³´
    - shared_vocabulary.json (ì„ íƒì‚¬í•­)
    
    Returns:
        {
            'valid': bool,
            'errors': [list of errors],
            'warnings': [list of warnings],
            'files': {detailed file structure},
            'model_bin_size_mb': float
        }
    """
    model_dir = Path(model_path)
    ct_model_dir = model_dir / "ctranslate2_model"
    
    diagnosis = {
        'valid': True,
        'errors': [],
        'warnings': [],
        'files': {},
        'model_bin_size_mb': None
    }
    
    # 1. ctranslate2_model í´ë” ì¡´ì¬ í™•ì¸
    if not ct_model_dir.exists():
        diagnosis['errors'].append(f"ctranslate2_model í´ë” ì—†ìŒ: {ct_model_dir}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 2. ctranslate2_model ë‚´ ëª¨ë“  íŒŒì¼ ë‚˜ì—´
    try:
        ct_files = list(ct_model_dir.rglob("*"))
        diagnosis['files']['total_count'] = len(ct_files)
        diagnosis['files']['list'] = []
        
        for file_path in sorted(ct_files)[:30]:  # ì²˜ìŒ 30ê°œ
            if file_path.is_file():
                size_kb = file_path.stat().st_size / 1024
                diagnosis['files']['list'].append({
                    'name': file_path.name,
                    'relative_path': str(file_path.relative_to(ct_model_dir)),
                    'size_kb': size_kb
                })
    except Exception as e:
        diagnosis['errors'].append(f"íŒŒì¼ ë‚˜ì—´ ì‹¤íŒ¨: {e}")
        diagnosis['valid'] = False
        return diagnosis
    
    # 3. í•„ìˆ˜ íŒŒì¼ í™•ì¸ (CTranslate2 í¬ë§·)
    critical_files = {
        'model.bin': 'CTranslate2 ë³€í™˜ëœ ëª¨ë¸ ë°”ì´ë„ˆë¦¬',
        'config.json': 'Whisper ëª¨ë¸ ì„¤ì •'
    }
    
    for file_name, description in critical_files.items():
        file_path = ct_model_dir / file_name
        if not file_path.exists():
            diagnosis['errors'].append(f"ëˆ„ë½: {file_name} ({description})")
            diagnosis['valid'] = False
        else:
            size_kb = file_path.stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{file_name}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB (ì†ìƒ ê°€ëŠ¥ì„±)")
    
    # 4. í† í¬ë‚˜ì´ì € íŒŒì¼ í™•ì¸ (vocabulary.json ë˜ëŠ” tokens.json)
    # CTranslate2ëŠ” OpenAI Whisperì˜ tokenizer.jsonì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    vocab_files = ['vocabulary.json', 'tokens.json', 'tokenizer.json']
    has_vocab = False
    for vocab_file in vocab_files:
        if (ct_model_dir / vocab_file).exists():
            has_vocab = True
            size_kb = (ct_model_dir / vocab_file).stat().st_size / 1024
            if size_kb < 10:
                diagnosis['warnings'].append(f"{vocab_file}ì´ ë„ˆë¬´ ì‘ìŒ: {size_kb:.1f}KB")
            break
    
    if not has_vocab:
        diagnosis['warnings'].append(f"í† í¬ë‚˜ì´ì € íŒŒì¼ ì—†ìŒ (vocabulary.json, tokens.json, tokenizer.json ì¤‘ í•˜ë‚˜ í•„ìš”)")
    
    # 5. model.bin ìƒì„¸ ê²€ì‚¬
    model_bin = ct_model_dir / "model.bin"
    if model_bin.exists():
        size_bytes = model_bin.stat().st_size
        size_mb = size_bytes / (1024 * 1024)
        diagnosis['model_bin_size_mb'] = size_mb
        
        if size_mb < 100:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° ì‘ìŒ: {size_mb:.1f}MB (ì†ìƒ ë˜ëŠ” ë³€í™˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)")
            diagnosis['valid'] = False
        
        if size_mb > 5000:
            diagnosis['warnings'].append(f"model.binì´ ë§¤ìš° í¼: {size_mb:.1f}MB (ì–‘ìí™” í™•ì¸ í•„ìš”)")
    
    return diagnosis


def validate_faster_whisper_model(model_path: str) -> bool:
    """
    faster-whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦
    diagnose_faster_whisper_modelì˜ ê°„ë‹¨í•œ ë˜í¼
    """
    diagnosis = diagnose_faster_whisper_model(model_path)
    
    print(f"   ğŸ“‚ faster-whisper ëª¨ë¸ ê²€ì¦: {model_path}")
    
    if diagnosis['files']['total_count'] > 0:
        print(f"   âœ“ ctranslate2_model í´ë” ìˆìŒ ({diagnosis['files']['total_count']}ê°œ íŒŒì¼)")
    
    if diagnosis['model_bin_size_mb']:
        print(f"   âœ“ model.bin: {diagnosis['model_bin_size_mb']:.1f}MB")
    
    for warning in diagnosis['warnings']:
        print(f"   âš ï¸  {warning}")
    
    for error in diagnosis['errors']:
        print(f"   âŒ {error}")
    
    return diagnosis['valid']


def validate_whisper_model(model_path: str) -> bool:
    """
    OpenAI Whisper ëª¨ë¸ ìœ íš¨ì„± ê²€ì¦ (PyTorch ëª¨ë¸ í˜•ì‹)
    
    ì£¼ì˜: OpenAI WhisperëŠ” ê³µì‹ì ìœ¼ë¡œ ë‹¤ìŒ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:
    - tiny, base, small, medium, large
    
    "large-v3", "large-v3-turbo" ê°™ì€ ë³€í˜•ì€ huggingfaceì—ì„œë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ
    ìš´ì˜ì„œë²„ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.
    
    Args:
        model_path: ëª¨ë¸ í´ë” ê²½ë¡œ (ì°¸ê³ ìš©)
    
    Returns:
        True if ìœ íš¨, False otherwise
    """
    model_dir = Path(model_path)
    
    if not model_dir.exists():
        print(f"   âš ï¸  ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}")
        return False
    
    # pytorch_model.bin ë˜ëŠ” model.safetensors ì¤‘ í•˜ë‚˜ í•„ìš”
    has_pytorch = (model_dir / "pytorch_model.bin").exists()
    has_safetensors = (model_dir / "model.safetensors").exists()
    
    if not (has_pytorch or has_safetensors):
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”")
        return False
    
    # config.json, tokens.json í•„ìˆ˜
    required_files = ["config.json", "tokenizer.json"]
    missing_files = []
    
    for file in required_files:
        file_path = model_dir / file
        if not file_path.exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"   âš ï¸  Whisper ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {', '.join(missing_files)}")
        return False
    
    print(f"   âœ“ Whisper ëª¨ë¸ êµ¬ì¡° ìœ íš¨")
    return True


def auto_extract_model_if_needed(models_dir: str = "models") -> Path:
    """
    í•„ìš”ì‹œ ëª¨ë¸ ìë™ ì••ì¶• í•´ì œ
    
    Args:
        models_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ (ì˜ˆ: "models")
    
    Returns:
        ëª¨ë¸ í´ë” ê²½ë¡œ (models/openai_whisper-large-v3-turbo)
    
    Raises:
        RuntimeError: ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨
        FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
    """
    models_path = Path(models_dir)
    model_folder = models_path / "openai_whisper-large-v3-turbo"
    tar_file = models_path / "whisper-model.tar.gz"
    
    # ì´ë¯¸ í•´ì œë˜ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if model_folder.exists():
        return model_folder
    
    # ì••ì¶• íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ í•´ì œ
    if tar_file.exists():
        print("ğŸ“¦ ëª¨ë¸ ì••ì¶• íŒŒì¼ ê°ì§€, ìë™ í•´ì œ ì¤‘...")
        try:
            with tarfile.open(tar_file, "r:gz") as tar:
                # ì•ˆì „ì„± ê²€ì‚¬: tar ë©¤ë²„ ê²€ì¦
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        raise RuntimeError(f"ë³´ì•ˆ ìœ„í—˜: ì˜ëª»ëœ ê²½ë¡œ {member.name}")
                tar.extractall(path=models_path)
            print("âœ… ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ")
            
            # ì••ì¶• íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            tar_file.unlink()
            print("ğŸ—‘ï¸  ì••ì¶• íŒŒì¼ ì‚­ì œ")
            
            return model_folder
        except tarfile.TarError as e:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ tar íŒŒì¼: {e}")
            raise RuntimeError(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}") from e
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê²½ë¡œ ë°˜í™˜ (ë‹¤ìš´ë¡œë“œ í”„ë¡¬í”„íŠ¸)
    return model_folder


class WhisperSTT:
    """faster-whisper / OpenAI Whisper ìë™ ì„ íƒ STT í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, device: str = "cpu", compute_type: str = "float16"):
        """
        Whisper STT ì´ˆê¸°í™”
        
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: "models")
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu', 'cuda', ë˜ëŠ” 'auto')
            compute_type: ê³„ì‚° íƒ€ì… ('float32', 'float16', 'int8')
        
        Raises:
            FileNotFoundError: ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ
            RuntimeError: ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨
        
        ì§€ì› ëª¨ë¸ í˜•ì‹:
        - CTranslate2: ctranslate2_model/ í´ë” (model.bin)
        - transformers: PyTorch/SafeTensors (pytorch_model.bin ë˜ëŠ” model.safetensors)
        - OpenAI Whisper: ê³µì‹ ëª¨ë¸ëª…ë§Œ (tiny, base, small, medium, large)
        """
        # ëª¨ë¸ì´ ì••ì¶•ë˜ì–´ ìˆìœ¼ë©´ ìë™ í•´ì œ
        models_dir = str(Path(model_path).parent)
        self.model_path = str(auto_extract_model_if_needed(models_dir))
        
        # ëª¨ë¸ ê²½ë¡œ ìœ íš¨ì„± ìµœì¢… í™•ì¸
        if not Path(self.model_path).exists():
            raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                                  f"ì•„ë˜ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
                                  f"1. ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ê°€? (download_model_hf.py ì‹¤í–‰)\n"
                                  f"2. ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ê°€? (ê¸°ë³¸ê°’: models/openai_whisper-large-v3-turbo)\n"
                                  f"3. ìš´ì˜ì„œë²„ì¸ê°€? (ì˜¤í”„ë¼ì¸ ë°°í¬ì¸ ê²½ìš° ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ì‹œì¼œì•¼ í•¨)")
        
        self.device = device if device != "auto" else ("cuda" if self._is_cuda_available() else "cpu")
        self.compute_type = compute_type
        self.backend = None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì¶”ì ìš© í”Œë˜ê·¸
        self.faster_whisper_available = False
        self.transformers_available = False
        self.whisper_available = False
        
        print(f"\nğŸ“Š STT ëª¨ë¸ ë¡œë“œ ì‹œì‘")
        print(f"   ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ:")
        print(f"     - faster-whisper: {FASTER_WHISPER_AVAILABLE}")
        print(f"     - transformers: {TRANSFORMERS_AVAILABLE}")
        print(f"     - openai-whisper: {WHISPER_AVAILABLE}\n")
        
        # 1ï¸âƒ£ faster-whisper ì‹œë„ (CTranslate2 ëª¨ë¸, ê°€ì¥ ë¹ ë¦„)
        if FASTER_WHISPER_AVAILABLE:
            self._try_faster_whisper()
        
        # 2ï¸âƒ£ transformers ì‹œë„ (PyTorch/HF ëª¨ë¸)
        if self.backend is None and TRANSFORMERS_AVAILABLE:
            self._try_transformers()
        
        # 3ï¸âƒ£ OpenAI Whisper ì‹œë„ (ê³µì‹ ëª¨ë¸ë§Œ)
        if self.backend is None and WHISPER_AVAILABLE:
            self._try_whisper()
        
        # ëª¨ë‘ ì‹¤íŒ¨
        if self.backend is None:
            available = []
            if FASTER_WHISPER_AVAILABLE:
                available.append("faster-whisper")
            if TRANSFORMERS_AVAILABLE:
                available.append("transformers")
            if WHISPER_AVAILABLE:
                available.append("openai-whisper")
            
            raise RuntimeError(
                f"âŒ ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨ (ì‚¬ìš© ê°€ëŠ¥: {', '.join(available)})\n\n"
                "ğŸ”§ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"1. ëª¨ë¸ ê²½ë¡œ: {self.model_path}\n"
                f"2. í•„ìš”í•œ íŒŒì¼:\n"
                f"   - CTranslate2: {self.model_path}/ctranslate2_model/model.bin\n"
                f"   - PyTorch: {self.model_path}/pytorch_model.bin ë˜ëŠ” model.safetensors\n"
                f"3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: python download_model_hf.py\n"
                f"4. ë¡œì»¬ ìºì‹œì—ì„œ ë³µì‚¬: cp -r ~/.cache/huggingface/hub/models--openai--whisper-large-v3-turbo/snapshots/*/  {self.model_path}"
            )
        

        # faster-whisper ë¡œë“œ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬
        if self.backend is None:
            raise RuntimeError(
                "âŒ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨\n\n"
                "ğŸ”§ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n"
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"1. ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸:\n"
                f"   ê²½ë¡œ: {self.model_path}\n"
                f"   í•„ìˆ˜: {self.model_path}/ctranslate2_model/model.bin (1.5GB+)\n\n"
                f"2. CTranslate2 ë³€í™˜ ì™„ë£Œ í™•ì¸:\n"
                f"   ls -lh {self.model_path}/ctranslate2_model/\n"
                f"   model.bin (1.5GB), config.json (2.2KB), vocabulary.json\n\n"
                "3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë³€í™˜:\n"
                "   python download_model_hf.py  # ~30-45ë¶„\n\n"
                "4. Docker ë§ˆìš´íŠ¸ í™•ì¸ (ìš´ì˜ì„œë²„):\n"
                "   docker exec stt-engine ls -lh /app/models/ctranslate2_model/"
            )
    
    def _try_faster_whisper(self):
        """faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš©, ìƒì„¸ ì§„ë‹¨ í¬í•¨)"""
        try:
            logger.info(f"ğŸ”„ faster-whisper ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device}, compute: {self.compute_type})")
            
            # ëª¨ë¸ êµ¬ì¡° ìƒì„¸ ì§„ë‹¨
            diagnosis = diagnose_faster_whisper_model(self.model_path)
            
            if not diagnosis['valid']:
                print(f"\n   âŒ ëª¨ë¸ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨:")
                for error in diagnosis['errors']:
                    print(f"      - {error}")
                
                # CTranslate2 ë³€í™˜ ê°€ì´ë“œ
                if "tokenizer.json" in str(diagnosis['errors']):
                    print(f"\n   ğŸ’¡ CTranslate2 ë³€í™˜ ì •ë³´:")
                    print(f"      OpenAI Whisperì˜ tokenizer.jsonì€ CTranslate2ë¡œ ë³€í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    print(f"      ëŒ€ì‹  ë‹¤ìŒ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
                    print(f"      - vocabulary.json")
                    print(f"      - tokens.json")
                    print(f"      - tokenizer.json (ì›ë³¸ ë³´ì¡´ëœ ê²½ìš°)")
                
                return
            
            # ê²½ê³  í™•ì¸
            if diagnosis['warnings']:
                print(f"\n   âš ï¸  ì£¼ì˜ì‚¬í•­:")
                for warning in diagnosis['warnings']:
                    print(f"      - {warning}")
            
            # íŒŒì¼ ëª©ë¡ ì¶œë ¥
            if diagnosis['files']['list']:
                print(f"\n   ğŸ“‚ CTranslate2 ëª¨ë¸ íŒŒì¼ ({diagnosis['files']['total_count']}ê°œ):")
                for file_info in diagnosis['files']['list'][:10]:
                    print(f"      âœ“ {file_info['name']} ({file_info['size_kb']:.1f}KB)")
                if len(diagnosis['files']['list']) > 10:
                    print(f"      ... ì™¸ {len(diagnosis['files']['list']) - 10}ê°œ")
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„ - CTranslate2 ëª¨ë¸ ì„œë¸Œë””ë ‰í† ë¦¬ ì‚¬ìš©
            # faster-whisperê°€ tokenizer íŒŒì¼ì„ ì´ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ìŒ
            ct2_model_dir = Path(self.model_path) / "ctranslate2_model"
            
            print(f"\n   ğŸ“¦ faster-whisper WhisperModel ë¡œë“œ ì¤‘...")
            print(f"   ğŸ“ ëª¨ë¸ ê²½ë¡œ: {ct2_model_dir}")
            
            self.model = WhisperModel(
                str(ct2_model_dir),
                device=self.device,
                compute_type=self.compute_type,
                num_workers=4,
                cpu_threads=4,
                download_root=None,
                local_files_only=True
            )
            
            self.backend = self.model  # ì‹¤ì œ ëª¨ë¸ ê°ì²´ë¥¼ backendì— ì €ì¥
            # WhisperModel ê°ì²´ì— _backend_type ì†ì„± ì¶”ê°€
            if not hasattr(self.backend, '_backend_type'):
                self.backend._backend_type = 'faster-whisper'
            self.faster_whisper_available = True  # í”Œë˜ê·¸ ì„¤ì •
            logger.info(f"âœ… faster-whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ faster-whisper: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", exc_info=True)
            logger.error(f"   ê²½ë¡œ: {self.model_path}")
            logger.error(f"   í•´ê²°: download_model_hf.py ë° CTranslate2 ë³€í™˜ í™•ì¸")
        except Exception as e:
            error_str = str(e)
            logger.error(f"âŒ faster-whisper ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}", exc_info=True)
            logger.error(f"   ë©”ì‹œì§€: {error_str[:200]}")
            
            # ì•Œë ¤ì§„ ì—ëŸ¬ ì§„ë‹¨
            if "vocabulary" in error_str.lower() or "token" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: í† í¬ë‚˜ì´ì €/ì–´íœ˜ ì˜¤ë¥˜")
                print(f"      - CTranslate2 ë³€í™˜ì´ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ")
                print(f"      - í•„ìš”í•œ íŒŒì¼: vocabulary.json, tokens.json ë“±")
                print(f"      - download_model_hf.pyì˜ CTranslate2 ë³€í™˜ ë¡œê·¸ í™•ì¸")
            elif "model.bin" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: model.bin ë¡œë“œ ì˜¤ë¥˜")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 1: model.bin íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¶ˆì™„ì „í•¨")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 2: CTranslate2 ë³€í™˜ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŒ")
                print(f"      - ê°€ëŠ¥í•œ ì›ì¸ 3: config.jsonì´ ì†ìƒë¨ (2.2KB ì´ìƒì¸ì§€ í™•ì¸)")
                print(f"\n   ğŸ”§ í•´ê²° ë°©ë²•:")
                print(f"      1. EC2ì—ì„œ ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ:")
                print(f"         rm -rf models/openai_whisper-large-v3-turbo")
                print(f"         python3 download_model_hf.py")
                print(f"      2. Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ:")
                print(f"         bash scripts/build-server-image.sh")
                print(f"      3. ì»¨í…Œì´ë„ˆ ì¬ì‹¤í–‰ (ëª¨ë¸ ë§ˆìš´íŠ¸)")
                print(f"         docker run -v $(pwd)/models:/app/models ...")
            elif "not found" in error_str.lower() or "no such file" in error_str.lower():
                print(f"\n   ğŸ’¡ ë¶„ì„: íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜")
                print(f"      - ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
                print(f"      - ctranslate2_model í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸")
                print(f"      - model.bin íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸")
                print(f"\n   ğŸ”§ í•´ê²° ë°©ë²•:")
                print(f"      1. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í™•ì¸:")
                print(f"         docker exec -it <container> ls -lh /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/")
                print(f"      2. ë§ˆìš´íŠ¸ í™•ì¸:")
                print(f"         docker inspect <container> | grep -A 5 'Mounts'")
            else:
                print(f"\n   ğŸ’¡ ìƒì„¸ ì§„ë‹¨ì„ ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
                print(f"      1. {self.model_path}/ctranslate2_model/ í´ë” ì¡´ì¬")
                print(f"      2. model.bin íŒŒì¼ í¬ê¸° (100MB ì´ìƒ, ì¼ë°˜ì ìœ¼ë¡œ 1.5GB)")
                print(f"      3. config.json íŒŒì¼ í¬ê¸° (2.2KB ì´ìƒ)")
                print(f"      4. vocabulary.json íŒŒì¼ ì¡´ì¬")
                print(f"\n   ğŸ“‹ ë””ë²„ê·¸ ëª…ë ¹ì–´:")
                print(f"      docker exec -it <container> bash")
                print(f"      ls -lh /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/")
                print(f"      file /app/models/openai_whisper-large-v3-turbo/ctranslate2_model/config.json")

    
    def _try_transformers(self):
        """
        transformers WhisperForConditionalGenerationìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„
        
        íŠ¹ì§•:
        - Hugging Face ëª¨ë¸ ì§ì ‘ ì§€ì› (PyTorch/SafeTensors)
        - large-v3-turbo í¬í•¨ ëª¨ë“  HF Whisper ëª¨ë¸ ì§€ì›
        - GPU ê°€ì† ê°€ëŠ¥
        - ë” ëŠë¦¼ (faster-whisper ëŒ€ë¹„ 2-3ë°°)
        
        ì§€ì› íŒŒì¼:
        - pytorch_model.bin (PyTorch í˜•ì‹)
        - model.safetensors (SafeTensors í˜•ì‹, ë” ë¹ ë¦„)
        - config.json, tokenizer.json ë“±
        """
        try:
            print(f"ğŸ”„ transformersë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„... (ë””ë°”ì´ìŠ¤: {self.device})")
            
            from transformers import WhisperProcessor, WhisperForConditionalGeneration
            import torch
            
            model_path = Path(self.model_path)
            
            # PyTorch ëª¨ë¸ íŒŒì¼ í™•ì¸
            has_pytorch = (model_path / "pytorch_model.bin").exists()
            has_safetensors = (model_path / "model.safetensors").exists()
            
            if not (has_pytorch or has_safetensors):
                print(f"   âš ï¸  PyTorch ëª¨ë¸ íŒŒì¼ ì—†ìŒ (pytorch_model.bin ë˜ëŠ” model.safetensors í•„ìš”)")
                return
            
            # ë¡œì»¬ ìºì‹œì—ì„œ ë¡œë“œ (HF í—ˆë¸Œ ì ‘ê·¼ ë°©ì§€)
            processor = WhisperProcessor.from_pretrained(str(model_path), local_files_only=True)
            model = WhisperForConditionalGeneration.from_pretrained(str(model_path), local_files_only=True)
            
            # GPUë¡œ ì´ë™
            if self.device == "cuda" and torch.cuda.is_available():
                model = model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œ
            model.eval()
            
            self.backend = type('TransformersBackend', (), {
                'processor': processor,
                'model': model,
                'device': self.device,
                'transcribe': self._transcribe_with_transformers,
                '_backend_type': 'transformers'  # ë°±ì—”ë“œ íƒ€ì… ì‹ë³„ì ì¶”ê°€
            })()
            self.transformers_available = True  # í”Œë˜ê·¸ ì„¤ì •
            
            print(f"   âœ… transformers ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            print(f"      íƒ€ì…: WhisperForConditionalGeneration")
            print(f"      íŒŒì¼: {'SafeTensors' if has_safetensors else 'PyTorch'}")
            
        except FileNotFoundError as e:
            print(f"   âš ï¸  ë¡œì»¬ ìºì‹œ ì‹¤íŒ¨: {e}")
            print(f"      ì‹œë„: Hugging Face í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œ...")
            
            try:
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                import torch
                
                processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3-turbo")
                model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3-turbo")
                
                if self.device == "cuda" and torch.cuda.is_available():
                    model = model.to(self.device)
                
                model.eval()
                
                self.backend = type('TransformersBackend', (), {
                    'processor': processor,
                    'model': model,
                    'device': self.device,
                    'transcribe': self._transcribe_with_transformers,
                    '_backend_type': 'transformers'  # ë°±ì—”ë“œ íƒ€ì… ì‹ë³„ì ì¶”ê°€
                })()
                self.transformers_available = True  # í”Œë˜ê·¸ ì„¤ì •
                
                print(f"   âœ… HF í—ˆë¸Œì—ì„œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
                
            except Exception as e2:
                print(f"   âŒ HF í—ˆë¸Œ ë¡œë“œ ì‹¤íŒ¨: {type(e2).__name__}")
                
        except Exception as e:
            print(f"   âŒ transformers ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
            print(f"      ì—ëŸ¬: {str(e)[:150]}")
    
    def _transcribe_with_transformers(self, audio_path: str, language: Optional[str] = None) -> Dict:
        """
        transformersë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ (ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬)
        
        WhisperëŠ” ìµœëŒ€ 30ì´ˆ ìŒì„±ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ,
        ê¸´ ìŒì„±ì€ 30ì´ˆ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬ í›„ ê²°í•©í•©ë‹ˆë‹¤.
        """
        import librosa
        import torch
        import numpy as np
        import gc
        
        logger.info(f"[transformers] ë³€í™˜ ì‹œì‘ (íŒŒì¼: {Path(audio_path).name})")
        
        try:
            from stt_utils import check_memory_available, check_audio_file
            
            # 1. íŒŒì¼ ê²€ì¦
            logger.debug(f"[transformers] íŒŒì¼ ê²€ì¦ ì¤‘...")
            file_check = check_audio_file(audio_path, logger=logger)
            if not file_check['valid']:
                error_msg = f"transformers transcription failed: íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨ - {file_check['errors'][0]}"
                logger.error(f"âŒ {error_msg}")
                return {
                    "text": "",
                    "error": error_msg,
                    "backend": "transformers"
                }
            
            logger.info(f"âœ“ íŒŒì¼ ê²€ì¦ ì™„ë£Œ (ê¸¸ì´: {file_check['duration_sec']:.1f}ì´ˆ)")
            
            # ê²½ê³  ì¶œë ¥
            for warning in file_check['warnings']:
                logger.warning(f"âš ï¸  {warning}")
            
            # 2. ë©”ëª¨ë¦¬ í™•ì¸ (ëª¨ë¸ í¬ê¸° ì•½ 3GB + ì²˜ë¦¬ìš© 1GB = 4GB)
            logger.debug(f"[transformers] ë©”ëª¨ë¦¬ í™•ì¸ ì¤‘...")
            memory_check = check_memory_available(required_mb=4000, logger=logger)
            if memory_check['critical']:
                error_msg = f"transformers transcription failed: ë©”ëª¨ë¦¬ ë¶€ì¡± - {memory_check['message']}"
                logger.error(f"âŒ {error_msg}")
                return {
                    "text": "",
                    "error": error_msg,
                    "backend": "transformers",
                    "memory_info": memory_check
                }
            
            logger.info(f"âœ“ ë©”ëª¨ë¦¬ í™•ì¸ ì™„ë£Œ (ì‚¬ìš© ê°€ëŠ¥: {memory_check['available_mb']:.0f}MB)")
            
            # 3. ìŒì„± ë¡œë“œ
            logger.info(f"[transformers] ìŒì„± íŒŒì¼ ë¡œë“œ ì¤‘: {Path(audio_path).name}")
            try:
                audio, sr = librosa.load(audio_path, sr=16000)
                duration_seconds = len(audio) / sr
                logger.info(f"âœ“ ìŒì„± ë¡œë“œ ì™„ë£Œ (ê¸¸ì´: {duration_seconds:.1f}ì´ˆ, ìƒ˜í”Œ: {len(audio):,}, SR: {sr}Hz)")
            except ModuleNotFoundError as e:
                error_msg = f"transformers transcription failed: ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ - {type(e).__name__}: {str(e)[:100]}"
                logger.error(f"âŒ {error_msg}", exc_info=True)
                return {
                    "text": "",
                    "error": error_msg,
                    "backend": "transformers"
                }
            except MemoryError as e:
                error_msg = f"transformers transcription failed: ë©”ëª¨ë¦¬ ë¶€ì¡± - ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨"
                logger.error(f"âŒ {error_msg}", exc_info=True)
                return {
                    "text": "",
                    "error": error_msg,
                    "backend": "transformers"
                }
            except Exception as e:
                error_msg = f"transformers transcription failed: ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ - {type(e).__name__}: {str(e)[:100]}"
                logger.error(f"âŒ {error_msg}", exc_info=True)
                return {
                    "text": "",
                    "error": error_msg,
                    "backend": "transformers"
                }
            
            # Whisper ìµœëŒ€ ì…ë ¥: 30ì´ˆ (480,000 ìƒ˜í”Œ @ 16kHz)
            max_samples = 30 * sr  # 480,000 ìƒ˜í”Œ
            hop_length = max_samples // 2  # 15ì´ˆ ì˜¤ë²„ë©
            
            all_texts = []
            start_idx = 0
            segment_idx = 0
            total_segments = (len(audio) + hop_length - 1) // hop_length
            
            logger.info(f"[transformers] ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹œì‘ (ì´ {total_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
            
            while start_idx < len(audio):
                try:
                    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                    end_idx = min(start_idx + max_samples, len(audio))
                    segment = audio[start_idx:end_idx]
                    segment_duration = len(segment) / sr
                    
                    logger.debug(f"[transformers] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx+1}/{total_segments}: {start_idx//sr:.1f}~{end_idx//sr:.1f}ì´ˆ ({segment_duration:.1f}ì´ˆ)")
                    
                    # í”„ë¡œì„¸ì‹± (ë©”ëª¨ë¦¬ ì²´í¬)
                    logger.debug(f"[transformers] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} í”„ë¡œì„¸ì‹± ì¤‘...")
                    try:
                        input_features = self.backend.processor(
                            segment, 
                            sampling_rate=16000, 
                            return_tensors="pt"
                        ).input_features
                        logger.debug(f"âœ“ í”„ë¡œì„¸ì‹± ì™„ë£Œ (input_features shape: {input_features.shape})")
                    except MemoryError:
                        error_msg = f"transformers transcription failed: ë©”ëª¨ë¦¬ ë¶€ì¡± - ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ì²˜ë¦¬ ì¤‘"
                        logger.error(f"âŒ {error_msg}", exc_info=True)
                        return {
                            "text": "",
                            "error": error_msg,
                            "backend": "transformers",
                            "segment_failed": segment_idx,
                            "partial_text": " ".join(all_texts) if all_texts else ""
                        }
                    except Exception as e:
                        error_msg = f"transformers transcription failed: í”„ë¡œì„¸ì‹± ì‹¤íŒ¨ - {type(e).__name__}: {str(e)[:100]}"
                        logger.error(f"âŒ {error_msg}", exc_info=True)
                        return {
                            "text": "",
                            "error": error_msg,
                            "backend": "transformers",
                            "segment_failed": segment_idx
                        }
                    
                    # ëª¨ë¸ì˜ dtypeì— ë§ì¶”ê¸° (float32 â†’ float16)
                    model_dtype = self.backend.model.dtype
                    input_features = input_features.to(model_dtype)
                    
                    if self.device == "cuda":
                        input_features = input_features.to(self.device)
                    
                    # ì¶”ë¡  (language ì§€ì •)
                    logger.debug(f"[transformers] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ì¶”ë¡  ì¤‘ (device: {self.device}, dtype: {model_dtype})...")
                    try:
                        with torch.no_grad():
                            predicted_ids = self.backend.model.generate(
                                input_features, 
                                language="ko"
                            )
                        logger.debug(f"âœ“ ì¶”ë¡  ì™„ë£Œ (predicted_ids shape: {predicted_ids.shape})")
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                            error_msg = f"transformers transcription failed: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ì¶”ë¡  ì¤‘"
                            logger.error(f"âŒ {error_msg}", exc_info=True)
                            return {
                                "text": "",
                                "error": error_msg,
                                "backend": "transformers",
                                "segment_failed": segment_idx,
                                "partial_text": " ".join(all_texts) if all_texts else "",
                                "suggestion": "CPU ëª¨ë“œë¡œ ì „í™˜í•˜ê±°ë‚˜ -e STT_DEVICE=cpu ì‚¬ìš©"
                            }
                        logger.error(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}", exc_info=True)
                        raise
                    except MemoryError:
                        error_msg = f"transformers transcription failed: ë©”ëª¨ë¦¬ ë¶€ì¡± - ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ì¶”ë¡  ì¤‘"
                        logger.error(f"âŒ {error_msg}", exc_info=True)
                        return {
                            "text": "",
                            "error": error_msg,
                            "backend": "transformers",
                            "segment_failed": segment_idx,
                            "partial_text": " ".join(all_texts) if all_texts else ""
                        }
                    
                    # ë””ì½”ë”©
                    logger.debug(f"[transformers] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ë””ì½”ë”© ì¤‘...")
                    transcription = self.backend.processor.batch_decode(
                        predicted_ids, 
                        skip_special_tokens=True
                    )
                    
                    text = transcription[0] if transcription else ""
                    if text.strip():
                        all_texts.append(text)
                        logger.info(f"[TRANSCRIBE] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx}: '{text[:60]}...'")
                    else:
                        logger.debug(f"[TRANSCRIBE] ì„¸ê·¸ë¨¼íŠ¸ {segment_idx}: (ë¬´ìŒ)")
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del input_features, predicted_ids
                    gc.collect()
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                    
                except Exception as e:
                    if "out of memory" not in str(e).lower():
                        logger.warning(f"âš ï¸  ì„¸ê·¸ë¨¼íŠ¸ {segment_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {type(e).__name__}: {str(e)[:100]}")
                    raise
                
                # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ (50% ì˜¤ë²„ë©)
                start_idx += hop_length
                segment_idx += 1
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            full_text = " ".join(all_texts)
            
            logger.info(f"[TRANSCRIBE] ì™„ë£Œ - {segment_idx}ê°œ ì„¸ê·¸ë¨¼íŠ¸, ì´ {duration_seconds:.1f}ì´ˆ ì²˜ë¦¬")
            
            return {
                "text": full_text,
                "language": language or "ko",
                "backend": "transformers",
                "duration": duration_seconds,
                "segments_processed": segment_idx
            }
        
        except MemoryError as e:
            error_msg = f"transformers transcription failed: ë©”ëª¨ë¦¬ ë¶€ì¡±"
            logger.error(f"âŒ {error_msg}")
            return {
                "text": "",
                "error": error_msg,
                "backend": "transformers",
                "memory_error": True
            }
        except Exception as e:
            error_msg = f"transformers transcription failed: {type(e).__name__}: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error("Traceback:", exc_info=True)
            return {
                "text": "",
                "error": error_msg,
                "backend": "transformers"
            }

    
    def _try_whisper(self):
        """
        OpenAI Whisperë¡œ ëª¨ë¸ ë¡œë“œ ì‹œë„
        
        âš ï¸ ê³µì‹ ëª¨ë¸ë§Œ ì§€ì› (tiny, base, small, medium, large)
        large-v3-turboëŠ” OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹ˆë¯€ë¡œ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
        """
        try:
            print(f"ğŸ”„ OpenAI Whisper ì‹œë„... (ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›)")
            
            import whisper
            
            # OpenAI Whisper ì§€ì› ëª¨ë¸ í™•ì¸
            supported_models = whisper.available_models()
            
            model_path = Path(self.model_path)
            model_name = model_path.name
            
            # large-v3-turboëŠ” OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜
            if "turbo" in model_name.lower() or "v3-turbo" in model_name.lower():
                print(f"   âš ï¸  '{model_name}'ì€(ëŠ”) OpenAI ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜")
                print(f"       ì§€ì› ëª¨ë¸: {supported_models}")
                return
            
            # ê³µì‹ ëª¨ë¸ì¸ì§€ í™•ì¸
            if not any(m in model_name for m in ['tiny', 'base', 'small', 'medium', 'large']):
                print(f"   âš ï¸  '{model_name}'ì€(ëŠ”) ê³µì‹ ëª¨ë¸ì´ ì•„ë‹˜")
                print(f"       ì§€ì› ëª¨ë¸: {supported_models}")
                return
            
            # ë¡œë“œ ì‹œë„
            model = whisper.load_model("large", device=self.device)
            
            self.backend = type('WhisperBackend', (), {
                'model': model,
                'device': self.device,
                'transcribe': self._transcribe_with_whisper,
                '_backend_type': 'openai-whisper'  # ë°±ì—”ë“œ íƒ€ì… ì‹ë³„ì ì¶”ê°€
            })()
            self.whisper_available = True  # í”Œë˜ê·¸ ì„¤ì •
            
            print(f"   âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ! (large)")
            
        except Exception as e:
            print(f"   âŒ OpenAI Whisper ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}")
    
    def _transcribe_with_whisper(self, audio_path: str, language: Optional[str] = None) -> Dict:
        """OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹"""
        logger.info(f"[openai-whisper] ë³€í™˜ ì‹œì‘ (íŒŒì¼: {Path(audio_path).name})")
        
        try:
            logger.debug(f"[openai-whisper] ëª¨ë¸ í˜¸ì¶œ: transcribe(audio_path, language={language})")
            result = self.backend.model.transcribe(audio_path, language=language)
            
            logger.info(f"âœ“ openai-whisper ë³€í™˜ ì™„ë£Œ")
            
            text = result.get("text", "")
            detected_language = result.get("language", "unknown")
            logger.info(f"  ê²°ê³¼: {len(text)} ê¸€ì, ì–¸ì–´: {detected_language}")
            
            return {
                "success": True,
                "text": text.strip(),
                "language": detected_language,
                "backend": "openai-whisper"
            }
        except Exception as e:
            logger.error(f"âŒ openai-whisper ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {e}", exc_info=True)
            return {
                "success": False,
                "text": "",
                "error": f"openai-whisper ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {str(e)[:100]}",
                "backend": "openai-whisper"
            }
    
    @staticmethod
    def _explain_openai_whisper_limitations():
        """
        OpenAI Whisperì˜ ì•„í‚¤í…ì²˜ ì œí•œì‚¬í•­ ì„¤ëª…
        (ì´ ë©”ì„œë“œëŠ” ì°¸ê³  ëª©ì ìœ¼ë¡œë§Œ ìœ ì§€ë¨)
        
        âš ï¸ OpenAI Whisper.load_model()ì˜ ì œí•œì‚¬í•­:
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        - ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›: tiny, base, small, medium, large, turbo
        - ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì› ì•ˆí•¨: large-v3, large-v3-turbo ë“±
        - ë¡œì»¬ PyTorch ëª¨ë¸ ì§ì ‘ ë¡œë“œ ë¶ˆê°€
        - ëª¨ë¸ëª… hardcoding: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ëª¨ë¸ ê±°ë¶€
        
        ë”°ë¼ì„œ faster-whisper + CTranslate2ê°€ ìœ ì¼í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.
        """
        print("\nâŒ OpenAI Whisper ì§€ì› ë¶ˆê°€ (ì•„í‚¤í…ì²˜ ì œí•œ):")
        print("â”" * 60)
        print("OpenAI Whisper.load_model()ì€ ê³µì‹ ëª¨ë¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤:")
        print("  âœ“ tiny.en, tiny, base.en, base")
        print("  âœ“ small.en, small, medium.en, medium")
        print("  âœ“ large, turbo (ì¼ë¶€)")
        print("\nHugging Face ì»¤ìŠ¤í…€ ëª¨ë¸ ë¯¸ì§€ì›:")
        print("  âœ— large-v3, large-v3-turbo (ì´ í”„ë¡œì íŠ¸ì˜ ëª¨ë¸)")
        print("  âœ— ë¡œì»¬ PyTorch ëª¨ë¸ ì§ì ‘ ë¡œë“œ ë¶ˆê°€")
        print("\nğŸ’¡ ì†”ë£¨ì…˜: faster-whisper + CTranslate2 ë˜ëŠ” transformers ì‚¬ìš©")
        print("â”" * 60)

    
    @staticmethod
    def _is_cuda_available() -> bool:
        """CUDA ê°€ìš©ì„± í™•ì¸"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def reload_backend(self, backend: Optional[str] = None) -> str:
        """
        ë°±ì—”ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¬ë¡œë“œí•©ë‹ˆë‹¤.
        ê¸°ì¡´ ë°±ì—”ë“œë¥¼ ì–¸ë¡œë“œí•˜ê³  ìƒˆ ë°±ì—”ë“œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            backend: ë¡œë“œí•  ë°±ì—”ë“œ
                    - "faster-whisper": faster-whisper ì‚¬ìš©
                    - "transformers": transformers ì‚¬ìš©
                    - "openai-whisper": OpenAI Whisper ì‚¬ìš©
                    - None (ê¸°ë³¸ê°’): ê¸°ë³¸ ìˆœì„œëŒ€ë¡œ ìë™ ì„ íƒ (faster-whisper â†’ transformers â†’ openai-whisper)
        
        Returns:
            ë¡œë“œëœ ë°±ì—”ë“œ ì´ë¦„
            
        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ ìš”ì²­
            RuntimeError: ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨
        
        ì˜ˆì‹œ:
            stt = WhisperSTT(model_path)  # faster-whisper ë¡œë“œ
            
            # 100ê°œ íŒŒì¼ ì²˜ë¦¬
            for f in files[:100]:
                stt.transcribe(f)
            
            # transformersë¡œ ë³€ê²½
            stt.reload_backend("transformers")
            
            # ë‹¤ë¥¸ 100ê°œ íŒŒì¼ ì²˜ë¦¬
            for f in files[100:]:
                stt.transcribe(f)
        """
        import gc
        
        # ê¸°ì¡´ ë°±ì—”ë“œ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì •ë¦¬)
        if self.backend is not None:
            logger.info(f"ğŸ”„ ê¸°ì¡´ ë°±ì—”ë“œ ì–¸ë¡œë“œ ì¤‘...")
            try:
                # ë©”ëª¨ë¦¬ ëª…ì‹œì  í•´ì œ
                if hasattr(self.backend, 'model'):
                    del self.backend.model
                if hasattr(self.backend, 'processor'):
                    del self.backend.processor
                if hasattr(self.backend, '_transformers_model'):
                    del self.backend._transformers_model
                
                del self.backend
                self.backend = None
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                try:
                    import torch
                    torch.cuda.empty_cache()
                except:
                    pass
                
                logger.info(f"âœ“ ê¸°ì¡´ ë°±ì—”ë“œ ì–¸ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  ê¸°ì¡´ ë°±ì—”ë“œ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìƒˆ ë°±ì—”ë“œ ë¡œë“œ
        if backend:
            backend = backend.lower().strip()
            logger.info(f"ğŸ“Œ ìš”ì²­ ë°±ì—”ë“œ: {backend}")
            
            # ë°±ì—”ë“œ ë³„ì¹­ ì²˜ë¦¬
            backend_aliases = {
                "faster-whisper": "faster-whisper",
                "faster_whisper": "faster-whisper",
                "transformers": "transformers",
                "openai-whisper": "openai-whisper",
                "openai_whisper": "openai-whisper",
                "whisper": "openai-whisper"
            }
            
            backend_canonical = backend_aliases.get(backend)
            if not backend_canonical:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ: {backend}")
                logger.info(f"   ì§€ì› ë°±ì—”ë“œ: faster-whisper, transformers, openai-whisper")
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ: {backend}")
            
            # ìš”ì²­ëœ ë°±ì—”ë“œ ë¡œë“œ
            if backend_canonical == "faster-whisper" and FASTER_WHISPER_AVAILABLE:
                logger.info(f"â†’ faster-whisper ë¡œë“œ ì¤‘...")
                self._try_faster_whisper()
                if self.backend is not None:
                    logger.info(f"âœ… faster-whisper ë¡œë“œ ì„±ê³µ")
                    return "faster-whisper"
            
            elif backend_canonical == "transformers" and TRANSFORMERS_AVAILABLE:
                logger.info(f"â†’ transformers ë¡œë“œ ì¤‘...")
                self._try_transformers()
                if self.backend is not None:
                    logger.info(f"âœ… transformers ë¡œë“œ ì„±ê³µ")
                    return "transformers"
            
            elif backend_canonical == "openai-whisper" and WHISPER_AVAILABLE:
                logger.info(f"â†’ openai-whisper ë¡œë“œ ì¤‘...")
                self._try_whisper()
                if self.backend is not None:
                    logger.info(f"âœ… openai-whisper ë¡œë“œ ì„±ê³µ")
                    return "openai-whisper"
            
            # ìš”ì²­ëœ ë°±ì—”ë“œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŒ
            logger.error(f"âŒ '{backend_canonical}' ë°±ì—”ë“œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logger.error(f"   íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸: pip install {backend_canonical}")
            raise RuntimeError(f"'{backend_canonical}' ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨")
        
        else:
            # ê¸°ë³¸ ìˆœì„œëŒ€ë¡œ ìë™ ë¡œë“œ
            logger.info(f"â†’ ë°±ì—”ë“œ ìë™ ì„ íƒ (ê¸°ë³¸ ìˆœì„œ)")
            
            if FASTER_WHISPER_AVAILABLE:
                logger.info(f"â†’ faster-whisper ë¡œë“œ ì¤‘...")
                self._try_faster_whisper()
                if self.backend is not None:
                    logger.info(f"âœ… faster-whisper ë¡œë“œ ì„±ê³µ")
                    return "faster-whisper"
            
            if self.backend is None and TRANSFORMERS_AVAILABLE:
                logger.info(f"â†’ transformers ë¡œë“œ ì¤‘...")
                self._try_transformers()
                if self.backend is not None:
                    logger.info(f"âœ… transformers ë¡œë“œ ì„±ê³µ")
                    return "transformers"
            
            if self.backend is None and WHISPER_AVAILABLE:
                logger.info(f"â†’ openai-whisper ë¡œë“œ ì¤‘...")
                self._try_whisper()
                if self.backend is not None:
                    logger.info(f"âœ… openai-whisper ë¡œë“œ ì„±ê³µ")
                    return "openai-whisper"
            
            # ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨
            logger.error(f"âŒ ëª¨ë“  ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨")
            raise RuntimeError(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def transcribe(self, audio_path: str, language: Optional[str] = None, backend: Optional[str] = None, **kwargs) -> Dict:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        í˜„ì¬ ë¡œë“œëœ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ reload_backend()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            language: ìŒì„± ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
            backend: ë¬´ì‹œë¨ (í˜¸í™˜ì„± ìœ ì§€ìš©, ì‚¬ìš©í•˜ë ¤ë©´ reload_backend() í˜¸ì¶œ)
            **kwargs: ì¶”ê°€ ì˜µì…˜
        
        Returns:
            ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        ì˜ˆì‹œ:
            stt = WhisperSTT(model_path)  # faster-whisper ë¡œë“œ
            result = stt.transcribe("audio.wav", language="ko")
            
            # transformersë¡œ ë³€ê²½í•˜ë ¤ë©´
            stt.reload_backend("transformers")
            result = stt.transcribe("audio.wav", language="ko")
        """
        try:
            logger.info(f"ğŸ“‚ ìŒì„± íŒŒì¼ ë¡œë“œ ì‹œì‘: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not Path(audio_path).exists():
                logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {audio_path}")
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
            
            logger.info(f"âœ“ íŒŒì¼ ì¡´ì¬ í™•ì¸: {audio_path}")
            
            # í˜„ì¬ ë¡œë“œëœ ë°±ì—”ë“œ í™•ì¸
            backend_type = type(self.backend).__name__
            if hasattr(self.backend, '_backend_type'):
                backend_name = self.backend._backend_type
                logger.info(f"ğŸ”§ í˜„ì¬ ë¡œë“œëœ ë°±ì—”ë“œ: {backend_name}")
            else:
                backend_name = backend_type
                logger.info(f"ğŸ”§ í˜„ì¬ ë¡œë“œëœ ë°±ì—”ë“œ: {backend_type}")
            
            # backend íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì‹œ (reload_backend()ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨)
            if backend:
                logger.warning(f"âš ï¸  backend íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤. reload_backend()ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
            
            # í˜„ì¬ ë¡œë“œëœ ë°±ì—”ë“œë¡œ ë³€í™˜ ì‹œì‘
            if backend_name == "faster-whisper" or backend_type == 'WhisperModel':
                logger.info(f"â†’ faster-whisper ë°±ì—”ë“œë¡œ ë³€í™˜ ì‹œì‘")
                return self._transcribe_faster_whisper(audio_path, language, **kwargs)
            elif backend_name == "transformers" or backend_type == 'TransformersBackend':
                logger.info(f"â†’ transformers ë°±ì—”ë“œë¡œ ë³€í™˜ ì‹œì‘")
                return self._transcribe_with_transformers(audio_path, language)
            elif backend_name == "openai-whisper" or backend_type == 'WhisperBackend':
                logger.info(f"â†’ openai-whisper ë°±ì—”ë“œë¡œ ë³€í™˜ ì‹œì‘")
                return self._transcribe_with_whisper(audio_path, language)
            else:
                logger.info(f"â†’ ì œë„¤ë¦­ ë°±ì—”ë“œ ê°ì²´ë¡œ ë³€í™˜ ì‹œë„ (íƒ€ì…: {backend_type})")
                if hasattr(self.backend, 'transcribe'):
                    result = self.backend.transcribe(audio_path, language)
                    logger.info(f"âœ“ ì œë„¤ë¦­ ë°±ì—”ë“œ ë³€í™˜ ì™„ë£Œ")
                    return result
                else:
                    logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ: {backend_type}")
                    raise RuntimeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ì—”ë“œ: {backend_type}")
        
        except FileNotFoundError as e:
            logger.error(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"íŒŒì¼ ì˜¤ë¥˜: {str(e)}",
                "error_type": "FileNotFoundError",
                "audio_path": audio_path
            }
        except ValueError as e:
            logger.error(f"âŒ ê°’ ì˜¤ë¥˜: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"ê°’ ì˜¤ë¥˜: {str(e)}",
                "error_type": "ValueError",
                "audio_path": audio_path
            }
        except RuntimeError as e:
            logger.error(f"âŒ ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"ëŸ°íƒ€ì„ ì˜¤ë¥˜: {str(e)}",
                "error_type": "RuntimeError",
                "audio_path": audio_path
            }
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}", exc_info=True)
            return {
                "success": False,
                "error": f"{type(e).__name__}: {str(e)}",
                "error_type": type(e).__name__,
                "audio_path": audio_path
            }
    
    def _transcribe_faster_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """faster-whisper (WhisperModel)ë¡œ ë³€í™˜
        
        ì£¼ì˜: faster-whisperëŠ” ë‚´ë¶€ì ìœ¼ë¡œ preprocessor_config.jsonì—ì„œ feature_sizeë¥¼ ì½ìŠµë‹ˆë‹¤.
        turbo ëª¨ë¸ì€ 128 mel-binsì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.
        """
        import locale
        
        logger.info(f"[faster-whisper] ë³€í™˜ ì‹œì‘ (íŒŒì¼: {Path(audio_path).name})")
        
        # ë¡œì¼€ì¼ ì„¤ì • í™•ì¸ ë° ë¡œê¹…
        try:
            current_locale = locale.getlocale()
            logger.debug(f"[faster-whisper] í˜„ì¬ ë¡œì¼€ì¼: {current_locale}")
            
            # UTF-8 ë¡œì¼€ì¼ ì„¤ì • (í•œê¸€ ì²˜ë¦¬ ê°œì„ )
            try:
                locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8')
                logger.debug(f"[faster-whisper] ë¡œì¼€ì¼ ì„¤ì •: ko_KR.UTF-8")
            except Exception as locale_e:
                logger.debug(f"[faster-whisper] ko_KR.UTF-8 ì„¤ì • ì‹¤íŒ¨: {locale_e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        except Exception as e:
            logger.warning(f"[faster-whisper] ë¡œì¼€ì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        try:
            # language íŒŒë¼ë¯¸í„° ì •ê·œí™” (í•œê¸€ ì…ë ¥ì— ëŒ€í•œ alias ì§€ì›)
            language_to_use = language
            if language and language.lower() in ['ko', 'korean']:
                language_to_use = 'ko'
                logger.info(f"[faster-whisper] ì–¸ì–´ ì„¤ì •: ko (í•œêµ­ì–´)")
            elif language:
                logger.info(f"[faster-whisper] ì–¸ì–´ ì„¤ì •: {language}")
            else:
                logger.info(f"[faster-whisper] ì–¸ì–´: ìë™ ê°ì§€")
            
            logger.info(f"[faster-whisper] ëª¨ë¸ ì„¤ì •: beam_size={kwargs.get('beam_size', 5)}, "
                        f"best_of={kwargs.get('best_of', 5)}, "
                        f"patience={kwargs.get('patience', 1)}, "
                        f"temperature={kwargs.get('temperature', 0)}")
            
            logger.debug(f"[faster-whisper] transcribe() í˜¸ì¶œ: language={language_to_use}")
            
            segments, info = self.backend.transcribe(
                audio_path,
                language=language_to_use,
                beam_size=kwargs.get("beam_size", 5),
                best_of=kwargs.get("best_of", 5),
                patience=kwargs.get("patience", 1),
                temperature=kwargs.get("temperature", 0)
            )
            
            logger.info(f"âœ“ faster-whisper ë³€í™˜ ì™„ë£Œ")
            
            # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì§‘
            text = "".join([segment.text for segment in segments])
            detected_language = info.language if info else language_to_use or "unknown"
            
            logger.info(f"  ê²°ê³¼: {len(text)} ê¸€ì, ê°ì§€ëœ ì–¸ì–´: {detected_language}")
            logger.debug(f"  ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 200ì): {text[:200]}")
            
            return {
                "success": True,
                "text": text.strip(),
                "audio_path": audio_path,
                "language": detected_language,
                "duration": info.duration if info else None,
                "backend": "faster-whisper"
            }
        except Exception as e:
            error_msg = str(e)[:200]
            logger.error(f"âŒ faster-whisper ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}", exc_info=True)
            logger.error(f"   ìš”ì²­ ì–¸ì–´: {language_to_use}")
            logger.error(f"   ë©”ì‹œì§€: {error_msg}")
            
            # ì•Œë ¤ì§„ ì—ëŸ¬ ì§„ë‹¨
            if "vocabulary" in error_msg.lower() or "token" in error_msg.lower():
                logger.error(f"   ë¶„ì„: í† í¬ë‚˜ì´ì €/ì–´íœ˜ ì˜¤ë¥˜ - ëª¨ë¸ ì„¤ì • íŒŒì¼ ëˆ„ë½ ê°€ëŠ¥")
            elif "shape" in error_msg.lower() and "128" in error_msg:
                logger.error(f"   ë¶„ì„: mel-spectrogram í˜•ìƒ ì˜¤ë¥˜ - preprocessor_config.jsonì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            elif "model.bin" in error_msg.lower():
                logger.error(f"   ë¶„ì„: model.bin ë¡œë“œ ì˜¤ë¥˜ - CTranslate2 ë³€í™˜ ì‹¤íŒ¨ ê°€ëŠ¥")
            
            return {
                "success": False,
                "error": f"faster-whisper ë³€í™˜ ì‹¤íŒ¨: {type(e).__name__}: {error_msg}",
                "audio_path": audio_path,
                "backend": "faster-whisper",
                "requested_language": language_to_use
            }
    
    def _transcribe_whisper(self, audio_path: str, language: Optional[str] = None, **kwargs) -> Dict:
        """OpenAI Whisperë¡œ ë³€í™˜"""
        result = self.model.transcribe(
            audio_path,
            language=language
        )
        
        text = result.get("text", "").strip()
        
        return {
            "success": True,
            "text": text,
            "audio_path": audio_path,
            "language": language or "unknown",
            "duration": None,
            "backend": "whisper"
        }


def test_stt(model_path: str, audio_dir: str = "audio", device: str = "cpu"):
    """
    STT í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ (ë””ë²„ê¹…ìš©, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
    
    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        audio_dir: í…ŒìŠ¤íŠ¸í•  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    ì°¸ê³ : FastAPI ì„œë¹„ìŠ¤ (api_server.py)ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  ë•ŒëŠ”
         ì´ í•¨ìˆ˜ê°€ ì•„ë‹Œ WhisperSTT í´ë˜ìŠ¤ë¥¼ ì§ì ‘ importí•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # STT ì´ˆê¸°í™”
    stt = WhisperSTT(
        model_path,
        device=device,
        compute_type="float16"
    )
    
    print(f"\nğŸ“Š ì‚¬ìš© ë°±ì—”ë“œ: {stt.backend}\n")
    
    # ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ í™•ì¸
    audio_path = Path(audio_dir)
    if not audio_path.exists():
        print(f"âš ï¸  ìŒì„± íŒŒì¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ì§€ì›í•˜ëŠ” ìŒì„± íŒŒì¼ í˜•ì‹
    supported_formats = ("*.wav", "*.mp3", "*.flac", "*.ogg", "*.m4a")
    audio_files = []
    for fmt in supported_formats:
        audio_files.extend(audio_path.glob(fmt))
    
    if not audio_files:
        print(f"âš ï¸  ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
        return
    
    # ê° íŒŒì¼ì— ëŒ€í•´ STT ìˆ˜í–‰
    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œì˜ ìŒì„± íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")
    
    for idx, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(audio_files)}] ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        result = stt.transcribe(str(audio_file))
        
        if result["success"]:
            print(f"âœ… íŒŒì¼: {audio_file.name}")
            print(f"ğŸ“ ê²°ê³¼:\n{result['text']}")
            if result.get("duration"):
                print(f"â±ï¸  ìŒì„± ê¸¸ì´: {result['duration']:.1f}ì´ˆ")
            print(f"ğŸ”§ ì‚¬ìš© ë°±ì—”ë“œ: {result.get('backend', 'unknown')}")
        else:
            print(f"âŒ íŒŒì¼: {audio_file.name}")
            print(f"ğŸ”´ ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


# ============================================================================
# ì£¼ì˜: ì´ íŒŒì¼ì€ api_server.pyì˜ FastAPI ì„œë¹„ìŠ¤ì—ì„œ importë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.
# api_server.py:
#   from stt_engine import WhisperSTT
#   stt = WhisperSTT(model_path=..., device=...)
#   result = stt.transcribe(audio_path)
#
# ë”°ë¼ì„œ ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤.
# ë§Œì•½ ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:
#   python stt_engine.py  (ë‹¨, audio/ ë””ë ‰í† ë¦¬ì— ìŒì„± íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
# ============================================================================
