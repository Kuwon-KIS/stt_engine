#!/usr/bin/env python3
"""
CTranslate2 ëª¨ë¸ ë³€í™˜ - transformers ì§ì ‘ í™œìš©
"""

import sys
import json
from pathlib import Path

print("=" * 80)
print("ðŸš€ CTranslate2 ëª¨ë¸ ë³€í™˜ (Direct Conversion)")
print("=" * 80)
print()

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
try:
    import torch
    import ctranslate2
    from transformers import AutoModelForCausalLM, WhisperForConditionalGeneration, AutoProcessor
    import safetensors
    
    print(f"âœ… PyTorch: {torch.__version__}")
    print(f"âœ… CTranslate2: {ctranslate2.__version__}")
    print(f"âœ… Transformers: ready")
    print()
    
except ImportError as e:
    print(f"âŒ íŒ¨í‚¤ì§€ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

model_path = Path(__file__).parent / "models" / "openai_whisper-large-v3-turbo"

print(f"ðŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")
print()

# Step 1: ëª¨ë¸ íŒŒì¼ í™•ì¸
print("1ï¸âƒ£  ëª¨ë¸ íŒŒì¼ í™•ì¸...")
print("-" * 80)

model_safetensors = model_path / "model.safetensors"
config_file = model_path / "config.json"

if not model_safetensors.exists():
    print(f"âŒ model.safetensorsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_safetensors}")
    sys.exit(1)

if not config_file.exists():
    print(f"âŒ config.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_file}")
    sys.exit(1)

size_gb = model_safetensors.stat().st_size / (1024 ** 3)
print(f"âœ… ëª¨ë¸ íŒŒì¼: {model_safetensors.name} ({size_gb:.2f}GB)")
print(f"âœ… ì„¤ì • íŒŒì¼: {config_file.name}")
print()

# Step 2: ëª¨ë¸ ë¡œë“œ ë° ë³€í™˜
print("2ï¸âƒ£  ëª¨ë¸ ë¡œë“œ ì¤‘...")
print("-" * 80)
print("   (ì´ ê³¼ì •ì€ 1-3ë¶„ ì†Œìš”ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤)")
print()

try:
    # ëª¨ë¸ ë¡œë“œ
    print("   â€¢ PyTorch ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = WhisperForConditionalGeneration.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    print("   âœ… PyTorch ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
except Exception as e:
    print(f"   âš ï¸  PyTorch ë¡œë“œ ì‹¤íŒ¨: {e}")
    print()
    print("   ëŒ€ì²´ ë°©ë²•: safetensors ì§ì ‘ ë³€í™˜...")
    print()
    
    try:
        # safetensorsë¥¼ PyTorchë¡œ ì§ì ‘ ë³€í™˜
        from safetensors.torch import load_file
        
        state_dict = load_file(str(model_safetensors))
        print(f"   âœ… safetensors ë¡œë“œ: {len(state_dict)} ê°œ íŒŒë¼ë¯¸í„°")
        
        # ëª¨ë¸ êµ¬ì¡° ìƒì„±
        with open(config_file) as f:
            config = json.load(f)
        
        print(f"   âœ… ì„¤ì • ë¡œë“œ: {config.get('model_type', 'unknown')} ëª¨ë¸")
        
        # CTranslate2 ë³€í™˜
        print()
        print("   â€¢ CTranslate2 ë³€í™˜ ì¤‘...")
        
        from ctranslate2.converters.converter import Converter
        
        # ìž„ì‹œ PyTorch ëª¨ë¸ ì €ìž¥
        temp_dir = model_path / "temp_pytorch"
        temp_dir.mkdir(exist_ok=True)
        
        # state_dict ì €ìž¥
        torch.save(state_dict, temp_dir / "pytorch_model.bin")
        
        # config ì €ìž¥
        with open(temp_dir / "config.json", "w") as f:
            json.dump(config, f)
        
        # ë³€í™˜ ì‹¤í–‰
        converter = Converter(
            "models/Whisper",
            str(temp_dir),
            str(model_path),
            force=True,
        )
        converter.convert()
        
        print("   âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ")
        
        # ìž„ì‹œ íŒŒì¼ ì •ë¦¬
        import shutil
        shutil.rmtree(temp_dir)
        
    except Exception as e2:
        print(f"   âŒ ë³€í™˜ ì‹¤íŒ¨: {e2}")
        sys.exit(1)

print()

# Step 3: model.bin í™•ì¸
print("3ï¸âƒ£  ë³€í™˜ ê²°ê³¼ í™•ì¸...")
print("-" * 80)

model_bin = model_path / "model.bin"
if model_bin.exists():
    size_gb = model_bin.stat().st_size / (1024 ** 3)
    print(f"âœ… model.bin ìƒì„±ë¨: {size_gb:.2f}GB")
else:
    print(f"âš ï¸  model.binì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ")
    print()
    print("   í˜„ìž¬ íŒŒì¼ ëª©ë¡:")
    for f in sorted(model_path.glob("*")):
        if f.is_file():
            size = f.stat().st_size
            if size > 1024 ** 3:
                size_str = f"{size / (1024**3):.2f}GB"
            elif size > 1024 ** 2:
                size_str = f"{size / (1024**2):.1f}MB"
            else:
                size_str = f"{size / 1024:.1f}KB"
            print(f"   - {f.name:40s} {size_str:>10s}")

print()
print("=" * 80)
print("âœ… ìž‘ì—… ì™„ë£Œ!")
print("=" * 80)
print()
