#!/usr/bin/env python3
"""
ëª¨ë¸ í˜•ì‹ ë¬¸ì œ ì¬í˜„ ë° í•´ê²° ë°©ì•ˆ ìŠ¤í¬ë¦½íŠ¸

Docker ë‚´ë¶€ì—ì„œ ë°œìƒí•˜ëŠ” "model.binì„ ì°¾ì„ ìˆ˜ ì—†ìŒ" ì—ëŸ¬ ë¶„ì„
"""

import sys
from pathlib import Path

print("=" * 80)
print("ğŸ”´ Docker ë‚´ë¶€ ëª¨ë¸ ë¡œë“œ ì—ëŸ¬ ë¶„ì„")
print("=" * 80)

BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"

# 1ë‹¨ê³„: í˜„ì¬ ëª¨ë¸ ìƒíƒœ
print("\n1ï¸âƒ£  í˜„ì¬ ëª¨ë¸ íŒŒì¼ ìƒíƒœ")
print("-" * 80)

model_files = {
    "model.safetensors": "âœ“ ìˆìŒ" if (models_dir / "model.safetensors").exists() else "âœ— ì—†ìŒ",
    "model.bin": "âœ“ ìˆìŒ" if (models_dir / "model.bin").exists() else "âœ— ì—†ìŒ",
}

for file, status in model_files.items():
    print(f"{file:30s} {status}")

# 2ë‹¨ê³„: ë¬¸ì œ ë¶„ì„
print("\n2ï¸âƒ£  ë¬¸ì œì˜ ì›ì¸")
print("-" * 80)

print("""
í˜„ì¬ ìƒí™©:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ í˜„ì¬: model.safetensors (1.5GB) - HuggingFace PyTorch í˜•ì‹
âœ— í•„ìš”: model.bin (ì´ì „ ë²„ì „ CTranslate2 í˜•ì‹ ë˜ëŠ” PyTorch < 2.2)

Dockerì—ì„œ ë°œìƒí•˜ëŠ” ì—ëŸ¬:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Error: "Unable to open file 'model.bin' in model"

ì´ìœ :
  1ï¸âƒ£  faster-whisperê°€ CTranslate2 ë°±ì—”ë“œ ì‚¬ìš© ì¤‘
      â†’ PyTorch ë²„ì „ì´ 2.2 ë¯¸ë§Œì¼ ê°€ëŠ¥ì„±
      â†’ ë˜ëŠ” CTranslate2ê°€ ìš°ì„ ì ìœ¼ë¡œ ë¡œë“œë¨
  
  2ï¸âƒ£  CTranslate2ëŠ” model.safetensors ë¯¸ì§€ì›
      â†’ model.bin í˜•ì‹ë§Œ ì§€ì›
      â†’ model.binì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹¤íŒ¨

3ï¸âƒ£  stt_engine.pyì˜ local_files_only=True ì„¤ì •
      â†’ ì™¸ë¶€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë¶ˆê°€
      â†’ ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš© ê°•ì œ
      â†’ model.binì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë‚´ ì´ì „ ê²€ì¦ì—ì„œ ë†“ì¹œ ì :
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ ì‹¤ì œ Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì§€ ì•ŠìŒ
âŒ faster-whisperì˜ CTranslate2 ë°±ì—”ë“œ ê³ ë ¤ ë¶€ì¡±
âŒ ë¡œì»¬ Mac í™˜ê²½ (PyTorch 2.1.2)ê³¼ Docker í™˜ê²½ (PyTorch 2.6.0)ì˜ ì°¨ì´ ê°„ê³¼
âŒ "model.binì„ ëª» ì°¾ëŠ”ë‹¤"ëŠ” ê²½ê³  ë¬´ì‹œ

ê²°ë¡ : ë‚´ ê²€ì¦ì´ ë¶ˆì™„ì „í–ˆìŠµë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤! ğŸ™

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

# 3ë‹¨ê³„: í•´ê²° ë°©ë²•
print("\n3ï¸âƒ£  í•´ê²° ë°©ë²• (3ê°€ì§€ ì˜µì…˜)")
print("=" * 80)

solutions = """
âœ… ì˜µì…˜ 1: Dockerfileì—ì„œ CTranslate2 ì œê±° (ê¶Œì¥) ğŸŸ¢
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

í˜„ì¬ Dockerfileì„ ìˆ˜ì •í•˜ì—¬ faster-whisperê°€ PyTorch ë°±ì—”ë“œë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •

ìˆ˜ì • ë°©ë²•:
  1. Dockerfileì—ì„œ faster-whisper ì„¤ì¹˜ ì „ì—:
     
     RUN pip install --no-deps faster-whisper
     
  2. ë˜ëŠ” CTranslate2ë¥¼ ì œê±°:
     
     RUN pip uninstall -y ctranslate2

ì¥ì :
  âœ“ model.safetensors ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
  âœ“ í˜„ì¬ ëª¨ë¸ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
  âœ“ PyTorch 2.6.0ì—ì„œ ìµœê³  ì„±ëŠ¥
  âœ“ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… ì˜µì…˜ 2: ëª¨ë¸ì„ CTranslate2 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ëŒ€ì•ˆ) ğŸŸ 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

í˜„ì¬ model.safetensorsë¥¼ CTranslate2 í˜¸í™˜ model.binìœ¼ë¡œ ë³€í™˜

ë³€í™˜ ë°©ë²•:
  1. ë³€í™˜ ë„êµ¬ ì„¤ì¹˜:
     pip install -U ctranslate2 huggingface-hub
  
  2. ë³€í™˜ ì‹¤í–‰:
     ct2-transformers-converter \\
       --model openai/whisper-large-v3-turbo \\
       --output_dir ./models-ct2 \\
       --model_type whisper \\
       --quantization int8
  
  3. ìƒì„±ëœ model.binì„ models/ í´ë”ë¡œ ì´ë™:
     cp models-ct2/model.bin models/

ì¥ì :
  âœ“ CTranslate2 ìµœì í™” ì‚¬ìš© ê°€ëŠ¥
  âœ“ ì•½ 70% í¬ê¸° ê°ì†Œ (400MB)
  âœ“ ì¶”ë¡  ì†ë„ í–¥ìƒ
  
ë‹¨ì :
  âœ— ì¶”ê°€ ë³€í™˜ ì‹œê°„ (ì•½ 5-10ë¶„)
  âœ— íŒŒì¼ í¬ê¸° ê°ì†Œ (í˜„ì¬ 1.5GB â†’ 400MB)
  âœ— ëª¨ë¸ ìˆ˜ì • ë¶ˆê°€ëŠ¥

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… ì˜µì…˜ 3: stt_engine.py ìˆ˜ì • (ì‘ê¸‰ ì²˜ì¹˜) ğŸ”´
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

stt_engine.pyì—ì„œ device ê¸°ë°˜ìœ¼ë¡œ ë°±ì—”ë“œ ì„ íƒ

ìˆ˜ì • ë°©ë²•:
  í˜„ì¬:
    self.model = WhisperModel(
        self.model_path,
        device=self.device,
        local_files_only=True
    )
  
  ìˆ˜ì •:
    # CTranslate2 ì‚¬ìš© ì‹œë„ ì•ˆ í•˜ê³  PyTorch ì§ì ‘ ì‚¬ìš©
    os.environ['CTRANSLATE2_USE_CUDA'] = '0' if self.device == 'cpu' else '1'
    
    self.model = WhisperModel(
        self.model_path,
        device=self.device,
        local_files_only=True,
        cpu_threads=8
    )

ë‹¨ì :
  âœ— ì„ì‹œ ë°©í¸
  âœ— ì—¬ì „íˆ CTranslate2 ì—ëŸ¬ ê°€ëŠ¥ì„±
  âœ— ê·¼ë³¸ì  í•´ê²° ì•„ë‹˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

print(solutions)

# 4ë‹¨ê³„: ê¶Œì¥ ë°©ë²•
print("\n4ï¸âƒ£  ê¶Œì¥ ë°©ë²• (ì˜µì…˜ 1 + 2)")
print("=" * 80)

print("""
ìµœê³ ì˜ í•´ê²°ì±…:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£  ë¨¼ì € Dockerfile ìˆ˜ì • (CTranslate2 ì œê±°)
    - build-stt-engine-cuda.sh í™•ì¸
    - faster-whisper ì„¤ì¹˜ ë¶€ë¶„ì—ì„œ CTranslate2 ì œê±°
    
2ï¸âƒ£  ê·¸ ë‹¤ìŒ ëª¨ë¸ í˜•ì‹ ì„ íƒ:
    
    A) í˜„ì¬ ëª¨ë¸ ì‚¬ìš© (ê¶Œì¥) ğŸŸ¢
       - model.safetensors ìœ ì§€ (1.5GB)
       - í˜¸í™˜ì„± ìµœê³ , ë³€í™˜ ë¶ˆí•„ìš”
       - Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ í›„ ì‚¬ìš©
    
    B) ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš© (ì„ íƒ) ğŸŸ 
       - model.binìœ¼ë¡œ ë³€í™˜ (400MB)
       - ì„±ëŠ¥ í–¥ìƒ, í¬ê¸° ê°ì†Œ
       - ë³€í™˜ ì‹œê°„ ì†Œìš”

3ï¸âƒ£  Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ
    bash build-stt-engine-cuda.sh

4ï¸âƒ£  ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸
    bash run-docker-gpu.sh
    curl http://localhost:8003/health

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ë‹¤ìŒ ë‹¨ê³„:
  1. build-stt-engine-cuda.sh í™•ì¸ ë° ìˆ˜ì •
  2. ì„ íƒí•œ ì˜µì…˜ 1 ë˜ëŠ” ì˜µì…˜ 2 ì‹¤í–‰
  3. Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ
  4. ì¬í…ŒìŠ¤íŠ¸

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

print("\n" + "=" * 80)
