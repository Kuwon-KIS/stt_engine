#!/usr/bin/env python3
"""
CTranslate2 ëª¨ë¸ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ (Macìš©)
PyTorch í˜•ì‹ì˜ ëª¨ë¸ì„ CTranslate2ê°€ ì½ì„ ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„

ì‚¬ìš©:
  python3 setup_and_convert.py
"""

import sys
import shutil
from pathlib import Path

WORKSPACE = Path(__file__).parent.absolute()
MODELS_DIR = WORKSPACE / "models"
MODEL_PATH = MODELS_DIR / "openai_whisper-large-v3-turbo"

print("=" * 80)
print("ğŸš€ ëª¨ë¸ ì¤€ë¹„ ì‘ì—… ì‹œì‘")
print("=" * 80)
print()

# Step 1: ëª¨ë¸ íŒŒì¼ í™•ì¸
print("1ï¸âƒ£  ëª¨ë¸ íŒŒì¼ í™•ì¸...")
print("-" * 80)

if not MODEL_PATH.exists():
    print(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {MODEL_PATH}")
    sys.exit(1)

print(f"âœ… ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
print()

# í•„ìš”í•œ íŒŒì¼ í™•ì¸
required_files = ["config.json", "model.safetensors"]
for fname in required_files:
    fpath = MODEL_PATH / fname
    if fpath.exists():
        size_gb = fpath.stat().st_size / (1024 ** 3)
        print(f"âœ… {fname:30s} {size_gb:6.2f}GB")
    else:
        print(f"âŒ {fname:30s} (ì—†ìŒ)")

print()

# Step 2: pytorch_model.bin í™•ì¸
print("2ï¸âƒ£  PyTorch ëª¨ë¸ íŒŒì¼ í™•ì¸...")
print("-" * 80)

pytorch_bin = MODEL_PATH / "pytorch_model.bin"
if pytorch_bin.exists():
    size_gb = pytorch_bin.stat().st_size / (1024 ** 3)
    print(f"âœ… pytorch_model.bin ì°¾ìŒ: {size_gb:.2f}GB")
else:
    print(f"âš ï¸  pytorch_model.bin ì—†ìŒ (ì„ íƒì‚¬í•­)")

print()

# Step 3: model.bin ìƒì„± ë˜ëŠ” í™•ì¸
print("3ï¸âƒ£  CTranslate2 í˜¸í™˜ ëª¨ë¸ íŒŒì¼ ìƒì„±...")
print("-" * 80)

model_bin = MODEL_PATH / "model.bin"

if model_bin.exists():
    size_gb = model_bin.stat().st_size / (1024 ** 3)
    print(f"âœ… model.bin ì´ë¯¸ ì¡´ì¬: {size_gb:.2f}GB")
elif pytorch_bin.exists():
    # pytorch_model.binì„ model.binìœ¼ë¡œ ë³µì‚¬
    print(f"ğŸ“‹ pytorch_model.bin â†’ model.bin ë³µì‚¬ ì¤‘...")
    try:
        shutil.copy2(pytorch_bin, model_bin)
        size_gb = model_bin.stat().st_size / (1024 ** 3)
        print(f"âœ… model.bin ìƒì„± ì™„ë£Œ: {size_gb:.2f}GB")
    except Exception as e:
        print(f"âŒ ë³µì‚¬ ì‹¤íŒ¨: {e}")
        sys.exit(1)
else:
    print(f"âš ï¸  PyTorch ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    print(f"   (model.safetensorsë¥¼ ì‚¬ìš©í•˜ë©´ openai-whisper ë°±ì—”ë“œ ì‚¬ìš©)")

print()

# Step 4: ìµœì¢… íŒŒì¼ ëª©ë¡
print("4ï¸âƒ£  ìµœì¢… ëª¨ë¸ íŒŒì¼ ìƒíƒœ...")
print("-" * 80)

print(f"ğŸ“ {MODEL_PATH}/")
model_files = []
for f in sorted(MODEL_PATH.glob("*")):
    if f.is_file() and f.suffix in [".bin", ".safetensors", ".json"]:
        size = f.stat().st_size
        if size > 1024 ** 3:
            size_str = f"{size / (1024**3):.2f}GB"
        elif size > 1024 ** 2:
            size_str = f"{size / (1024**2):.1f}MB"
        else:
            size_str = f"{size / 1024:.1f}KB"
        
        marker = ""
        if f.name == "model.bin":
            marker = " â† CTranslate2 (faster-whisper)"
        elif f.name == "model.safetensors":
            marker = " â† PyTorch (openai-whisper)"
        
        print(f"   {f.name:35s} {size_str:>10s}{marker}")
        model_files.append((f.name, size))

print()
print("=" * 80)
print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
print("=" * 80)
print()
print("ğŸ“ ë°°í¬ ê°€ëŠ¥í•œ ìƒíƒœ:")
print()

if (MODEL_PATH / "model.bin").exists():
    print("âœ… faster-whisper ì§€ì›: model.bin ì¡´ì¬")
if (MODEL_PATH / "model.safetensors").exists():
    print("âœ… openai-whisper ì§€ì›: model.safetensors ì¡´ì¬")

print()
print("ğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print()
print("1ï¸âƒ£  Linux ì„œë²„ë¡œ model.bin ì „ì†¡:")
print(f"   scp {MODEL_PATH}/model.bin <user>@<server>:{MODEL_PATH}/")
print()
print("2ï¸âƒ£  ë˜ëŠ” ì „ì²´ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì „ì†¡:")
print(f"   scp -r {MODEL_PATH} <user>@<server>:{MODELS_DIR}/")
print()
print("3ï¸âƒ£  Linux ì„œë²„ì—ì„œ Docker ì¬ë¹Œë“œ:")
print("   bash scripts/build-stt-engine-cuda.sh")
print()
print("4ï¸âƒ£  Docker ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸:")
print("   curl http://localhost:8003/health")
print()


