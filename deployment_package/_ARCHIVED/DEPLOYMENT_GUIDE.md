# STT Engine ì˜¤í”„ë¼ì¸ ë°°í¬ ê°€ì´ë“œ

## ðŸ“‹ ëª©ì°¨
1. [ë°°í¬ ì¤€ë¹„ (ì¸í„°ë„· ìžˆëŠ” í™˜ê²½)](#ë°°í¬-ì¤€ë¹„)
2. [ì„œë²„ ë°°í¬ (ì¸í„°ë„· ì—†ëŠ” í™˜ê²½)](#ì„œë²„-ë°°í¬)
3. [ê²€ì¦](#ê²€ì¦)
4. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ë°°í¬ ì¤€ë¹„

### 1ë‹¨ê³„: ë¡œì»¬ í™˜ê²½ì—ì„œ wheel ë‹¤ìš´ë¡œë“œ

**ìš”êµ¬ì‚¬í•­:**
- Python 3.11.x
- ì¸í„°ë„· ì—°ê²°
- ì•½ 5GB ì´ìƒì˜ ì €ìž¥ ê³µê°„

**ì‹¤í–‰:**
```bash
cd deployment_package
chmod +x download_wheels.sh
./download_wheels.sh
```

**ì˜ˆìƒ ë‹¤ìš´ë¡œë“œ ì‹œê°„:** 15-30ë¶„ (ì¸í„°ë„· ì†ë„ì— ë”°ë¼)

**ìƒì„±ë˜ëŠ” íŒŒì¼:**
- `wheels/` - ëª¨ë“  .whl íŒŒì¼ (ì•½ 2-3GB)

### 2ë‹¨ê³„: ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„

```bash
# deployment_package ì „ì²´ë¥¼ ë³µì‚¬
cp -r deployment_package /path/to/transfer/location
```

**íŒ¨í‚¤ì§€ êµ¬ì¡°:**
```
deployment_package/
â”œâ”€â”€ wheels/                          # ëª¨ë“  .whl íŒŒì¼
â”œâ”€â”€ deploy.sh                        # ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ setup_offline.sh                 # ì˜¤í”„ë¼ì¸ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements-cuda-12.9.txt       # ìš”êµ¬ì‚¬í•­ íŒŒì¼
â”œâ”€â”€ DEPLOYMENT_GUIDE.md              # ì´ íŒŒì¼
â””â”€â”€ README.md                        # ì„¤ëª…ì„œ
```

### 3ë‹¨ê³„: Linux ì„œë²„ë¡œ ì „ì†¡

USB, ë„¤íŠ¸ì›Œí¬ ë“œë¼ì´ë¸Œ ë“±ìœ¼ë¡œ `deployment_package`ë¥¼ ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

---

## ì„œë²„ ë°°í¬

### ì„œë²„ ìš”êµ¬ì‚¬í•­

**í•„ìˆ˜:**
- Python 3.11.5
- NVIDIA Driver 575.57.08 ì´ìƒ
- CUDA 12.1/12.9 í˜¸í™˜

**í™•ì¸ ëª…ë ¹:**
```bash
python3 --version
nvidia-smi
nvidia-smi --query-gpu=name --format=csv,noheader
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Python 3.11.5
# NVIDIA-SMI output...
Tesla V100  # ë˜ëŠ” ë‹¤ë¥¸ GPU
```

### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

#### ì˜µì…˜ A: ìžë™ ë°°í¬ (ê¶Œìž¥)

```bash
cd deployment_package
chmod +x deploy.sh
./deploy.sh /opt/stt_engine_venv
```

**ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©:**
```bash
./deploy.sh
# ê°€ìƒí™˜ê²½ì´ ~/.venv/stt_engineì— ìƒì„±ë¨
```

#### ì˜µì…˜ B: ìˆ˜ë™ ì„¤ì¹˜

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv /opt/stt_engine_venv

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™”
source /opt/stt_engine_venv/bin/activate

# 3. pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel

# 4. ì˜¤í”„ë¼ì¸ì—ì„œ ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --no-index --find-links=deployment_package/wheels \
    deployment_package/wheels/*.whl
```

### ê²€ì¦

```bash
source /opt/stt_engine_venv/bin/activate

python3 -c "
import torch
import transformers
import fastapi
print('âœ… PyTorch:', torch.__version__)
print('âœ… Transformers:', transformers.__version__)
print('âœ… FastAPI:', fastapi.__version__)
print('âœ… CUDA Available:', torch.cuda.is_available())
print('âœ… GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')
"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
âœ… PyTorch: 2.1.2
âœ… Transformers: 4.37.2
âœ… FastAPI: 0.109.0
âœ… CUDA Available: True
âœ… GPU: Tesla V100
```

---

## STT ì—”ì§„ ì„¤ì •

### 1ë‹¨ê³„: ì†ŒìŠ¤ì½”ë“œ ë³µì‚¬

```bash
# deployment_package ì™¸ë¶€ì˜ ì†ŒìŠ¤íŒŒì¼ë“¤ì„ ë³µì‚¬
cp -r stt_engine /opt/
```

**í•„ìš”í•œ íŒŒì¼:**
- `api_server.py`
- `stt_engine.py`
- `vllm_client.py`
- `model_manager.py`
- ê¸°íƒ€ .py íŒŒì¼ë“¤

### 2ë‹¨ê³„: ëª¨ë¸ ì¤€ë¹„

#### ê²½ìš° A: ì¸í„°ë„· ì ‘ì† ê°€ëŠ¥ (ê¶Œìž¥)

```bash
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate

python3 download_model.py
```

**ë‹¤ìš´ë¡œë“œ ì‹œê°„:** ì•½ 20-30ë¶„
**í•„ìš” ê³µê°„:** ì•½ 5GB

#### ê²½ìš° B: ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©

ë¡œì»¬ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•œ í›„ `models/` ë””ë ‰í† ë¦¬ë¥¼ ì „ì†¡:

```bash
# ë¡œì»¬ì—ì„œ
python3 download_model.py

# ì „ì†¡
scp -r models/ user@server:/opt/stt_engine/
```

### 3ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
cd /opt/stt_engine

# .env íŒŒì¼ ìƒì„± (í•„ìš”ì‹œ)
cat > .env << EOF
VLLM_API_URL=http://localhost:8000
VLLM_MODEL_NAME=meta-llama/Llama-2-7b-hf
EOF
```

---

## ì‹¤í–‰

### í„°ë¯¸ë„ 1: STT ì—”ì§„ ì‹œìž‘

```bash
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate

python3 api_server.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ðŸ”— vLLM ì„œë²„ ì—°ê²° ì„¤ì •
   API URL: http://localhost:8000
   ëª¨ë¸: meta-llama/Llama-2-7b-hf
âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ
INFO:     Uvicorn running on http://0.0.0.0:8001
```

### í„°ë¯¸ë„ 2: vLLM ì„œë²„ ì‹œìž‘

```bash
# Docker ì‚¬ìš© (ê¶Œìž¥)
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16

# ë˜ëŠ” venv ì„¤ì¹˜ í›„
source /opt/vllm_venv/bin/activate
python3 -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16
```

### í„°ë¯¸ë„ 3: í…ŒìŠ¤íŠ¸

```bash
source /opt/stt_engine_venv/bin/activate

# í—¬ìŠ¤ ì²´í¬
python3 api_client.py --health

# STT ë³€í™˜
python3 api_client.py --transcribe audio.wav

# STT + vLLM ì²˜ë¦¬
python3 api_client.py --process audio.wav --instruction "ìš”ì•½í•´ì£¼ì„¸ìš”"
```

---

## ê²€ì¦

### í—¬ìŠ¤ ì²´í¬ API

```bash
curl http://localhost:8001/health
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{
  "status": "healthy",
  "device": "cuda",
  "models_loaded": true
}
```

### STT API í…ŒìŠ¤íŠ¸

```bash
curl -X POST \
  -F "file=@audio.wav" \
  http://localhost:8001/transcribe
```

---

## ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤í™” (ì„ íƒ)

### systemd ì„œë¹„ìŠ¤ ìƒì„±

```bash
sudo cat > /etc/systemd/system/stt-engine.service << EOF
[Unit]
Description=STT Engine Service
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=/opt/stt_engine
ExecStart=/opt/stt_engine_venv/bin/python3 api_server.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable stt-engine
sudo systemctl start stt-engine
sudo systemctl status stt-engine
```

### ì„œë¹„ìŠ¤ ê´€ë¦¬

```bash
# ìƒíƒœ í™•ì¸
sudo systemctl status stt-engine

# ë¡œê·¸ í™•ì¸
sudo journalctl -u stt-engine -f

# ì‹œìž‘/ì¤‘ì§€
sudo systemctl start stt-engine
sudo systemctl stop stt-engine
sudo systemctl restart stt-engine
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: CUDA ê´€ë ¨ ì˜¤ë¥˜

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory
```

**í•´ê²°:**
```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ëª¨ë¸ í¬ê¸° í™•ì¸
ls -lh models/

# í•„ìš”ì‹œ smaller ëª¨ë¸ ì‚¬ìš©
# vllm_client.pyì—ì„œ ëª¨ë¸ëª… ë³€ê²½
```

### ë¬¸ì œ 2: í¬íŠ¸ ì´ë¯¸ ì‚¬ìš© ì¤‘

**ì¦ìƒ:**
```
Address already in use: ('0.0.0.0', 8001)
```

**í•´ê²°:**
```bash
# ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
lsof -i :8001

# í¬íŠ¸ ë³€ê²½
python3 api_server.py --port 8002
```

### ë¬¸ì œ 3: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
FileNotFoundError: models/openai_whisper-large-v3-turbo not found
```

**í•´ê²°:**
```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
ls -la models/

# í•„ìš”ì‹œ ëª¨ë¸ ìž¬ë‹¤ìš´ë¡œë“œ
python3 download_model.py
```

### ë¬¸ì œ 4: vLLM ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
âŒ vLLM ì„œë²„ ì—°ê²° ë¶ˆê°€
```

**í•´ê²°:**
```bash
# vLLM ì„œë²„ ì‹¤í–‰ í™•ì¸
curl http://localhost:8000/health

# vLLM ì‹œìž‘ (ë³„ë„ í„°ë¯¸ë„)
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf
```

### ë¬¸ì œ 5: íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR: Could not find a version that satisfies the requirement
```

**í•´ê²°:**
```bash
# ìˆ˜ë™ìœ¼ë¡œ wheel íŒŒì¼ ì§€ì • ì„¤ì¹˜
cd deployment_package/wheels
pip install *.whl --no-index

# ë˜ëŠ” íŠ¹ì • íŒ¨í‚¤ì§€ë§Œ
pip install --no-index --find-links=. torch-2.1.2+cu121-cp311-cp311-linux_x86_64.whl
```

---

## ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœëŒ€í™”

```python
# api_server.py ìˆ˜ì •
import torch
torch.cuda.empty_cache()
```

### vLLM ìµœì í™”

```bash
docker run --gpus all \
  -p 8000:8000 \
  --ipc=host \
  -e VLLM_ATTENTION_BACKEND=xformers \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.9
```

---

## ë¬¸ì˜ ë° í”¼ë“œë°±

ë°°í¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì •ë³´ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”:

1. **ì‹œìŠ¤í…œ ì •ë³´:**
   ```bash
   uname -a
   python3 --version
   nvidia-smi
   ```

2. **ì—ëŸ¬ ë¡œê·¸:**
   ```bash
   # STT Engine ë¡œê·¸
   tail -100 stt_engine.log
   ```

3. **pip ì •ë³´:**
   ```bash
   pip list
   ```

---

**ë°°í¬ ê°€ì´ë“œ ë²„ì „:** 1.0  
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2026-01-30  
**ìž‘ì„±ìž:** STT Engine Team
