# STT Engine μ¤ν”„λΌμΈ λ°°ν¬ ν¨ν‚¤μ§€

[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![CUDA 12.1/12.9](https://img.shields.io/badge/CUDA-12.1%2F12.9-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

μ™Έλ¶€ μΈν„°λ„· ν†µμ‹ μ΄ λ¶κ°€λ¥ν• Linux μ„λ²„μ— STT Engineμ„ λ°°ν¬ν•κΈ° μ„ν• μ™„μ „ν• ν¨ν‚¤μ§€μ…λ‹λ‹¤.

## π― νΉμ§•

- β… **μ™„μ „ μ¤ν”„λΌμΈ μ„¤μΉ** - μΈν„°λ„· μ—°κ²° λ¶ν•„μ”
- β… **μλ™ λ°°ν¬ μ¤ν¬λ¦½νΈ** - μ›ν΄λ¦­ μ„¤μΉ
- β… **CUDA 12.1/12.9 μµμ ν™”** - GPU μ™„λ²½ μ§€μ›
- β… **Python 3.11 μ§€μ›** - μµμ‹  Python λ²„μ „
- β… **μλ™ κ²€μ¦** - μ„¤μΉ ν›„ μλ™ ν™•μΈ
- β… **μƒμ„Έν• κ°€μ΄λ“** - λ‹¨κ³„λ³„ μ„¤λ…

## π“¦ ν¨ν‚¤μ§€ κµ¬μ„±

```
deployment_package/
β”β”€β”€ wheels/                          # λ¨λ“  μμ΅΄μ„± .whl νμΌ (2-3GB)
β”‚   β”β”€β”€ torch-2.1.2+cu121-*.whl
β”‚   β”β”€β”€ torchaudio-2.1.2+cu121-*.whl
β”‚   β”β”€β”€ transformers-4.37.2-*.whl
β”‚   β”β”€β”€ librosa-0.10.0-*.whl
β”‚   β””β”€β”€ ... (50+ ν¨ν‚¤μ§€)
β”‚
β”β”€β”€ deploy.sh                        # π€ λ°°ν¬ μ¤ν¬λ¦½νΈ (μΈν„°λ„· μ—†μ)
β”β”€β”€ setup_offline.sh                 # π“¦ μλ™ μ„¤μΉ μ¤ν¬λ¦½νΈ
β”β”€β”€ download_wheels.sh               # β¬‡οΈ  wheel λ‹¤μ΄λ΅λ“ (μΈν„°λ„· μμ)
β”‚
β”β”€β”€ requirements-cuda-12.9.txt       # μ”κµ¬μ‚¬ν•­ λ…μ‹
β”β”€β”€ DEPLOYMENT_GUIDE.md              # π“– μƒμ„Έ λ°°ν¬ κ°€μ΄λ“
β””β”€β”€ README.md                        # μ΄ νμΌ
```

## π€ λΉ λ¥Έ μ‹μ‘

### λ‹¨κ³„ 1: λ΅μ»¬ ν™κ²½μ—μ„ wheel λ‹¤μ΄λ΅λ“ (μΈν„°λ„· μλ” κ³³)

```bash
cd deployment_package
chmod +x download_wheels.sh
./download_wheels.sh
```

**μ”κµ¬μ‚¬ν•­:**
- Python 3.11.x
- μΈν„°λ„· μ—°κ²°
- 5GB μ΄μƒ μ €μ¥ κ³µκ°„

**μ‹κ°„:** μ•½ 15-30λ¶„

### λ‹¨κ³„ 2: Linux μ„λ²„λ΅ μ „μ†΅

```bash
# λ΅μ»¬μ—μ„
scp -r deployment_package user@server:/home/user/

# λλ” USB/λ„¤νΈμ›ν¬ λ“λΌμ΄λΈλ΅ μ „μ†΅
```

### λ‹¨κ³„ 3: Linux μ„λ²„μ—μ„ λ°°ν¬

```bash
cd deployment_package
chmod +x deploy.sh
./deploy.sh /opt/stt_engine_venv
```

**μμƒ μ‹κ°„:** 5-10λ¶„  
**μ”κµ¬μ‚¬ν•­:** Python 3.11.5, NVIDIA Driver

### λ‹¨κ³„ 4: κ²€μ¦

```bash
source /opt/stt_engine_venv/bin/activate
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

**μ„±κ³µ λ©”μ‹μ§€:**
```
CUDA: True
```

## π“‹ μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### Linux μ„λ²„

| ν•­λ© | μ”κµ¬μ‚¬ν•­ | ν™•μΈ λ…λ Ή |
|------|---------|----------|
| **OS** | Ubuntu 20.04+ λλ” λ™λ“± | `lsb_release -a` |
| **Python** | 3.11.5 (ν•„μ) | `python3 --version` |
| **NVIDIA Driver** | 575+ | `nvidia-smi` |
| **CUDA** | 12.1 λλ” 12.9 νΈν™ | `nvidia-smi \| grep CUDA` |
| **GPU λ©”λ¨λ¦¬** | 6GB μ΄μƒ | `nvidia-smi` |
| **λ””μ¤ν¬ κ³µκ°„** | 10GB μ΄μƒ | `df -h` |
| **RAM** | 16GB μ΄μƒ (κ¶μ¥) | `free -h` |

### λ΅μ»¬ ν™κ²½ (wheel λ‹¤μ΄λ΅λ“)

| ν•­λ© | μ”κµ¬μ‚¬ν•­ |
|------|---------|
| **Python** | 3.11.x |
| **μΈν„°λ„·** | ν•„μ (15-30λ¶„) |
| **μ €μ¥ κ³µκ°„** | 5GB μ΄μƒ |

## π“– μμ„Έν• κ°€μ΄λ“

μ „μ²΄ λ°°ν¬ κ³Όμ •, νΈλ¬λΈ”μν…, μµμ ν™” λ°©λ²•μ€:

π“„ **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)** μ°Έμ΅°

μ£Όμ” μ„Ήμ…:
- [λ°°ν¬ μ¤€λΉ„](DEPLOYMENT_GUIDE.md#λ°°ν¬-μ¤€λΉ„)
- [μ„λ²„ λ°°ν¬](DEPLOYMENT_GUIDE.md#μ„λ²„-λ°°ν¬)
- [STT μ—”μ§„ μ„¤μ •](DEPLOYMENT_GUIDE.md#stt-μ—”μ§„-μ„¤μ •)
- [μ‹¤ν–‰](DEPLOYMENT_GUIDE.md#μ‹¤ν–‰)
- [νΈλ¬λΈ”μν…](DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…)

## π”§ ν¬ν•¨λ ν¨ν‚¤μ§€

### ν•µμ‹¬ λΌμ΄λΈλ¬λ¦¬

| ν¨ν‚¤μ§€ | λ²„μ „ | μ©λ„ |
|--------|------|------|
| **torch** | 2.1.2 | λ”¥λ¬λ‹ ν”„λ μ„μ›ν¬ (CUDA 12.1) |
| **torchaudio** | 2.1.2 | μμ„± μ²λ¦¬ (CUDA 12.1) |
| **transformers** | 4.37.2 | Hugging Face λ¨λΈ |
| **librosa** | 0.10.0 | μ¤λ””μ¤ μ‹ νΈ μ²λ¦¬ |
| **scipy** | 1.12.0 | κ³Όν•™ κ³„μ‚° |
| **numpy** | 1.24.3 | μμΉ μ—°μ‚° |

### μ›Ή ν”„λ μ„μ›ν¬

| ν¨ν‚¤μ§€ | λ²„μ „ | μ©λ„ |
|--------|------|------|
| **fastapi** | 0.109.0 | REST API ν”„λ μ„μ›ν¬ |
| **uvicorn** | 0.27.0 | ASGI μ„λ²„ |
| **pydantic** | 2.5.3 | λ°μ΄ν„° κ²€μ¦ |
| **requests** | 2.31.0 | HTTP ν΄λΌμ΄μ–ΈνΈ |

### κΈ°νƒ€

| ν¨ν‚¤μ§€ | λ²„μ „ | μ©λ„ |
|--------|------|------|
| **huggingface-hub** | 0.21.4 | λ¨λΈ λ‹¤μ΄λ΅λ“ |
| **python-dotenv** | 1.0.0 | ν™κ²½ λ³€μ |
| **pyyaml** | 6.0.1 | YAML νμ‹± |

## π’» μ‚¬μ© μμ 

### 1. ν—¬μ¤ μ²΄ν¬

```bash
curl http://localhost:8001/health
```

```json
{
  "status": "healthy",
  "device": "cuda",
  "models_loaded": true
}
```

### 2. μμ„± λ³€ν™ (STT)

```bash
curl -X POST \
  -F "file=@audio.wav" \
  http://localhost:8001/transcribe
```

```json
{
  "success": true,
  "text": "μ•λ…•ν•μ„Έμ”, μμ„± μΈμ‹ ν…μ¤νΈμ…λ‹λ‹¤.",
  "language": "ko"
}
```

### 3. μμ„± λ³€ν™ + ν…μ¤νΈ μ²λ¦¬

```bash
curl -X POST \
  -F "file=@audio.wav" \
  -F "instruction=λ‹¤μ ν…μ¤νΈλ¥Ό μ”μ•½ν•΄μ£Όμ„Έμ”:" \
  http://localhost:8001/transcribe-and-process
```

```json
{
  "success": true,
  "stt_result": {
    "text": "μ•λ…•ν•μ„Έμ”, μμ„± μΈμ‹ ν…μ¤νΈμ…λ‹λ‹¤."
  },
  "vllm_result": {
    "processed_text": "μμ„± μΈμ‹ ν…μ¤νΈμ…λ‹λ‹¤."
  }
}
```

### 4. Python ν΄λΌμ΄μ–ΈνΈ μ‚¬μ©

```python
from api_client import STTClient

client = STTClient("http://localhost:8001")

# ν—¬μ¤ μ²΄ν¬
client.health_check()

# STT λ³€ν™
result = client.transcribe("audio.wav")
print(result['text'])

# STT + vLLM μ²λ¦¬
result = client.transcribe_and_process(
    "audio.wav",
    instruction="μ΄ λ‚΄μ©μ„ μ”μ•½ν•΄μ¤„ μ μλ‚μ”?"
)
```

## π› οΈ νΈλ¬λΈ”μν…

### CUDA λ¬Έμ 

```bash
# CUDA κ°€μ©μ„± ν™•μΈ
python3 -c "import torch; print(torch.cuda.is_available())"

# GPU μ •λ³΄ ν™•μΈ
nvidia-smi

# CUDA λ²„μ „ ν™•μΈ
cat /usr/local/cuda/version.txt
```

### ν¬νΈ μ¶©λ

```bash
# μ‚¬μ© μ¤‘μΈ ν¬νΈ ν™•μΈ
lsof -i :8001
lsof -i :8000

# ν¬νΈ λ³€κ²½ μ‹¤ν–‰
python3 api_server.py --port 8002
```

### λ¨λΈ λ¬Έμ 

```bash
# λ¨λΈ λ””λ ‰ν† λ¦¬ ν™•μΈ
ls -la models/

# λ¨λΈ μ¬λ‹¤μ΄λ΅λ“ (μΈν„°λ„· ν•„μ”)
python3 download_model.py
```

λ” λ§μ€ νΈλ¬λΈ”μν…μ€ **[DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…](DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…)** μ°Έμ΅°

## π“ μ„¤μΉ ν›„ λ””λ ‰ν† λ¦¬ κµ¬μ΅°

```
/opt/
β”β”€β”€ stt_engine/                      # μ†μ¤ μ½”λ“
β”‚   β”β”€β”€ api_server.py
β”‚   β”β”€β”€ stt_engine.py
β”‚   β”β”€β”€ models/
β”‚   β”‚   β””β”€β”€ openai_whisper-large-v3-turbo/
β”‚   β””β”€β”€ ...
β”‚
β””β”€β”€ stt_engine_venv/                 # κ°€μƒν™κ²½
    β”β”€β”€ bin/
    β”‚   β”β”€β”€ python3
    β”‚   β”β”€β”€ pip
    β”‚   β””β”€β”€ activate
    β”β”€β”€ lib/
    β”‚   β””β”€β”€ python3.11/
    β”‚       β””β”€β”€ site-packages/       # μ„¤μΉλ λ¨λ“  ν¨ν‚¤μ§€
    β””β”€β”€ ...
```

## π” λ³΄μ• κ³ λ ¤μ‚¬ν•­

1. **λ°©ν™”λ²½ μ„¤μ •**
   ```bash
   sudo ufw allow 8001/tcp  # STT Engine
   sudo ufw allow 8000/tcp  # vLLM (ν•„μ”μ‹)
   ```

2. **κ°€μƒν™κ²½ λ¶„λ¦¬**
   - μ‹μ¤ν… Pythonκ³Ό λ¶„λ¦¬λ ν™κ²½ μ‚¬μ©
   - κ¶μ¥: `/opt/` λλ” `/home/user/` μ„μΉ

3. **λ΅κ·Έ λ¨λ‹ν„°λ§**
   ```bash
   tail -f /var/log/stt-engine.log
   ```

## π“ λΌμ΄μ„ μ¤

MIT License - μμ λ΅­κ² μ‚¬μ©, μμ •, λ°°ν¬ κ°€λ¥

## π¤ κΈ°μ—¬

λ²„κ·Έ λ³΄κ³  λ° κ°μ„  μ‚¬ν•­μ€ μ΄μλ΅ λ“±λ΅ν•΄μ£Όμ„Έμ”.

## π“ μ§€μ›

λ¬Έμ  λ°μƒ μ‹:

1. **DEPLOYMENT_GUIDE.md** νΈλ¬λΈ”μν… μ„Ήμ… ν™•μΈ
2. μ‹μ¤ν… μ •λ³΄ μμ§‘:
   ```bash
   python3 --version
   nvidia-smi
   pip list
   ```
3. μ—λ¬ λ΅κ·Έ ν™•μΈ λ° κ³µμ 

---

**ν¨ν‚¤μ§€ λ²„μ „:** 1.0  
**Python:** 3.11  
**CUDA:** 12.1/12.9  
**μƒμ„± λ‚ μ§:** 2026-01-30
