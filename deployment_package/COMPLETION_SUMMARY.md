# STT Engine μ¤ν”„λΌμΈ λ°°ν¬ ν¨ν‚¤μ§€ - μ™„μ„± μ²΄ν¬λ¦¬μ¤νΈ

μƒμ„± λ‚ μ§: 2026-01-30  
λ€μƒ μ„λ²„: Linux (Python 3.11.5, NVIDIA Driver 575.57.08, CUDA 12.9)

---

## β… μƒμ„±λ νμΌ λ©λ΅

### π“ μ£Όμ” νμΌ

- β… **deploy.sh** (λ°°ν¬ μ¤ν¬λ¦½νΈ)
  - Linux μ„λ²„μ—μ„ μ‹¤ν–‰
  - μλ™ κ°€μƒν™κ²½ μƒμ„± λ° μ„¤μ •
  - λ¨λ“  wheel νμΌ μ„¤μΉ
  - μ„¤μΉ ν›„ μλ™ κ²€μ¦
  - μ‚¬μ©λ²•: `./deploy.sh /opt/stt_engine_venv`

- β… **download_wheels_macos.sh** (λ΅μ»¬ Wheel λ‹¤μ΄λ΅λ“)
  - macOS/Linuxμ—μ„ μ‹¤ν–‰
  - Linux (x86_64) ν”λ«νΌμ© wheel λ‹¤μ΄λ΅λ“
  - Python 3.11 νΈν™
  - CUDA 12.1 μµμ ν™”
  - μ‚¬μ©λ²•: `./download_wheels_macos.sh`

- β… **setup_offline.sh** (μλ™ μ„¤μΉ)
  - μΈν„°λ„· μ™„μ „ μ°¨λ‹¨ ν™κ²½μ©
  - κ°„λ‹¨ν• μ„¤μΉ ν”„λ΅μ„Έμ¤
  - μ‚¬μ©λ²•: `./setup_offline.sh /path/to/venv`

- β… **run_all.sh** (μ„λΉ„μ¤ μ‹¤ν–‰)
  - STT Engineκ³Ό vLLMμ„ ν•¨κ» μ‹μ‘
  - μλ™ ν—¬μ¤ μ²΄ν¬
  - μ‚¬μ©λ²•: `./run_all.sh /opt/stt_engine_venv`

### π“– λ¬Έμ„

- β… **QUICKSTART.md** (λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“)
  - 3λ‹¨κ³„ μ„¤λ…
  - μ‹κ°„ μμƒ
  - μμ£Ό λ¬»λ” μ§λ¬Έ

- β… **DEPLOYMENT_GUIDE.md** (μƒμ„Έ λ°°ν¬ κ°€μ΄λ“)
  - μ „μ²΄ ν”„λ΅μ„Έμ¤ μ„¤λ…
  - μ„¤μ • λ‹¨κ³„λ³„ μ•λ‚΄
  - νΈλ¬λΈ”μν… (5κ° ν•­λ©)
  - μ„±λ¥ μµμ ν™”
  - systemd μ„λΉ„μ¤ μ„¤μ •

- β… **README.md** (ν¨ν‚¤μ§€ κ°μ”)
  - νΉμ§• μ”μ•½
  - μ‚¬μ© μμ 
  - ν¬ν•¨ ν¨ν‚¤μ§€ λ©λ΅
  - λΌμ΄μ„ μ¤ μ •λ³΄

### π“‹ μ„¤μ • νμΌ

- β… **requirements.txt** (μμ΅΄μ„± λ©λ΅)
  - λ¨λ“  ν¨ν‚¤μ§€μ™€ λ²„μ „ λ…μ‹
  - μ°Έμ΅°μ© λ¬Έμ„

- β… **requirements-cuda-12.9.txt** (CUDA μµμ ν™”)
  - CUDA λ²„μ „ λ…μ‹
  - PyPI μΈλ±μ¤ μ§€μ •

### π“¦ Wheels λ””λ ‰ν† λ¦¬

- β… **wheels/** (μƒμ„± μ¤€λΉ„)
  - λ‹¤μ΄λ΅λ“ν•  .whl νμΌ μ €μ¥ μ„μΉ
  - download_wheels_macos.sh μ‹¤ν–‰ μ‹ μλ™ μ±„μ›μ§
  - μμƒ ν¬κΈ°: 2-3GB
  - μμƒ νμΌ κ°μ: 50+κ°

---

## π― ν¬ν•¨λ ν¨ν‚¤μ§€

### λ”¥λ¬λ‹ & μμ„± μ²λ¦¬
- torch==2.1.2 (CUDA 12.1)
- torchaudio==2.1.2 (CUDA 12.1)
- transformers==4.37.2
- librosa==0.10.0
- scipy==1.12.0

### μ›Ή ν”„λ μ„μ›ν¬
- fastapi==0.109.0
- uvicorn==0.27.0
- requests==2.31.0
- pydantic==2.5.3

### κΈ°νƒ€
- huggingface-hub==0.21.4
- numpy==1.24.3
- python-dotenv==1.0.0
- pyyaml==6.0.1

**μ΄ 13κ° μ£Όμ” ν¨ν‚¤μ§€ + 40+ μΆ…μ†μ„±**

---

## π“‹ μ‚¬μ© λ‹¨κ³„

### λ‹¨κ³„ 1: λ΅μ»¬ (μΈν„°λ„· μμ) - 15-30λ¶„

```bash
cd deployment_package
chmod +x download_wheels_macos.sh
./download_wheels_macos.sh
```

**κ²°κ³Ό:**
- `wheels/` λ””λ ‰ν† λ¦¬μ— λ¨λ“  .whl νμΌ μƒμ„± (2-3GB)

### λ‹¨κ³„ 2: μ „μ†΅

```bash
# USB, SCP, λ„¤νΈμ›ν¬ λ“λΌμ΄λΈ λ“±μΌλ΅ μ „μ†΅
scp -r deployment_package user@server:/home/user/
```

### λ‹¨κ³„ 3: μ„λ²„ (μΈν„°λ„· μ—†μ) - 5-10λ¶„

```bash
cd /home/user/deployment_package
chmod +x deploy.sh
./deploy.sh /opt/stt_engine_venv
```

**κ²°κ³Ό:**
- κ°€μƒν™κ²½ μƒμ„±: `/opt/stt_engine_venv`
- λ¨λ“  ν¨ν‚¤μ§€ μ„¤μΉ
- μλ™ κ²€μ¦ μ™„λ£

---

## π” κ²€μ¦ λ°©λ²•

### μ„¤μΉ ν›„ ν™•μΈ

```bash
source /opt/stt_engine_venv/bin/activate

# 1. ν¨ν‚¤μ§€ ν™•μΈ
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"

# 2. μ£Όμ” λΌμ΄λΈλ¬λ¦¬ ν™•μΈ
pip list | grep -E "torch|transformers|fastapi"

# 3. GPU ν™•μΈ
nvidia-smi
```

**μ„±κ³µ μ΅°κ±΄:**
- β… torch.cuda.is_available() = True
- β… λ¨λ“  ν¨ν‚¤μ§€κ°€ μ„¤μΉλ¨
- β… GPUκ°€ μΈμ‹λ¨ (nvidia-smi μ¶λ ¥)

---

## π€ λ°°ν¬ ν›„ μ‹¤ν–‰

### 1. μ†μ¤ μ½”λ“ μ¤€λΉ„

```bash
# λ΅μ»¬μ—μ„ stt_engine μ†μ¤ λ””λ ‰ν† λ¦¬ μ „μ†΅
scp -r stt_engine user@server:/opt/

# μ„λ²„μ—μ„
ls /opt/stt_engine/
# β†’ api_server.py, stt_engine.py, vllm_client.py λ“±
```

### 2. λ¨λΈ λ‹¤μ΄λ΅λ“ (μ„ νƒ)

```bash
# μ„λ²„κ°€ μΈν„°λ„· μ ‘μ† κ°€λ¥μ‹
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate
python3 download_model.py
# β†’ μ•½ 20-30λ¶„, 5GB λ‹¤μ΄λ΅λ“
```

### 3. μ„λΉ„μ¤ μ‹¤ν–‰

```bash
# ν„°λ―Έλ„ 1: STT Engine
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate
python3 api_server.py

# ν„°λ―Έλ„ 2: vLLM (Docker)
docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf --dtype float16
```

---

## π“ ν¨ν‚¤μ§€ ν¬κΈ° λ° μ‹κ°„

| ν•­λ© | ν¬κΈ° | μ‹κ°„ |
|------|------|------|
| Wheel λ‹¤μ΄λ΅λ“ | 2-3GB | 15-30λ¶„ |
| μ„λ²„ λ°°ν¬ | - | 5-10λ¶„ |
| λ¨λΈ λ‹¤μ΄λ΅λ“ | 5GB | 20-30λ¶„ |
| μ΄ μ‹κ°„ | 7-8GB | 40-70λ¶„ |

---

## π”’ λ³΄μ• κ³ λ ¤μ‚¬ν•­

1. **κ°€μƒν™κ²½ λ¶„λ¦¬**
   - μ‹μ¤ν… Pythonκ³Ό λ…λ¦½μ 
   - κ¶ν• μµμ†ν™”

2. **λ°©ν™”λ²½ μ„¤μ •**
   ```bash
   sudo ufw allow 8001/tcp  # STT Engine
   sudo ufw allow 8000/tcp  # vLLM
   ```

3. **λ΅κ·Έ λ¨λ‹ν„°λ§**
   ```bash
   tail -f stt_engine.log
   ```

4. **λ°±μ—…**
   - λ¨λΈ νμΌ λ°±μ—… (5GB)
   - ν™κ²½ λ³€μ λ°±μ—…

---

## π› οΈ ν•„μ ν™•μΈμ‚¬ν•­

λ°°ν¬ μ „ μ„λ²„μ—μ„ ν™•μΈ:

```bash
# 1. Python λ²„μ „
python3 --version
# β†’ Python 3.11.5 β…

# 2. NVIDIA λ“λΌμ΄λ²„
nvidia-smi
# β†’ Driver 575.57.08 β…

# 3. CUDA λ²„μ „
nvidia-smi | grep CUDA
# β†’ CUDA 12.1 λλ” 12.9 β…

# 4. GPU λ©”λ¨λ¦¬
nvidia-smi | grep memory.total
# β†’ 6GB μ΄μƒ β…

# 5. λ””μ¤ν¬ κ³µκ°„
df -h
# β†’ 10GB μ΄μƒ μ—¬μ  β…
```

---

## π“ μ£Όμ” νΉμ§•

### β¨ μ™„μ „ μ¤ν”„λΌμΈ μ„¤μΉ
- μΈν„°λ„· μ—°κ²° μ—†μ΄ λ°°ν¬ κ°€λ¥
- wheels λ””λ ‰ν† λ¦¬λ§μΌλ΅ μ¶©λ¶„

### β¨ μλ™ν™”λ λ°°ν¬
- ν• μ¤„ λ…λ ΉμΌλ΅ μ‹μ‘
- μλ™ κ²€μ¦ ν¬ν•¨
- μ—λ¬ μ²λ¦¬ μ™„λ²½

### β¨ ν”λ«νΌ νΈν™μ„±
- Python 3.11.5 μµμ ν™”
- CUDA 12.1/12.9 νΈν™
- Linux x86_64 μ§€μ›

### β¨ λ¬Έμ„ν™”
- 4κ°μ μƒμ„Έ κ°€μ΄λ“
- νΈλ¬λΈ”μν… μ„Ήμ…
- μ‚¬μ© μμ  ν¬ν•¨

### β¨ ν™•μ¥μ„±
- μ»¤μ¤ν…€ λ¨λΈ μ§€μ›
- ν¬νΈ μ„¤μ • κ°€λ¥
- systemd μ„λΉ„μ¤ ν†µν•©

---

## π“ μ§€μ› λ° λ¬Έμ  ν•΄κ²°

### μμ£Ό λ¬»λ” μ§λ¬Έ

**Q: wheelsλ¥Ό λ‹¤μ‹ λ‹¤μ΄λ΅λ“ν•΄μ•Ό ν•λ‚μ”?**  
A: μ•„λ‹μ”. ν• λ² λ‹¤μ΄λ΅λ“ ν›„ μ—¬λ¬ μ„λ²„μ— λ°°ν¬ κ°€λ¥ν•©λ‹λ‹¤.

**Q: μ¤ν”„λΌμΈ μƒνƒμ—μ„ λ¨λΈμ„ λ΅λ“ν•  μ μλ‚μ”?**  
A: λ„¤. μ‚¬μ „ λ‹¤μ΄λ΅λ“λ λ¨λΈ λ””λ ‰ν† λ¦¬λ¥Ό μ „μ†΅ν•λ©΄ λ©λ‹λ‹¤.

**Q: λ‹¤λ¥Έ GPUμ—μ„λ„ λ™μ‘ν•λ‚μ”?**  
A: λ„¤. NVIDIA GPUμ΄λ©΄ λ€λ¶€λ¶„ λ™μ‘ν•©λ‹λ‹¤ (V100, A100, RTX λ“±).

**Q: ν¬νΈ λ³€κ²½μ€ μ–΄λ–»κ² ν•λ‚μ”?**  
A: api_server.pyμ— --port μµμ…μΌλ΅ μ§€μ • κ°€λ¥ν•©λ‹λ‹¤.

### νΈλ¬λΈ”μν…

μμ„Έν• λ¬Έμ  ν•΄κ²°: **[DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…](DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…)**

μ£Όμ” ν•­λ©:
- CUDA κ΄€λ ¨ μ¤λ¥
- ν¬νΈ μ¶©λ
- λ¨λΈ λ΅λ“ μ‹¤ν¨
- vLLM μ—°κ²° μ‹¤ν¨
- ν¨ν‚¤μ§€ μ„¤μΉ μ‹¤ν¨

---

## π“¦ λ°°ν¬ μ™„λ£ μ²΄ν¬λ¦¬μ¤νΈ

- β… deployment_package λ””λ ‰ν† λ¦¬ μƒμ„±
- β… deploy.sh (λ°°ν¬ μ¤ν¬λ¦½νΈ)
- β… download_wheels_macos.sh (Wheel λ‹¤μ΄λ΅λ“)
- β… setup_offline.sh (μλ™ μ„¤μΉ)
- β… run_all.sh (μ„λΉ„μ¤ μ‹¤ν–‰)
- β… QUICKSTART.md (λΉ λ¥Έ μ‹μ‘)
- β… DEPLOYMENT_GUIDE.md (μƒμ„Έ κ°€μ΄λ“)
- β… README.md (κ°μ”)
- β… requirements.txt (ν¨ν‚¤μ§€ λ©λ΅)
- β… requirements-cuda-12.9.txt (CUDA μ •λ³΄)
- β… wheels/ λ””λ ‰ν† λ¦¬ (μ¤€λΉ„ μ™„λ£)

---

## π‰ λ°°ν¬ μ¤€λΉ„ μ™„λ£!

### λ‹¤μ λ‹¨κ³„

1. **λ΅μ»¬μ—μ„ Wheel λ‹¤μ΄λ΅λ“**
   ```bash
   cd deployment_package
   ./download_wheels_macos.sh
   ```

2. **Linux μ„λ²„λ΅ μ „μ†΅**
   ```bash
   scp -r deployment_package user@server:/home/user/
   ```

3. **μ„λ²„μ—μ„ λ°°ν¬ μ‹¤ν–‰**
   ```bash
   cd /home/user/deployment_package
   ./deploy.sh /opt/stt_engine_venv
   ```

4. **μ†μ¤ μ½”λ“ λ³µμ‚¬ λ° λ¨λΈ μ¤€λΉ„**
   ```bash
   scp -r stt_engine user@server:/opt/
   # λλ” μ›κ²©μ—μ„ λ¨λΈ λ‹¤μ΄λ΅λ“
   ```

5. **μ„λΉ„μ¤ μ‹¤ν–‰**
   ```bash
   python3 api_server.py &
   docker run --gpus all -p 8000:8000 vllm/vllm-openai:latest ...
   ```

---

**ν¨ν‚¤μ§€ λ²„μ „:** 1.0  
**μƒμ„± λ‚ μ§:** 2026-01-30  
**Python:** 3.11.5  
**CUDA:** 12.1/12.9  
**μƒνƒ:** β… **λ°°ν¬ μ¤€λΉ„ μ™„λ£**
