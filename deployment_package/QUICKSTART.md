# STT Engine μ¤ν”„λΌμΈ λ°°ν¬ - λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“

## π“‹ μ „μ²΄ ν”„λ΅μ„Έμ¤

```
λ‹¨κ³„ 1 (λ΅μ»¬ - μΈν„°λ„· μμ)              λ‹¨κ³„ 2 (Linux μ„λ²„ - μΈν„°λ„· μ—†μ)
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”         β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ Wheel λ‹¤μ΄λ΅λ“ (15-30λ¶„)     β”‚         β”‚ λ°°ν¬ (5-10λ¶„)            β”‚
β”‚ β€Ά download_wheels_macos.sh   β”‚ β”€β”€β”€β”€β”€β”€β–Ί β”‚ β€Ά deploy.sh              β”‚
β”‚ β€Ά wheels/ μƒμ„± (2-3GB)       β”‚ (USB)   β”‚ β€Ά venv μƒμ„± λ° μ„¤μ •      β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”         β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                                         β†“
                                        λ‹¨κ³„ 3
                                        β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                                        β”‚ μ‹¤ν–‰ (μ§€μ†μ )            β”‚
                                        β”‚ β€Ά api_server.py          β”‚
                                        β”‚ β€Ά vLLM (Docker)          β”‚
                                        β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

---

## π–¥οΈ μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### λ΅μ»¬ λ¨Έμ‹  (wheel λ‹¤μ΄λ΅λ“)

```bash
# macOS λλ” Linux
β€Ά Python 3.11.x
β€Ά μΈν„°λ„· μ—°κ²° (ν•„μ)
β€Ά 5GB μ΄μƒ μ—¬μ  κ³µκ°„
```

### Linux μ„λ²„ (λ°°ν¬ λ€μƒ)

```bash
# ν™•μΈ λ…λ Ή
python3 --version                    # β†’ Python 3.11.5
nvidia-smi                          # β†’ Driver 575.57.08+
nvidia-smi | grep CUDA              # β†’ CUDA 12.1 λλ” 12.9
```

---

## π€ λΉ λ¥Έ μ‹μ‘ (3λ‹¨κ³„)

### π“ λ‹¨κ³„ 1: λ΅μ»¬μ—μ„ Wheel λ‹¤μ΄λ΅λ“

```bash
# 1. μ΄ λ””λ ‰ν† λ¦¬λ΅ μ΄λ™
cd deployment_package

# 2. μ¤ν¬λ¦½νΈ κ¶ν• μ„¤μ •
chmod +x download_wheels_macos.sh

# 3. μ‹¤ν–‰ (15-30λ¶„ μ†μ”)
./download_wheels_macos.sh

# κ²°κ³Ό: wheels/ λ””λ ‰ν† λ¦¬μ— .whl νμΌ μƒμ„± (2-3GB)
```

**ν™•μΈ:**
```bash
ls -lh wheels/ | head -20        # .whl νμΌλ“¤μ΄ λ³΄μ—¬μ•Ό ν•¨
du -sh wheels/                   # μ•½ 2-3GB
```

### π“ λ‹¨κ³„ 2: Linux μ„λ²„λ΅ μ „μ†΅

```bash
# λ°©λ²• A: scp μ‚¬μ©
scp -r deployment_package user@server:/home/user/

# λ°©λ²• B: USB/λ„¤νΈμ›ν¬ λ“λΌμ΄λΈ
cp -r deployment_package /media/usb/
```

### π“ λ‹¨κ³„ 3: Linux μ„λ²„μ—μ„ λ°°ν¬

```bash
# 1. μ„λ²„λ΅ μ ‘μ†
ssh user@server

# 2. deployment_packageλ΅ μ΄λ™
cd /home/user/deployment_package

# 3. λ°°ν¬ μ¤ν¬λ¦½νΈ μ‹¤ν–‰
chmod +x deploy.sh
./deploy.sh /opt/stt_engine_venv

# λλ” κΈ°λ³Έ κ²½λ΅ μ‚¬μ©
./deploy.sh
# β†’ ~/.venv/stt_engineμ— μ„¤μΉλ¨
```

**μμƒ μ¶λ ¥:**
```
β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”
β… λ°°ν¬ μ™„λ£!
β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”β”

κ°€μƒν™κ²½ κ²½λ΅: /opt/stt_engine_venv

λ‹¤μ λ‹¨κ³„:
  1. STT μ—”μ§„ μ†μ¤μ½”λ“λ¥Ό /opt/stt_engineμΌλ΅ λ³µμ‚¬
  2. λ¨λΈ λ‹¤μ΄λ΅λ“ (μΈν„°λ„· ν•„μ”μ‹)
  3. μ‹¤ν–‰: python3 api_server.py
```

---

## π”§ μ„¤μΉ ν›„ μ„Έν…

### 1. μ†μ¤μ½”λ“ λ³µμ‚¬

```bash
# λ΅μ»¬μ—μ„ (deployment_package μ™Έλ¶€)
scp -r stt_engine user@server:/opt/

# λλ” μ§μ ‘ (μ„λ²„μ—μ„)
git clone <repo> /opt/stt_engine
cd /opt/stt_engine
```

### 2. λ¨λΈ λ‹¤μ΄λ΅λ“

**κ²½μ° A: μ„λ²„κ°€ μΈν„°λ„· μ ‘μ† κ°€λ¥**

```bash
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate

python3 download_model.py
# β†’ μ•½ 20-30λ¶„, 5GB λ‹¤μ΄λ΅λ“
```

**κ²½μ° B: λ΅μ»¬μ—μ„ λ―Έλ¦¬ λ‹¤μ΄λ΅λ“**

```bash
# λ΅μ»¬μ—μ„ (μΈν„°λ„· μμ)
python3 download_model.py

# μ„λ²„λ΅ λ³µμ‚¬
scp -r models/ user@server:/opt/stt_engine/
```

### 3. ν™κ²½ μ„¤μ • (μ„ νƒ)

```bash
cd /opt/stt_engine

# .env νμΌ μƒμ„±
cat > .env << EOF
VLLM_API_URL=http://localhost:8000
VLLM_MODEL_NAME=meta-llama/Llama-2-7b-hf
EOF
```

---

## β–¶οΈ μ‹¤ν–‰

### μµμ… A: ν†µν•© μ¤ν¬λ¦½νΈ (κ¶μ¥)

```bash
# ν„°λ―Έλ„ 1
cd /opt/stt_engine
source /opt/stt_engine_venv/bin/activate
python3 api_server.py

# ν„°λ―Έλ„ 2 (vLLM - Docker ν•„μ”)
docker run --gpus all -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16
```

### μµμ… B: μ»¤μ¤ν…€ ν¬νΈ

```bash
# STT Engine - ν¬νΈ 8002λ΅ μ‹¤ν–‰
python3 api_server.py --port 8002

# vLLM - ν¬νΈ 8001λ΅ μ‹¤ν–‰
docker run --gpus all -p 8001:8000 \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-7b-hf \
  --dtype float16
```

---

## β… κ²€μ¦

### 1. ν¨ν‚¤μ§€ μ„¤μΉ ν™•μΈ

```bash
source /opt/stt_engine_venv/bin/activate

python3 -c "
import torch
import transformers
print('β… PyTorch:', torch.__version__)
print('β… Transformers:', transformers.__version__)
print('β… CUDA Available:', torch.cuda.is_available())
print('β… GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')
"
```

**μ„±κ³µ λ©”μ‹μ§€:**
```
β… PyTorch: 2.1.2
β… Transformers: 4.37.2
β… CUDA Available: True
β… GPU: Tesla V100
```

### 2. API ν—¬μ¤ μ²΄ν¬

```bash
# STT Engine
curl http://localhost:8001/health

# vLLM
curl http://localhost:8000/health
```

### 3. μμ„± λ³€ν™ ν…μ¤νΈ

```bash
# ν΄λΌμ΄μ–ΈνΈλ΅ ν…μ¤νΈ
source /opt/stt_engine_venv/bin/activate
cd /opt/stt_engine

python3 api_client.py --health

python3 api_client.py --transcribe test_audio.wav

python3 api_client.py --process test_audio.wav --instruction "μ”μ•½ν•΄μ£Όμ„Έμ”"
```

---

## π“¦ νμΌ κµ¬μ΅°

```
deployment_package/
β”β”€β”€ wheels/                              # λ¨λ“  .whl νμΌ (2-3GB)
β”‚   β”β”€β”€ torch-2.1.2+cu121-*.whl
β”‚   β”β”€β”€ transformers-4.37.2-*.whl
β”‚   β””β”€β”€ ... (50+ νμΌ)
β”‚
β”β”€β”€ π€ deploy.sh                         # λ©”μΈ λ°°ν¬ μ¤ν¬λ¦½νΈ (Linux)
β”β”€β”€ π“¥ download_wheels_macos.sh          # Wheel λ‹¤μ΄λ΅λ“ (macOS/Linux)
β”β”€β”€ π“¦ setup_offline.sh                  # μλ™ μ„¤μΉ μ¤ν¬λ¦½νΈ
β”β”€β”€ β–¶οΈ  run_all.sh                        # μ„λΉ„μ¤ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚
β”β”€β”€ requirements.txt                     # μμ΅΄μ„± λ©λ΅
β”β”€β”€ requirements-cuda-12.9.txt           # CUDA λ²„μ „ λ…μ‹
β”‚
β”β”€β”€ README.md                            # μƒμ„Έ μ„¤λ…μ„
β”β”€β”€ DEPLOYMENT_GUIDE.md                  # μ „μ²΄ λ°°ν¬ κ°€μ΄λ“
β””β”€β”€ QUICKSTART.md                        # μ΄ νμΌ
```

---

## π† λ¬Έμ  ν•΄κ²°

### CUDA κ΄€λ ¨

```bash
# CUDA κ°€μ©μ„± ν™•μΈ
python3 -c "import torch; print(torch.cuda.is_available())"

# μ„¤μΉλ CUDA λ²„μ „
cat /usr/local/cuda/version.txt

# GPU μ •λ³΄
nvidia-smi
```

### ν¬νΈ μ¶©λ

```bash
# μ‚¬μ© μ¤‘μΈ ν¬νΈ ν™•μΈ
lsof -i :8001
lsof -i :8000

# ν•΄κ²°: λ‹¤λ¥Έ ν¬νΈ μ‚¬μ©
python3 api_server.py --port 8002
```

### ν¨ν‚¤μ§€ λ„λ½

```bash
# μ„¤μΉλ ν¨ν‚¤μ§€ ν™•μΈ
pip list

# νΉμ • ν¨ν‚¤μ§€ μλ™ μ„¤μΉ
pip install --no-index --find-links=deployment_package/wheels <package>
```

λ” λ§μ€ λ¬Έμ  ν•΄κ²°: **[DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…](DEPLOYMENT_GUIDE.md#νΈλ¬λΈ”μν…)**

---

## π“ μμƒ μ‹κ°„

| λ‹¨κ³„ | μ‹κ°„ | λΉ„κ³  |
|------|------|------|
| Wheel λ‹¤μ΄λ΅λ“ | 15-30λ¶„ | μΈν„°λ„· μ†λ„μ— λ”°λΌ |
| μ„λ²„ λ°°ν¬ | 5-10λ¶„ | μ¤ν”„λΌμΈ μ„¤μΉ |
| λ¨λΈ λ‹¤μ΄λ΅λ“ | 20-30λ¶„ | μΈν„°λ„· ν•„μ”, μ„ νƒμ‚¬ν•­ |
| μ΄ μ‹κ°„ | 40-70λ¶„ | - |

---

## π’΅ ν

### λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰

```bash
# nohup μ‚¬μ©
nohup python3 api_server.py > stt.log 2>&1 &

# screen μ‚¬μ©
screen -S stt python3 api_server.py

# systemd μ„λΉ„μ¤ (μλ™ μ‹μ‘)
# λ‚΄μ©μ€ DEPLOYMENT_GUIDE.md μ°Έμ΅°
```

### λ΅κ·Έ ν™•μΈ

```bash
# μ‹¤μ‹κ°„ λ΅κ·Έ
tail -f stt.log

# μ—λ¬λ§ ν•„ν„°λ§
grep ERROR stt.log

# νΉμ • μ‹κ°„λ€
grep "2026-01-30" stt.log
```

### μ„±λ¥ λ¨λ‹ν„°λ§

```bash
# GPU λ¨λ‹ν„°λ§
watch -n 1 nvidia-smi

# λ©”λ¨λ¦¬ μ‚¬μ©
free -h

# λ””μ¤ν¬ μ‚¬μ©
df -h

# ν”„λ΅μ„Έμ¤ λ¨λ‹ν„°λ§
top -p $(pgrep -f api_server.py)
```

---

## π“ λ¬Έμ

λ¬Έμ  λ°μƒ μ‹ μ¤€λΉ„ν•  μ •λ³΄:

```bash
# μ‹μ¤ν… μ •λ³΄
python3 --version
nvidia-smi
nvidia-driver-query

# νμ΄μ¬ ν™κ²½
pip list | grep -E "torch|transformers|fastapi"

# μ—λ¬ λ΅κ·Έ
python3 api_server.py 2>&1 | tail -50
```

---

**μµμΆ… μμ •:** 2026-01-30  
**λ²„μ „:** 1.0  
**μƒνƒ:** β… μ™„μ„±
