#!/usr/bin/env python3
"""
STT Engine ëª¨ë¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ëª©ì :
  - ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ íŒŒì¼ ê²€ì¦
  - íŒŒì¼ í˜•ì‹ í™•ì¸ (safetensors vs model.bin)
  - CPU/GPU í˜¸í™˜ì„± ê²€ì¦
  - ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸

ì‚¬ìš©:
  conda activate stt-py311
  python validate_model_detailed.py
"""

import os
import sys
from pathlib import Path
import json

print("=" * 70)
print("ğŸ“‹ STT Engine ëª¨ë¸ ìƒì„¸ ê²€ì¦")
print("=" * 70)

BASE_DIR = Path(__file__).parent.absolute()
models_dir = BASE_DIR / "models"

# 1ë‹¨ê³„: íŒŒì¼ êµ¬ì¡° í™•ì¸
print("\n1ï¸âƒ£  íŒŒì¼ êµ¬ì¡° í™•ì¸")
print("-" * 70)

if not models_dir.exists():
    print("âŒ models/ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
    sys.exit(1)

print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {models_dir}")
print(f"ğŸ’¾ ì „ì²´ í¬ê¸°: {sum(f.stat().st_size for f in models_dir.rglob('*') if f.is_file()) / (1024**3):.2f}GB")

# íŒŒì¼ ëª©ë¡
files = {}
for file_path in sorted(models_dir.glob('*')):
    if file_path.is_file():
        size_mb = file_path.stat().st_size / (1024**2)
        files[file_path.name] = size_mb
        print(f"  âœ“ {file_path.name:40s} {size_mb:8.2f}MB")

print(f"\nâœ“ ì´ íŒŒì¼ ìˆ˜: {len(files)}")

# 2ë‹¨ê³„: íŒŒì¼ í˜•ì‹ ë¶„ì„
print("\n2ï¸âƒ£  íŒŒì¼ í˜•ì‹ ë¶„ì„")
print("-" * 70)

# model.safetensors í™•ì¸
if (models_dir / "model.safetensors").exists():
    print("âœ“ model.safetensors ë°œê²¬ (HuggingFace ì›ë³¸ í˜•ì‹)")
    print("  - í˜•ì‹: SafeTensors (PyTorch í‘œì¤€)")
    print("  - í˜¸í™˜ì„±: faster-whisper, transformers, ëª¨ë“  PyTorch ê¸°ë°˜ ë„êµ¬")
    print("  - ì¥ì : ì•ˆì „í•œ ì§ë ¬í™”, ë¹ ë¥¸ ë¡œë“œ")
    
    model_size = (models_dir / "model.safetensors").stat().st_size / (1024**3)
    print(f"  - í¬ê¸°: {model_size:.2f}GB")
elif (models_dir / "model.bin").exists():
    print("âœ— model.bin ë°œê²¬ (PyTorch pickle í˜•ì‹)")
    print("  - í˜•ì‹: PyTorch Pickle")
    print("  - í˜¸í™˜ì„±: PyTorch ë„êµ¬ë“¤")
else:
    print("âš ï¸  ëª¨ë¸ íŒŒì¼ ë°œê²¬ ì•ˆ ë¨ (model.safetensors ë˜ëŠ” model.bin)")

# 3ë‹¨ê³„: ì„¤ì • íŒŒì¼ ê²€ì¦
print("\n3ï¸âƒ£  ì„¤ì • íŒŒì¼ ê²€ì¦")
print("-" * 70)

required_configs = {
    "config.json": "ëª¨ë¸ êµ¬ì„±",
    "generation_config.json": "ìƒì„± ì„¤ì •",
    "preprocessor_config.json": "ì „ì²˜ë¦¬ ì„¤ì •",
    "tokenizer.json": "í† í¬ë‚˜ì´ì €"
}

for config_file, description in required_configs.items():
    path = models_dir / config_file
    if path.exists():
        print(f"âœ“ {config_file:40s} ({description})")
    else:
        print(f"âœ— {config_file:40s} ({description}) - MISSING")

# config.json ë‚´ìš© í™•ì¸
try:
    with open(models_dir / "config.json", "r") as f:
        config = json.load(f)
        print(f"\nâœ“ ëª¨ë¸ ì •ë³´:")
        print(f"  - Architecture: {config.get('architectures', ['Unknown'])[0]}")
        print(f"  - Model Type: {config.get('model_type', 'Unknown')}")
        print(f"  - Hidden Size: {config.get('d_model', 'Unknown')}")
except Exception as e:
    print(f"âš ï¸  config.json ì½ê¸° ì‹¤íŒ¨: {e}")

# 4ë‹¨ê³„: CPU/GPU í˜¸í™˜ì„± ê²€ì¦
print("\n4ï¸âƒ£  CPU/GPU í˜¸í™˜ì„± ê²€ì¦")
print("-" * 70)

try:
    import torch
    print(f"âœ“ PyTorch ë²„ì „: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"âœ“ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
        print(f"  - CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  - cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
    else:
        print(f"âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œë§Œ ê°€ëŠ¥)")
    
    print(f"âœ“ CPU í˜¸í™˜ì„±: í•­ìƒ ì§€ì›")
    
except ImportError:
    print("âš ï¸  PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

# 5ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
print("\n5ï¸âƒ£  ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("-" * 70)

try:
    from faster_whisper import WhisperModel
    
    print("â³ faster-whisperë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # CPUë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
    print("  1. CPU í…ŒìŠ¤íŠ¸...")
    try:
        model_cpu = WhisperModel(
            str(models_dir),
            device="cpu",
            compute_type="int8",
            local_files_only=True
        )
        print("  âœ“ CPU ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"  âœ— CPU ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # GPU í…ŒìŠ¤íŠ¸ (CUDA ì‚¬ìš© ê°€ëŠ¥ì‹œ)
    if torch.cuda.is_available():
        print("  2. GPU í…ŒìŠ¤íŠ¸...")
        try:
            model_gpu = WhisperModel(
                str(models_dir),
                device="cuda",
                compute_type="int8",
                local_files_only=True
            )
            print("  âœ“ GPU ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"  âœ— GPU ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print("  2. GPU í…ŒìŠ¤íŠ¸: CUDA ì‚¬ìš© ë¶ˆê°€ (ìŠ¤í‚µ)")
    
    print("âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
except ImportError:
    print("âš ï¸  faster-whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()

# 6ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥ì„± ìš”ì•½
print("\n6ï¸âƒ£  ì‚¬ìš© ê°€ëŠ¥ì„± ìš”ì•½")
print("=" * 70)

print("""
âœ“ ëª¨ë¸ í˜•ì‹: HuggingFace SafeTensors (PyTorch í‘œì¤€)
âœ“ í˜¸í™˜ì„±: CPU âœ“ GPU âœ“ ë‘˜ ë‹¤ ì§€ì›
âœ“ í¬ê¸°: 1.5GB (ì›ë³¸ ëª¨ë¸ í¬ê¸° - ì •ìƒ)
âœ“ ì˜¤í”„ë¼ì¸: local_files_only=Trueë¡œ ì‚¬ìš© ê°€ëŠ¥

âš™ï¸  ì‚¬ìš© ë°©ë²•:

# CPU ëª¨ë“œ
model = WhisperModel("/path/to/models", device="cpu", compute_type="int8")

# GPU ëª¨ë“œ
model = WhisperModel("/path/to/models", device="cuda", compute_type="int8")

ğŸ’¡ í¬ê¸° ì„¤ëª…:
  - 400MB: CTranslate2 í¬ë§· (ì»´íŒŒì¼ëœ í˜•ì‹, ìµœì í™”ë¨)
  - 1.5GB: HuggingFace SafeTensors (ì›ë³¸ í˜•ì‹, ìœ ì—°í•¨)
  
ë‘ í˜•ì‹ ëª¨ë‘ ì‘ë™í•˜ë©°, í˜„ì¬ëŠ” HuggingFace ì›ë³¸ í˜•ì‹ì´ë¯€ë¡œ
ë” ë„“ì€ í˜¸í™˜ì„±ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
""")

print("=" * 70)
print("âœ… ê²€ì¦ ì™„ë£Œ!")
print("=" * 70)
