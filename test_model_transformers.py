#!/usr/bin/env python3
"""
transformersë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ê²€ì¦
"""
import sys
from pathlib import Path

print("ğŸ” transformersë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ê²€ì¦")
print("=" * 60)

models_dir = Path("/Users/a113211/workspace/stt_engine/models")

try:
    from transformers import WhisperForConditionalGeneration, WhisperProcessor
    import torch
    
    print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {models_dir}")
    print()
    
    print("1ï¸âƒ£ WhisperProcessor ë¡œë“œ ì¤‘...")
    processor = WhisperProcessor.from_pretrained(
        "openai/whisper-large-v3-turbo",
        cache_dir=str(models_dir),
        local_files_only=True
    )
    print("âœ… Processor ë¡œë“œ ì™„ë£Œ")
    print(f"   - Sample Rate: {processor.feature_extractor.sampling_rate}")
    
    print("\n2ï¸âƒ£ WhisperForConditionalGeneration ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = WhisperForConditionalGeneration.from_pretrained(
        "openai/whisper-large-v3-turbo",
        cache_dir=str(models_dir),
        local_files_only=True,
        torch_dtype=torch.float16,
        device_map="cpu"
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   - Model Type: {type(model).__name__}")
    print(f"   - Device: CPU")
    print(f"   - Dtype: float16")
    
    print("\n3ï¸âƒ£ ëª¨ë¸ ì •ë³´")
    print("-" * 60)
    print(f"   - ëª¨ë¸ í¬ê¸°: {sum(p.numel() for p in model.parameters()) / 1e6:.0f}M parameters")
    print(f"   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.0f}M")
    
    print("\nâœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!")
    print("ğŸš€ HuggingFace ëª¨ë¸ í˜•ì‹ìœ¼ë¡œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    
    # ëª¨ë¸ summary
    print("\n4ï¸âƒ£ ëª¨ë¸ êµ¬ì¡°")
    print("-" * 60)
    print(f"   Encoder: {model.encoder}")
    print(f"   Decoder: {model.decoder}")
    
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜: {e}")
    sys.exit(1)
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
