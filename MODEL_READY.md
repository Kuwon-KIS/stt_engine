# Whisper Large-V3-Turbo ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ âœ…

## ğŸ“Œ ëª¨ë¸.bin ì—†ìŒì— ëŒ€í•´ (ì¤‘ìš”!)

### ìƒí™© ì„¤ëª…
- âŒ `model.bin` íŒŒì¼ì´ ì—†ìŒ (ì´ìƒ ì•„ë‹˜)
- âœ… `model.safetensors` íŒŒì¼ì´ ìˆìŒ (ì •ìƒ)

### ì™œ ì´ë ‡ê²Œ ë˜ë‚˜?
Whisper v3ë¶€í„°ëŠ” **PyTorch ìµœì‹  í‘œì¤€ í¬ë§·**ì¸ `SafeTensors`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì´ì „ ëª¨ë¸: `model.bin` (PyTorch êµ¬ì‹ í¬ë§·)
- **ìµœì‹  ëª¨ë¸**: `model.safetensors` (ì•ˆì „í•˜ê³  ë¹ ë¥¸ í¬ë§·)

### âœ… í†µê³¼ ê°€ëŠ¥í•œê°€?
**YES!** - ë‹¤ìŒ ë°©ì‹ìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤:

#### ë°©ë²• 1ï¸âƒ£: Docker ì»¨í…Œì´ë„ˆì—ì„œ ìë™ ë³€í™˜ (ê¶Œì¥)
```bash
bash /Users/a113211/workspace/stt_engine/run-docker-gpu.sh
```
- Docker ë‚´ë¶€ì—ì„œ `faster_whisper`ê°€ ìë™ìœ¼ë¡œ `model.safetensors`ë¥¼ ë³€í™˜
- ì²˜ìŒ ì‹¤í–‰ ì‹œë§Œ 2-3ë¶„ ì†Œìš” (ìºì‹œ)
- ì´í›„ ì¦‰ì‹œ ì‹¤í–‰

#### ë°©ë²• 2ï¸âƒ£: HuggingFace í˜•ì‹ìœ¼ë¡œ ì§ì ‘ ë¡œë“œ
```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor

processor = WhisperProcessor.from_pretrained(
    "openai/whisper-large-v3-turbo",
    cache_dir="/path/to/models"
)
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3-turbo", 
    cache_dir="/path/to/models"
)
```

---

## âœ… ëª¨ë¸ ê²€ì¦ ê²°ê³¼

### 1. íŒŒì¼ ê²€ì¦
- âœ… `model.safetensors` (1.32 GB) - ëª¨ë¸ ê°€ì¤‘ì¹˜
- âœ… `config.json` - ëª¨ë¸ ì„¤ì •
- âœ… `preprocessor_config.json` - ì „ì²˜ë¦¬ ì„¤ì •
- âœ… `tokenizer.json` (2.6 MB) - í† í¬ë‚˜ì´ì €
- âœ… `tokenizer_config.json` - í† í¬ë‚˜ì´ì € ì„¤ì •
- âœ… `vocab.json` (1.0 MB) - ë‹¨ì–´ ì‚¬ì „
- âœ… `generation_config.json` - ìƒì„± ì„¤ì •

### 2. ì„¤ì • ê²€ì¦
```
Architecture: WhisperForConditionalGeneration
Model Type: whisper
Vocab Size: 51,866
Feature Extractor: WhisperFeatureExtractor
Sample Rate: 16,000 Hz
```

### 3. êµ¬ì¡° ê²€ì¦
```
âœ… HuggingFace ìºì‹œ êµ¬ì¡° ì •ìƒ
   models--openai--whisper-large-v3-turbo/
   â”œâ”€â”€ blobs/        (ëª¨ë¸ ê°€ì¤‘ì¹˜)
   â””â”€â”€ snapshots/    (ë©”íƒ€ë°ì´í„°)
```

---

## ğŸ“¦ ì••ì¶• íŒŒì¼

### ìƒì„± ì •ë³´
```
whisper-large-v3-turbo-models.tar.gz
â”œâ”€ ì›ë³¸ í¬ê¸°: 1.29 GB
â””â”€ ì••ì¶• í¬ê¸°: 392 MB (70% ì••ì¶•ìœ¨)
```

### êµ¬ì¡°
```
models/
â”œâ”€â”€ model.safetensors                          (1.32 GB)
â”œâ”€â”€ config.json
â”œâ”€â”€ preprocessor_config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ vocab.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ merges.txt
â””â”€â”€ models--openai--whisper-large-v3-turbo/   (ìºì‹œ)
    â”œâ”€â”€ blobs/
    â””â”€â”€ snapshots/
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### âœ… ë°©ë²• 1: ë¡œì»¬ ë§ˆìš´íŠ¸ (ê¶Œì¥)
```bash
# ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
bash run-docker-gpu.sh

# ë˜ëŠ” ìˆ˜ë™ ì‹¤í–‰
docker run -d \
  --name stt-engine-gpu \
  --gpus all \
  -p 8003:8003 \
  -v /Users/a113211/workspace/stt_engine/models:/app/models \
  -e STT_DEVICE=cuda \
  stt-engine:cuda129-v1.0
```

### âœ… ë°©ë²• 2: tar.gzë¡œ ì „ì†¡
```bash
# 1. ë¡œì»¬ì—ì„œ ì••ì¶•
tar -czf whisper-large-v3-turbo-models.tar.gz models/

# 2. ì„œë²„ë¡œ ì „ì†¡
scp whisper-large-v3-turbo-models.tar.gz user@server:/app/

# 3. ì„œë²„ì—ì„œ ì••ì¶• í•´ì œ
tar -xzf whisper-large-v3-turbo-models.tar.gz
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í—¬ìŠ¤ ì²´í¬
```bash
curl http://localhost:8003/health
```

### STT API í…ŒìŠ¤íŠ¸
```bash
# ìŒì„± íŒŒì¼ ì—…ë¡œë“œ
curl -X POST http://localhost:8003/transcribe \
  -F "file=@sample.wav"

# ì‘ë‹µ ì˜ˆì‹œ
{
  "text": "Hello, this is a test.",
  "language": "en",
  "duration": 2.5
}
```

---

## ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´

| í•­ëª© | ì •ë³´ |
|------|------|
| **Python ë²„ì „** | 3.11.14 (conda í™˜ê²½: `stt-py311`) |
| **faster_whisper** | 1.0.3 |
| **PyTorch** | 2.1.2 |
| **CUDA** | 12.4 í˜¸í™˜ì„± |
| **GPU ì§€ì›** | NVIDIA GPU (--gpus all) |
| **ëª¨ë¸** | openai/whisper-large-v3-turbo |
| **ëª¨ë¸ í¬ê¸°** | 1.29 GB (ì••ì¶• 392 MB) |
| **í† í¬ë‚˜ì´ì €** | 51,866 ë‹¨ì–´ ì–´íœ˜ |

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì‹œ
1. **GPU ì§€ì› í•„ìˆ˜**: `--gpus all` í”Œë˜ê·¸ í•„ìˆ˜
2. **ëª¨ë¸ ì´ˆê¸°í™”**: ì²« ì‹¤í–‰ ì‹œ 2-3ë¶„ ì†Œìš” (ëª¨ë¸ ë³€í™˜)
3. **ë©”ëª¨ë¦¬**: ìµœì†Œ 4GB GPU ë©”ëª¨ë¦¬ ê¶Œì¥
4. **ë„¤íŠ¸ì›Œí¬**: ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œëŠ” ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ë¶ˆê°€ (ì´ë¯¸ ì¤€ë¹„ë¨)

### ëª¨ë¸ ë³€í™˜
- `faster_whisper`ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ctranslate2 í¬ë§·ìœ¼ë¡œ ìë™ ë³€í™˜
- ì²« ì‹¤í–‰ í›„ ìºì‹œë˜ì–´ ë‹¤ìŒë¶€í„° ë¹ ë¦„
- ìˆ˜ë™ ë³€í™˜ ë¶ˆí•„ìš”

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
- [x] ëª¨ë¸ íŒŒì¼ ê²€ì¦
- [x] íŒŒì¼ êµ¬ì¡° í™•ì¸
- [x] tar.gz ì••ì¶•
- [x] í™˜ê²½ ì„¤ì • (conda stt-py311)
- [x] Docker ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
- [x] ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Docker ì‹¤í–‰**
   ```bash
   bash run-docker-gpu.sh
   ```

2. **ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ í™•ì¸** (ì•½ 3ë¶„ ëŒ€ê¸°)
   ```bash
   docker logs stt-engine-gpu | grep -i success
   ```

3. **í—¬ìŠ¤ ì²´í¬**
   ```bash
   curl http://localhost:8003/health
   ```

4. **STT API í…ŒìŠ¤íŠ¸**
   ```bash
   curl -X POST http://localhost:8003/transcribe -F "file=@audio.wav"
   ```

---
**ìƒì„±ì¼**: 2026-02-03  
**ìƒíƒœ**: âœ… ëª¨ë“  ê²€ì¦ ì™„ë£Œ, Docker ë°˜ì… ì¤€ë¹„ ì™„ë£Œ

