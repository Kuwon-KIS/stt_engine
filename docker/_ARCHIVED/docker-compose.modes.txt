# STT Engine Docker Compose - 3가지 모드

## 모드 1: 기본 (모델 폴더 마운트)
version: '3.8'

services:
  stt-engine:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: stt-engine:latest
    container_name: stt-engine
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models              # 로컬 모델 폴더 마운트
      - ./audio:/app/audio                # 음성 파일
      - ./logs:/app/logs                  # 로그
    environment:
      - WHISPER_DEVICE=cuda               # GPU: cuda / CPU: cpu
      - VLLM_API_URL=http://vllm-server:8000
    depends_on:
      - vllm-server
    networks:
      - stt-network
    restart: unless-stopped

  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    ports:
      - "8000:8000"
    volumes:
      - vllm-cache:/root/.cache
    environment:
      - MODEL_NAME=meta-llama/Llama-2-7b-hf
      - CUDA_VISIBLE_DEVICES=0
    command: vllm serve meta-llama/Llama-2-7b-hf --host 0.0.0.0 --port 8000
    networks:
      - stt-network
    restart: unless-stopped

volumes:
  vllm-cache:

networks:
  stt-network:
    driver: bridge

---

## 모드 2: 압축 모델 자동 해제
version: '3.8'

services:
  stt-engine:
    build:
      context: .
      dockerfile: Dockerfile.compressed  # 압축 Dockerfile 사용
    image: stt-engine:compressed
    container_name: stt-engine
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models              # 압축 파일 포함
      - ./audio:/app/audio
      - ./logs:/app/logs
    environment:
      - WHISPER_DEVICE=cuda
      - VLLM_API_URL=http://vllm-server:8000
    depends_on:
      - vllm-server
    networks:
      - stt-network
    restart: unless-stopped

  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    ports:
      - "8000:8000"
    volumes:
      - vllm-cache:/root/.cache
    environment:
      - MODEL_NAME=meta-llama/Llama-2-7b-hf
    command: vllm serve meta-llama/Llama-2-7b-hf --host 0.0.0.0 --port 8000
    networks:
      - stt-network
    restart: unless-stopped

volumes:
  vllm-cache:

networks:
  stt-network:
    driver: bridge

---

## 모드 3: S3에서 자동 다운로드 (선택사항)
version: '3.8'

services:
  stt-engine:
    build:
      context: .
      dockerfile: Dockerfile.s3  # S3 Dockerfile (아래 참고)
    image: stt-engine:s3-enabled
    container_name: stt-engine
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
      - ./audio:/app/audio
      - ./logs:/app/logs
    environment:
      - WHISPER_DEVICE=cuda
      - VLLM_API_URL=http://vllm-server:8000
      # S3 설정 (환경 변수에서 로드)
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET=${S3_BUCKET:-stt-models}
      - S3_MODEL_KEY=${S3_MODEL_KEY:-whisper-model.tar.gz}
    depends_on:
      - vllm-server
    networks:
      - stt-network
    restart: unless-stopped

  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    ports:
      - "8000:8000"
    volumes:
      - vllm-cache:/root/.cache
    environment:
      - MODEL_NAME=meta-llama/Llama-2-7b-hf
    command: vllm serve meta-llama/Llama-2-7b-hf --host 0.0.0.0 --port 8000
    networks:
      - stt-network
    restart: unless-stopped

volumes:
  vllm-cache:

networks:
  stt-network:
    driver: bridge
