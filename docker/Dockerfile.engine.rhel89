# ============================================================================
# STT Engine - RHEL 8.9 Optimized with CUDA 12.9 + cuDNN
# 
# Build Platform: RHEL 8.9 AMI EC2
# Build Environment: Online (requires internet)
# Target Server: RHEL 8.9 with CUDA 12.9 + NVIDIA Driver 575.57.08
# Base Image: ubi8/python-311 (Red Hat Universal Base Image)
# glibc: 2.28 (RHEL 8.9 ê¸°ë³¸ê°’ - í˜¸í™˜ì„± 100%)
# cuDNN: wheel íŒ¨í‚¤ì§€ ì„¤ì¹˜
# Final Size: ~1.5GB (compressed tar.gz ~500MB)
#
# ë¹Œë“œ ëª…ë ¹ì–´:
#   docker build --platform linux/amd64 -t stt-engine:cuda129-rhel89-v1.2 -f docker/Dockerfile.engine.rhel89 .
# ============================================================================

FROM --platform=linux/amd64 registry.access.redhat.com/ubi8/python-311:latest

LABEL maintainer="STT Engine Team"
LABEL description="STT Engine RHEL 8.9 Optimized with CUDA 12.9"
LABEL os="RHEL 8.9"
LABEL glibc="2.28"
LABEL cuda.version="12.9"
LABEL cudnn.version="9.0.0.312"
LABEL pytorch.version="2.6.0"

# ğŸ’¡ ì¤‘ìš”: UBI 8 ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ìœ¼ë¡œ non-root ì‚¬ìš©ìë¡œ ì„¤ì •ë¨
# FROM ë°”ë¡œ ë‹¤ìŒì— rootë¡œ ì „í™˜í•˜ì—¬ ì´í›„ ëª¨ë“  RUN ë¸”ë¡ì´ root ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰ë˜ë„ë¡ í•¨
USER root

# ============================================================================
# Step 1: Subscription Manager ì™„ì „ ë¹„í™œì„±í™” + Rootless ì‚¬ìš©ì ìƒì„±
# ============================================================================

# ğŸ’¡ ëª¨ë“  root ê¶Œí•œ ì‘ì—…ì„ í•œ RUN ë¸”ë¡ì—ì„œ ìˆ˜í–‰ (ê¶Œí•œ ë¬¸ì œ ë°©ì§€)
RUN rm -rf /etc/rhsm /etc/pki/entitlement* 2>/dev/null || true && \
    subscription-manager clean 2>/dev/null || true && \
    yum clean all 2>/dev/null || true && \
    rm -rf /var/cache/yum/* && \
    # Non-root ì‚¬ìš©ì ìƒì„± (ì¡°ê±´ë¶€: ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ UID 2000ìœ¼ë¡œ ìƒì„±)
    (id stt-user >/dev/null 2>&1 || useradd -m -u 2000 -s /bin/bash stt-user) && \
    # sudo ê¶Œí•œ ë¶€ì—¬
    mkdir -p /etc/sudoers.d && \
    echo "stt-user ALL=(ALL) NOPASSWD: /usr/bin/yum" >> /etc/sudoers.d/stt-user && \
    chmod 0440 /etc/sudoers.d/stt-user

# ============================================================================
# Step 2: ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜ (ì•„ì§ rootë¡œ ì‹¤í–‰ ì¤‘)
# ============================================================================

RUN yum install -y \
    # CA ì¸ì¦ì„œ (SSL/TLS)
    ca-certificates \
    openssl \
    curl \
    wget \
    \
    # Audio/Media ì²˜ë¦¬ (Python librosaë¡œ ëŒ€ì²´)
    libsndfile \
    \
    # ê°œë°œ ë„êµ¬
    make \
    gcc \
    gcc-c++ \
    && yum clean all \
    && rm -rf /var/cache/yum/*

# ============================================================================
# Step 3: ì• í”Œë¦¬ì¼€ì´ì…˜ ë””ë ‰í† ë¦¬ ìƒì„± (rootë¡œ ì‹¤í–‰)
# ============================================================================

WORKDIR /app

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
RUN mkdir -p /app/models /app/logs /app/audio && \
    chmod -R 755 /app

# ============================================================================
# Step 4: cuDNN wheel ì„¤ì¹˜ (nvidia-cudnn-cu12) - ROOTë¡œ ì‹¤í–‰
# 
# RHEL 8.9ì—ì„œ pipë¥¼ í†µí•œ cuDNN ì„¤ì¹˜
# ë²„ì „: 9.0.0.312 (CUDA 12.x í˜¸í™˜)
# ============================================================================

RUN python3.11 -m pip install --upgrade nvidia-cudnn-cu12==9.0.0.312 && \
    ln -sf /opt/app-root/lib/python3.11/site-packages/nvidia/cudnn/lib/libcudnn.so.8 \
           /opt/app-root/lib/python3.11/site-packages/nvidia/cudnn/lib/libcudnn.so.9 && \
    ln -sf /opt/app-root/lib64/python3.11/site-packages/nvidia/cudnn/lib/libcudnn.so.8 \
           /opt/app-root/lib64/python3.11/site-packages/nvidia/cudnn/lib/libcudnn.so.9

# ============================================================================
# Step 6: Python íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ì—…ê·¸ë ˆì´ë“œ - ROOTë¡œ ì‹¤í–‰
# ============================================================================

RUN python3.11 -m pip install --upgrade \
    --trusted-host pypi.org \
    --trusted-host files.pythonhosted.org \
    pip setuptools wheel

# ============================================================================
# Step 7: Whisper ë° ì˜ì¡´ì„± ì„¤ì¹˜ (PyTorch ì œì™¸) - ROOTë¡œ ì‹¤í–‰
# 
# RHEL 8.9 í˜¸í™˜ ë²„ì „ ì‚¬ìš©:
# - faster-whisper: CTranslate2 ê¸°ë°˜ ë¹ ë¥¸ ì¶”ë¡ 
# - openai-whisper: fallback (ëŠë¦¼)
# ============================================================================

RUN python3.11 -m pip install --trusted-host files.pythonhosted.org \
    # Whisper implementations
    faster-whisper==1.2.1 \
    ctranslate2==4.7.1 \
    openai-whisper==20231117 \
    \
    # Audio processing
    librosa==0.10.0 \
    scipy==1.12.0 \
    numpy==1.24.3 \
    \
    # Model management & transformers (flexible versions for compatibility)
    huggingface-hub>=0.21 \
    "transformers>=4.30,<6" \
    \
    # Web framework
    fastapi==0.109.0 \
    uvicorn==0.27.0 \
    requests==2.31.0 \
    pydantic==2.5.3 \
    python-multipart==0.0.6 \
    \
    # Configuration
    python-dotenv==1.0.0 \
    pyyaml==6.0.1

# ============================================================================
# Step 8: PyTorch ì„¤ì¹˜ ë° CUDA ëŸ°íƒ€ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ - ROOTë¡œ ì‹¤í–‰
# 
# CUDA 12.4 ê³µì‹ ì§€ì› ë²„ì „ ì‚¬ìš©
# RHEL 8.9ì˜ CUDA 12.9 Runtimeê³¼ forward compatible
# torchaudioë„ í¬í•¨ (whisperì—ì„œ í•„ìˆ˜)
#
# ì¤‘ìš”: --no-depsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!
# PyTorchì˜ CUDA ì˜ì¡´ì„±(nvidia-cublas-cu12, nvidia-cusparse-cu12 ë“±)ì´
# ìë™ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ì•¼ libcusparseLt.so.0 ë“± CUDA ëŸ°íƒ€ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì œê³µë¨
# ============================================================================

RUN python3.11 -m pip install \
    --index-url https://download.pytorch.org/whl/cu124 \
    torch==2.6.0 \
    torchaudio==2.6.0

# Install CUDA runtime libraries explicitly
# These packages provide the *.so files needed for CUDA at runtime
# Especially libcusparseLt.so.0 from nvidia-cusparselt-cu12
RUN python3.11 -m pip install \
    nvidia-cublas-cu12 \
    nvidia-cusparse-cu12 \
    nvidia-cusparselt-cu12 \
    nvidia-cuda-runtime-cu12 \
    nvidia-cuda-cupti-cu12

# ============================================================================
# Step 8a: CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ìœ„ì¹˜ íƒìƒ‰ ë° ê²½ë¡œ ë“±ë¡
# ============================================================================
# PyTorchì˜ nvidia-* ì˜ì¡´ì„± ì„¤ì¹˜ë¡œ ì¸í•´ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—¬ëŸ¬ ìœ„ì¹˜ì— ì¡´ì¬í•  ìˆ˜ ìˆìŒ
# ëª¨ë“  ê°€ëŠ¥í•œ ìœ„ì¹˜ë¥¼ ì°¾ì•„ì„œ /usr/local/libì— ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
# ============================================================================

# Create ldconfig config file with ALL CUDA library paths
RUN cat > /etc/ld.so.conf.d/torch-cuda.conf << 'EOF'
/opt/app-root/lib/python3.11/site-packages/nvidia/cudnn/lib
/opt/app-root/lib/python3.11/site-packages/nvidia/cusparse/lib
/opt/app-root/lib/python3.11/site-packages/nvidia/cusparselt/lib
/opt/app-root/lib/python3.11/site-packages/nvidia/cublas/lib
/opt/app-root/lib/python3.11/site-packages/nvidia/cuda_runtime/lib
/opt/app-root/lib/python3.11/site-packages/cusparselt/lib
/opt/app-root/lib/python3.11/site-packages/torch/lib
/usr/local/lib
/usr/local/lib64
EOF

# Find and link CUDA libraries from nvidia/* and cusparselt/lib, rebuild ldconfig cache
RUN mkdir -p /usr/local/lib /usr/local/lib64 && \
    find /opt/app-root -path "*/site-packages/nvidia/*" -name "*.so*" -type f 2>/dev/null | while read lib; do \
        ln -sf "$lib" "/usr/local/lib/$(basename "$lib")" 2>/dev/null || true; \
        ln -sf "$lib" "/usr/local/lib64/$(basename "$lib")" 2>/dev/null || true; \
    done && \
    find /opt/app-root -path "*/site-packages/cusparselt/*" -name "*.so*" -type f 2>/dev/null | while read lib; do \
        ln -sf "$lib" "/usr/local/lib/$(basename "$lib")" 2>/dev/null || true; \
        ln -sf "$lib" "/usr/local/lib64/$(basename "$lib")" 2>/dev/null || true; \
    done && \
    find /opt/app-root -path "*/site-packages/torch/lib/*" -name "*.so*" -type f 2>/dev/null | while read lib; do \
        ln -sf "$lib" "/usr/local/lib/$(basename "$lib")" 2>/dev/null || true; \
        ln -sf "$lib" "/usr/local/lib64/$(basename "$lib")" 2>/dev/null || true; \
    done && \
    ldconfig && \
    echo "=== CUDA Libraries Found ===" && \
    ldconfig -p | grep -E "libcusparseLt|libcublas|libcusparse|libcudart|libcudnn" | sort -u | head -30

# Set LD_LIBRARY_PATH for CUDA library discovery at runtime
# Include all paths where CUDA libraries are installed
ENV LD_LIBRARY_PATH="/opt/app-root/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/app-root/lib/python3.11/site-packages/nvidia/cusparse/lib:/opt/app-root/lib/python3.11/site-packages/nvidia/cusparselt/lib:/opt/app-root/lib/python3.11/site-packages/nvidia/cublas/lib:/opt/app-root/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:/opt/app-root/lib/python3.11/site-packages/cusparselt/lib:/opt/app-root/lib/python3.11/site-packages/torch/lib:/usr/local/lib:/usr/local/lib64"

# ============================================================================
# Step 9: ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦ (ë¹Œë“œ ì‹œ í™•ì¸)
# ============================================================================
# Verify that critical CUDA libraries are discoverable
RUN python3.11 << 'EOF'
import ctypes
import os

print("\n=== CUDA Library Verification ===")

# Critical libraries that PyTorch needs
libs_to_find = [
    'libcusparseLt.so.0',
    'libcublas.so.12',
    'libcusparse.so.12',
    'libcudart.so.12',
    'libcudnn.so.9',
]

# Try to load each library
for lib_name in libs_to_find:
    try:
        lib = ctypes.CDLL(lib_name)
        print(f"âœ… {lib_name}: FOUND")
    except OSError as e:
        print(f"âŒ {lib_name}: NOT FOUND - {e}")

print("\n=== ldconfig cache ===")
os.system("ldconfig -p | grep -E 'libcusparseLt|libcudnn|libcublas' | head -10")

print("\n=== Library paths ===")
os.system("find /opt/app-root -name 'libcusparseLt*' -o -name 'libcudnn.so*' 2>/dev/null")
os.system("find /usr/local -name 'libcusparseLt*' 2>/dev/null")

EOF

# ============================================================================
# Step 10: ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì • (ROOTë¡œ ì‹¤í–‰)
# 
# ì£¼ì˜: USER ì „í™˜ BEFORE ëª¨ë“  ë””ë ‰í† ë¦¬/ê¶Œí•œ ì„¤ì • ì™„ë£Œ
# ROOT ê¶Œí•œì´ í•„ìš”í•œ mkdir/chownì€ USER ì „í™˜ ì „ì— ì™„ë£Œí•´ì•¼ í•¨
# ============================================================================

RUN mkdir -p /opt/app-root/src/.cache && \
    mkdir -p /app/models && \
    mkdir -p /app/logs && \
    mkdir -p /app/audio && \
    chown -R stt-user:stt-user /opt/app-root/src/.cache && \
    chown -R stt-user:stt-user /app/models && \
    chown -R stt-user:stt-user /app/logs && \
    chown -R stt-user:stt-user /app/audio && \
    chmod -R 755 /opt/app-root/src/.cache && \
    chmod -R 755 /app/models && \
    chmod -R 755 /app/logs && \
    chmod -R 755 /app/audio && \
    chmod 755 /app

USER stt-user

# ============================================================================
# Step 11: ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒì¼ ë³µì‚¬ (stt-user ê¶Œí•œ)
# ============================================================================

# ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒì¼ ë³µì‚¬
COPY --chown=stt-user:stt-user main.py /app/
COPY --chown=stt-user:stt-user api_server.py /app/
COPY --chown=stt-user:stt-user stt_engine.py /app/
COPY --chown=stt-user:stt-user requirements.txt /app/

# Python í™˜ê²½ ë³€ìˆ˜
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/app/models
ENV STT_DEVICE=cpu

# ============================================================================
# Step 12: í—¬ìŠ¤ ì²´í¬ ë° í¬íŠ¸ ë…¸ì¶œ
# ============================================================================

# Health check: 30ì´ˆë§ˆë‹¤ ì²´í¬, 10ì´ˆ íƒ€ì„ì•„ì›ƒ, 3ë²ˆ ì‹¤íŒ¨ ì‹œ unhealthy
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

# STT API í¬íŠ¸
EXPOSE 8003

# ============================================================================
# Step 13: ì‹œì‘ ëª…ë ¹ì–´
# ============================================================================

CMD ["python3.11", "main.py"]
