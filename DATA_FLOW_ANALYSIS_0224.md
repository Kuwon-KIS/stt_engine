# STT Engine Web UI - Complete Data Flow Analysis

## ğŸ“‹ Executive Summary

This document provides a comprehensive analysis of the data architecture, storage patterns, and API workflows for the STT Engine Web UI system. The system uses a **hybrid storage approach** combining filesystem-based audio storage with SQLite database for metadata tracking.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Web UI Layer                           â”‚
â”‚  (FastAPI + Jinja2 Templates + JavaScript)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Services Layer                             â”‚
â”‚  â€¢ FileService    â€¢ AnalysisService    â€¢ STTService         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Filesystem Storage  â”‚     SQLite Database                  â”‚
â”‚  (Audio Files)       â”‚     (Metadata & Results)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STT API Backend (Port 8003)                    â”‚
â”‚  â€¢ Real Mode: Whisper Model Processing                      â”‚
â”‚  â€¢ Dummy Mode: Simulated Responses (when model unavailable) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Storage Structure

### Directory Layout
```
web_ui/data/
â”œâ”€â”€ uploads/                    # User audio files
â”‚   â”œâ”€â”€ {emp_id}/              # Employee-specific directory
â”‚   â”‚   â”œâ”€â”€ {folder_path}/     # Date or custom folder name
â”‚   â”‚   â”‚   â”œâ”€â”€ file1.wav
â”‚   â”‚   â”‚   â”œâ”€â”€ file2.wav
â”‚   â”‚   â”‚   â””â”€â”€ file3.mp3
â”‚   â”‚   â”œâ”€â”€ 2026-02-24/        # Auto-generated date folders
â”‚   â”‚   â””â”€â”€ custom_folder/     # User-defined folders
â”‚   â””â”€â”€ 90002/
â”‚       â””â”€â”€ ...
â””â”€â”€ results/                   # Future: Analysis output files
```

### Path Resolution Rules

**Location**: `web_ui/config.py` (Lines 1-29)

```python
# Priority order:
# 1. DATA_DIR environment variable
# 2. /app/data (Docker mounted volume)
# 3. web_ui/data (Local development)

DATA_DIR = Path(os.getenv("DATA_DIR")) or Path("/app/data") or BASE_DIR / "data"
UPLOAD_DIR = DATA_DIR / "uploads"
```

**Key Paths**:
- **Docker Environment**: `/app/data/uploads/{emp_id}/{folder_path}/{filename}`
- **Local Development**: `web_ui/data/uploads/{emp_id}/{folder_path}/{filename}`

---

## ğŸ’¾ Database Schema

### 5 Core Tables

#### 1. **employees** - User Authentication & Info
```sql
CREATE TABLE employees (
    id INTEGER PRIMARY KEY,
    emp_id VARCHAR(10) UNIQUE NOT NULL,     -- Employee number
    name VARCHAR(100) NOT NULL,
    dept VARCHAR(100),
    created_at DATETIME,
    last_login DATETIME
);
```

#### 2. **file_uploads** - Audio File Metadata
```sql
CREATE TABLE file_uploads (
    id INTEGER PRIMARY KEY,
    emp_id VARCHAR(10) FOREIGN KEY,
    folder_path VARCHAR(500) NOT NULL,      -- "2026-02-24" or "custom_folder"
    filename VARCHAR(500) NOT NULL,         -- "sample.wav"
    file_size_mb FLOAT,
    uploaded_at DATETIME
);
```

**Purpose**: Tracks which files exist, their location, and metadata. Does NOT store audio content.

#### 3. **analysis_jobs** - Analysis Work Tracking
```sql
CREATE TABLE analysis_jobs (
    id INTEGER PRIMARY KEY,
    job_id VARCHAR(50) UNIQUE NOT NULL,     -- "job_abc123def456"
    emp_id VARCHAR(10) FOREIGN KEY,
    folder_path VARCHAR(500),                -- Which folder to analyze
    file_ids JSON,                           -- ["file1.wav", "file2.wav"]
    files_hash VARCHAR(64),                  -- SHA256 hash for change detection
    status VARCHAR(20),                      -- pending|processing|completed|failed
    options JSON,                            -- Analysis options
    created_at DATETIME,
    started_at DATETIME,
    completed_at DATETIME
);
```

**Purpose**: Tracks analysis workflow state. Each job represents one folder analysis request.

#### 4. **analysis_results** - STT & Detection Results
```sql
CREATE TABLE analysis_results (
    id INTEGER PRIMARY KEY,
    job_id VARCHAR(50) FOREIGN KEY,
    file_id VARCHAR(500),                    -- Filename
    stt_text TEXT,                           -- Transcribed text
    stt_metadata JSON,                       -- {duration, language, confidence, backend}
    classification_code VARCHAR(20),
    classification_category VARCHAR(100),
    classification_confidence FLOAT,
    improper_detection_results JSON,
    incomplete_detection_results JSON,
    created_at DATETIME
);
```

**Purpose**: Stores one result per audio file. Contains STT output and analysis results.

#### 5. **analysis_progress** - Real-time Progress Tracking
```sql
CREATE TABLE analysis_progress (
    id INTEGER PRIMARY KEY,
    job_id VARCHAR(50) FOREIGN KEY,
    current_file VARCHAR(500),
    progress_percent INTEGER,
    status VARCHAR(20),
    message TEXT,
    updated_at DATETIME
);
```

---

## ğŸ”„ Data Flow Workflows

### Workflow 1: File Upload

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚ (Browser)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /api/files/upload
       â”‚ FormData: {file, folder_name}
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FileService.upload_file()           â”‚
â”‚  web_ui/app/services/file_service.py â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ 1. Validate user (Employee exists?)
       â”œâ”€â†’ 2. Validate filename (sanitize)
       â”œâ”€â†’ 3. Create folder path
       â”‚      â€¢ If folder_name provided: use it
       â”‚      â€¢ Else: auto-generate "YYYY-MM-DD"
       â”œâ”€â†’ 4. Save file to disk
       â”‚      Path: data/uploads/{emp_id}/{folder_path}/{filename}
       â””â”€â†’ 5. Insert metadata to DB
              INSERT INTO file_uploads (emp_id, folder_path, filename, ...)
```

**Code Location**: `web_ui/app/services/file_service.py` (Lines 25-100)

**Key Functions**:
```python
# Filename validation
filename = file_utils.validate_filename(file.filename)

# Folder path creation
folder_path = file_utils.create_folder_path(emp_id, folder_name)
# Returns: "2026-02-24" or custom name

# File storage
full_file_path = user_dir / folder_path / filename
with open(full_file_path, 'wb') as f:
    f.write(file_content)

# DB record
file_record = FileUpload(
    emp_id=emp_id,
    folder_path=folder_path,
    filename=filename,
    file_size_mb=file_size_mb,
    uploaded_at=datetime.utcnow()
)
db.add(file_record)
db.commit()
```

---

### Workflow 2: Analysis Request & Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚ (Browser)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /api/analysis/start
       â”‚ Body: {folder_path, options}
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AnalysisService.start_analysis()          â”‚
â”‚  web_ui/app/services/analysis_service.py   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ 1. Query files in folder
       â”‚      SELECT * FROM file_uploads 
       â”‚      WHERE emp_id=? AND folder_path=?
       â”‚
       â”œâ”€â†’ 2. Calculate files hash (SHA256)
       â”‚      sorted_files = sorted([f.filename for f in files])
       â”‚      hash = sha256('|'.join(sorted_files))
       â”‚
       â”œâ”€â†’ 3. Check if already analyzed
       â”‚      If hash matches last job: return "unchanged"
       â”‚
       â”œâ”€â†’ 4. Create new analysis job
       â”‚      INSERT INTO analysis_jobs (job_id, emp_id, folder_path, 
       â”‚                                  file_ids, files_hash, status='pending')
       â”‚
       â””â”€â†’ 5. Launch background task
              background_tasks.add_task(AnalysisService.process_analysis_sync, ...)
```

**Code Location**: `web_ui/app/services/analysis_service.py` (Lines 56-148)

---

### Workflow 3: Background Analysis Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AnalysisService.process_analysis_sync()  â”‚
â”‚  (Runs in FastAPI BackgroundTasks thread) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ 1. Update job status to "processing"
       â”‚
       â”œâ”€â†’ FOR EACH file in file_list:
       â”‚   â”‚
       â”‚   â”œâ”€â†’ Track current file in memory
       â”‚   â”‚   _current_processing[job_id] = filename
       â”‚   â”‚
       â”‚   â”œâ”€â†’ Build file path
       â”‚   â”‚   file_path = data/uploads/{emp_id}/{folder_path}/{filename}
       â”‚   â”‚
       â”‚   â”œâ”€â†’ Call STT API
       â”‚   â”‚   stt_result = await stt_service.transcribe_local_file(file_path)
       â”‚   â”‚
       â”‚   â””â”€â†’ Save result to DB
       â”‚       INSERT INTO analysis_results (job_id, file_id, stt_text, ...)
       â”‚
       â””â”€â†’ 2. Update job status to "completed"
```

**Code Location**: `web_ui/app/services/analysis_service.py` (Lines 527-654)

**Dummy Data Implementation** (Current):
```python
# Line 557-564
test_confidence_values = [0.2, 0.45, 0.8]  # danger, warning, safe

for idx, filename in enumerate(files):
    confidence = test_confidence_values[idx % len(test_confidence_values)]
    
    result = AnalysisResult(
        job_id=job_id,
        file_id=filename,
        stt_text=stt_result.get('text', ''),
        stt_metadata={
            "confidence": confidence  # Cycling for testing
        }
    )
```

---

### Workflow 4: STT Service (Real vs Dummy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STTService.transcribe_local_file()  â”‚
â”‚  web_ui/app/services/stt_service.py  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ 1. Path conversion (Docker compatibility)
       â”‚   If "/app/data/..." â†’ "/app/web_ui/data/..."
       â”‚   Elif "/app/web_ui/data/..." â†’ keep as-is
       â”‚
       â”œâ”€â†’ 2. Send HTTP POST to STT API (port 8003)
       â”‚   URL: http://localhost:8003/transcribe
       â”‚   FormData: {file_path, language, options...}
       â”‚
       â”œâ”€â†’ 3. Handle response
       â”‚   â”œâ”€ SUCCESS (200) â†’ Return STT result
       â”‚   â””â”€ ERROR (503, timeout, connection error)
       â”‚      â””â”€â†’ Call _get_dummy_response()
       â”‚
       â””â”€â†’ 4. Dummy Response Generation
           â”œâ”€ Random sleep: 0-30 seconds
           â”œâ”€ Select random Korean dialogue (4 versions)
           â””â”€ Return mock result structure
```

**Code Location**: `web_ui/app/services/stt_service.py` (Lines 84-530)

**Real API Response Structure**:
```json
{
  "success": true,
  "text": "ìƒë‹´ì›: ì•ˆë…•í•˜ì„¸ìš”...",
  "duration_sec": 60.5,
  "backend": "faster-whisper",
  "language": "ko",
  "processing_steps": {
    "stt": true,
    "privacy_removal": false,
    "classification": false,
    "ai_agent": false
  }
}
```

**Dummy Response Structure**:
```json
{
  "success": true,
  "text": "ìƒë‹´ì›: ì•ˆë…•í•˜ì„¸ìš”...",  // Random dialogue from 4 versions
  "duration_sec": 60,
  "backend": "dummy",
  "language": "ko",
  "processing_steps": {
    "stt": true,
    "privacy_removal": false,
    "classification": false,
    "ai_agent": false
  },
  "_note": "âš ï¸ STT API ë¯¸ì‘ë‹µìœ¼ë¡œ Dummy ì‘ë‹µì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
}
```

**Dummy Dialogues** (Lines 437-474):
1. **Basic Sales Dialogue** (~410 chars) - Moderate, professional
2. **Aggressive Sales Dialogue** (~531 chars) - Pressure, guarantees, urgency
3. **Improper Sales Dialogue** (~531 chars) - Rushed, avoiding details
4. **Short Version** (~83 chars) - Fallback

---

## ğŸ” Result Fetching Mechanisms

### Real-Time Progress Polling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  JavaScript â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Polling: GET /api/analysis/progress/{job_id}
       â”‚ Interval: Every 2 seconds
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AnalysisService.get_progress()      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ 1. Query job from DB
       â”‚   SELECT * FROM analysis_jobs WHERE job_id=?
       â”‚
       â”œâ”€â†’ 2. Get completed results
       â”‚   SELECT * FROM analysis_results WHERE job_id=?
       â”‚
       â”œâ”€â†’ 3. Calculate progress
       â”‚   progress = (completed_files / total_files) * 100
       â”‚
       â”œâ”€â†’ 4. Get current processing file (from memory)
       â”‚   current_file = _current_processing[job_id]
       â”‚
       â””â”€â†’ 5. Build response for ALL files
           For each file in job.file_ids:
             If result exists:
               status = "completed"
               risk_level = calculate_risk(confidence)
             Elif file == current_file:
               status = "processing"
             Else:
               status = "pending"
```

**Code Location**: `web_ui/app/services/analysis_service.py` (Lines 152-268)

**Response Structure**:
```json
{
  "job_id": "job_abc123",
  "folder_path": "2026-02-24",
  "status": "processing",
  "progress": 66,
  "current_file": "file2.wav",
  "total_files": 3,
  "processed_files": 2,
  "results": [
    {
      "filename": "file1.wav",
      "stt_text": "ìƒë‹´ì›: ì•ˆë…•í•˜ì„¸ìš”...",
      "status": "completed",
      "confidence": 0.2,
      "risk_level": "danger"
    },
    {
      "filename": "file2.wav",
      "status": "processing",
      "confidence": 0,
      "risk_level": "safe"
    },
    {
      "filename": "file3.wav",
      "status": "pending",
      "confidence": 0,
      "risk_level": "safe"
    }
  ]
}
```

---

## ğŸ¯ Risk Level Detection Logic

**Location**: `web_ui/app/services/analysis_service.py` (Lines 215-225)

```python
# Get confidence from STT metadata
confidence = result.stt_metadata.get("confidence", 0.5)

# Determine risk level (lower confidence = higher risk)
if confidence < 0.3:
    risk_level = "danger"      # ë¶€ë‹¹ê¶Œìœ  ë°œê²¬
elif confidence < 0.6:
    risk_level = "warning"     # ì˜ì‹¬
else:
    risk_level = "safe"        # ì •ìƒ
```

**Current Test Mode** (Lines 557-564):
```python
# Cycling confidence values for deterministic testing
test_confidence_values = [0.2, 0.45, 0.8]  # danger, warning, safe

for idx, filename in enumerate(files):
    confidence = test_confidence_values[idx % len(test_confidence_values)]
    # File 1 â†’ 0.2 (danger)
    # File 2 â†’ 0.45 (warning)
    # File 3 â†’ 0.8 (safe)
    # File 4 â†’ 0.2 (danger) ...cycles
```

**Frontend Badge Styling** (`templates/analysis.html`):
```css
.status-danger  { background: #c62828; color: white; }
.status-warning { background: #f57c00; color: white; }
.status-safe    { background: #388e3c; color: white; }
```

---

## ğŸ“Š Data Consistency Mechanisms

### 1. File Hash Change Detection
```python
# Calculate hash of file list
current_hash = AnalysisService.calculate_files_hash(file_list)

# Compare with last completed job
if last_job and last_job.files_hash == current_hash:
    return "unchanged"  # Skip re-analysis
```

### 2. In-Memory Tracking
```python
# Track currently processing file
_current_processing: Dict[str, str] = {}  # {job_id: filename}

# Set when file processing starts
_current_processing[job_id] = filename

# Clear when job completes
del _current_processing[job_id]
```

### 3. Status Synchronization
```python
# Progress endpoint shows real-time status for each file
for filename in all_files:
    if result_exists(filename):
        status = "completed"
    elif filename == current_processing_file:
        status = "processing"
    else:
        status = "pending"
```

---

## ğŸ”§ Key Implementation Notes

### Path Conversion (Docker Compatibility)

**Problem**: Different paths between Web UI and STT API
- Web UI stores: `/app/web_ui/data/uploads/...`
- STT API expects: `/app/web_ui/data/...` (mounted volume)

**Solution**: `stt_service.py` (Lines 96-107)
```python
if file_path.startswith("/app/data/"):
    api_file_path = file_path.replace("/app/data/", "/app/web_ui/data/")
elif file_path.startswith("/app/web_ui/data/"):
    api_file_path = file_path  # No conversion needed
else:
    api_file_path = file_path  # Local development
```

### Audio File Serving

**Location**: `web_ui/app/routes/files.py` (Lines 142-193)

```python
@router.get("/audio/{emp_id}/{folder_path}/{filename}")
async def serve_audio_file(emp_id, folder_path, filename, request):
    # Security: Check session matches emp_id
    if session_emp_id != emp_id:
        raise HTTPException(403, "Access denied")
    
    # Build file path
    file_path = f"data/uploads/{emp_id}/{folder_path}/{filename}"
    
    # Serve with proper MIME type
    return FileResponse(
        path=file_path,
        media_type='audio/wav',  # or audio/mpeg for mp3
        filename=filename
    )
```

### CSV Export (Frontend)

**Location**: `templates/analysis.html` (Lines 840-869)

```javascript
function exportResults() {
    // UTF-8 BOM for Korean characters
    let csv = '\uFEFF';
    csv += 'íŒŒì¼ëª…,í…ìŠ¤íŠ¸,ë¶„ì„ìƒíƒœ\n';
    
    results.forEach(result => {
        csv += `"${result.filename}","${result.stt_text || ''}","${getRiskLevelText(result.risk_level)}"\n`;
    });
    
    // Download as CSV
    const blob = new Blob([csv], { type: 'text/csv;charset=utf-8;' });
    const link = document.createElement('a');
    link.href = URL.createObjectURL(blob);
    link.download = `analysis_results_${Date.now()}.csv`;
    link.click();
}
```

---

## ğŸš€ Improvement Opportunities

### 1. Dummy Mode Should Mirror Real API More Closely

**Current Issue**: Confidence values are deterministically cycled, not realistic

**Recommendation**:
```python
# Add keyword-based confidence generation
def calculate_realistic_confidence(text):
    """Calculate confidence based on dialogue content"""
    risk_keywords = ["ë³´ì¥", "í™•ì‹¤", "ì›ê¸ˆ ë³´ì¥", "ì ˆëŒ€", "ë¬´ì¡°ê±´"]
    risk_count = sum(1 for keyword in risk_keywords if keyword in text)
    
    # Base confidence: 0.6-0.9 (safe range)
    # Reduce by 0.1-0.2 per risk keyword
    confidence = 0.85 - (risk_count * 0.15)
    return max(0.1, min(0.95, confidence))
```

### 2. Better File Status Tracking

**Current**: Uses in-memory dictionary (lost on server restart)

**Recommendation**: Add `status` column to `analysis_results` table
```sql
ALTER TABLE analysis_results ADD COLUMN status VARCHAR(20);
-- Values: pending | processing | completed | failed
```

### 3. Audio File Cleanup

**Missing**: No mechanism to delete orphaned files

**Recommendation**: Add cleanup task
```python
def cleanup_orphaned_files(emp_id):
    """Remove files from disk that are not in DB"""
    db_files = {f.filename for f in db.query(FileUpload).filter_by(emp_id=emp_id)}
    disk_files = set(list_files(user_dir))
    
    orphaned = disk_files - db_files
    for filename in orphaned:
        file_path.unlink()  # Delete
```

### 4. Progress Persistence

**Current**: Progress only calculated from DB queries

**Recommendation**: Use `analysis_progress` table more effectively
```python
# Update progress in DB during processing
progress_record = AnalysisProgress(
    job_id=job_id,
    current_file=filename,
    progress_percent=int((idx / total) * 100),
    status="processing",
    updated_at=datetime.utcnow()
)
db.merge(progress_record)
db.commit()
```

---

## ğŸ“ Summary

### Data Storage Strategy
- **Audio Files**: Filesystem (`data/uploads/{emp_id}/{folder_path}/`)
- **Metadata**: SQLite database (5 tables)
- **Results**: Database with JSON fields for complex data

### Workflow States
1. **Upload**: File â†’ Disk + DB metadata
2. **Analysis Request**: Create job â†’ Background task
3. **Processing**: Sequential file processing â†’ Save results
4. **Progress**: Real-time polling â†’ Frontend updates
5. **Results**: Query DB â†’ Display in UI

### Dummy vs Real API
- **Real**: HTTP POST to port 8003 â†’ Whisper model â†’ JSON response
- **Dummy**: API error â†’ Generate mock dialogue â†’ Same JSON structure
- **Key**: Both return identical data structure for seamless testing

### Critical Files
- `config.py` - Path configuration
- `app/services/stt_service.py` - STT API communication + dummy
- `app/services/analysis_service.py` - Job orchestration + results
- `app/services/file_service.py` - File management
- `app/models/database.py` - Schema definitions
- `templates/analysis.html` - Frontend result display

---

## ğŸ“ Ready for Changes

You now have complete understanding of:
1. âœ… Where files are stored (filesystem structure)
2. âœ… How metadata is tracked (database schema)
3. âœ… How analysis flows (workflows)
4. âœ… How results are fetched (polling + queries)
5. âœ… How dummy mode works (and should be improved)
6. âœ… Path handling between services (Docker compatibility)

**You are now prepared to make informed changes to the system.**
