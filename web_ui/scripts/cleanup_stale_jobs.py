#!/usr/bin/env python3
"""
ì¤‘ë‹¨ëœ ë¶„ì„ ì‘ì—… ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸

ìƒíƒœ:
- pending/processingì¸ë° ì˜¤ë˜ëœ ì‘ì—… â†’ completedë¡œ ë³€ê²½í•˜ê³  ìƒíƒœ ë©”ì‹œì§€ ê¸°ë¡
- DB ìš©ëŸ‰ ì •ë¦¬: ì´ì „ ì‘ì—…ë“¤ ì‚­ì œ ì˜µì…˜
"""

import sys
import os
from datetime import datetime, timedelta
from pathlib import Path

# ì›¹UI ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.db import SessionLocal
from app.models.database import AnalysisJob, AnalysisResult


def cleanup_stale_jobs(hours=24, dry_run=True):
    """
    ì¤‘ë‹¨ëœ ì‘ì—… ì •ë¦¬
    
    Args:
        hours: ì´ ì‹œê°„ ì´ìƒ ê²½ê³¼í•œ pending/processing ì‘ì—… ì •ë¦¬
        dry_run: Trueë©´ ì¡°íšŒë§Œ, Falseë©´ ì‹¤ì œ ì—…ë°ì´íŠ¸
    """
    db = SessionLocal()
    
    try:
        cutoff = datetime.now() - timedelta(hours=hours)
        
        # ì¤‘ë‹¨ëœ ì‘ì—… ì°¾ê¸°
        stale_jobs = db.query(AnalysisJob).filter(
            AnalysisJob.status.in_(['pending', 'processing']),
            AnalysisJob.created_at < cutoff
        ).order_by(AnalysisJob.created_at.asc()).all()
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ì¤‘ë‹¨ëœ ì‘ì—… ì •ë¦¬ ë¦¬í¬íŠ¸ ({hours}ì‹œê°„ ì´ìƒ ê²½ê³¼)")
        print(f"{'='*70}\n")
        
        if not stale_jobs:
            print("âœ… ì¤‘ë‹¨ëœ ì‘ì—… ì—†ìŒ\n")
            return 0
        
        print(f"âŒ ë°œê²¬ëœ ì¤‘ë‹¨ëœ ì‘ì—…: {len(stale_jobs)}ê°œ\n")
        
        for i, job in enumerate(stale_jobs, 1):
            elapsed = datetime.now() - job.created_at
            days = elapsed.days
            hours_part = elapsed.seconds // 3600
            
            # í•´ë‹¹ ì‘ì—…ì˜ ë¶„ì„ ê²°ê³¼ í†µê³„
            results = db.query(AnalysisResult).filter(
                AnalysisResult.job_id == job.id
            ).all()
            completed_count = sum(1 for r in results if r.status == 'completed')
            
            print(f"{i}. Job ID: {job.job_id}")
            print(f"   ìƒíƒœ: {job.status} â†’ ì •ë¦¬ í•„ìš”")
            print(f"   í´ë”: {job.folder_path}")
            print(f"   ìƒì„±ì¼: {job.created_at.strftime('%Y-%m-%d %H:%M:%S')} ({days}ì¼ {hours_part}ì‹œê°„ ì „)")
            print(f"   íŒŒì¼ ë¶„ì„: {len(results)}ê°œ (ì™„ë£Œ: {completed_count}ê°œ)")
            print()
        
        if not dry_run:
            print("ğŸ”§ ì‹¤ì œ ì •ë¦¬ ì‘ì—… ìˆ˜í–‰ ì¤‘...\n")
            cleaned_count = 0
            for job in stale_jobs:
                job.status = 'completed'
                job.updated_at = datetime.now()
                cleaned_count += 1
                print(f"  âœ“ {job.job_id}: ì •ë¦¬ ì™„ë£Œ")
            
            db.commit()
            print(f"\nâœ… ì´ {cleaned_count}ê°œ ì‘ì—… ì •ë¦¬ ì™„ë£Œ\n")
        else:
            print(f"â„¹ï¸  Dry-run ëª¨ë“œ: ìœ„ ì‘ì—…ë“¤ì´ ì •ë¦¬ë©ë‹ˆë‹¤ (--apply í”Œë˜ê·¸ë¡œ ì‹¤í–‰)\n")
        
        return len(stale_jobs)
    
    finally:
        db.close()


def show_job_stats():
    """ì‘ì—… ìƒíƒœ í†µê³„"""
    db = SessionLocal()
    
    try:
        print(f"\n{'='*70}")
        print("ğŸ“ˆ ì „ì²´ ì‘ì—… ìƒíƒœ í†µê³„")
        print(f"{'='*70}\n")
        
        # ìƒíƒœë³„ ì§‘ê³„
        statuses = ['pending', 'processing', 'completed']
        total_jobs = 0
        
        for status in statuses:
            count = db.query(AnalysisJob).filter(
                AnalysisJob.status == status
            ).count()
            total_jobs += count
            print(f"  {status:12} : {count:4}ê°œ")
        
        print(f"  {'â”€'*20}")
        print(f"  {'total':12} : {total_jobs:4}ê°œ\n")
        
        # ì˜¤ë˜ëœ ì‘ì—… ì¡°íšŒ
        one_week_ago = datetime.now() - timedelta(days=7)
        old_jobs = db.query(AnalysisJob).filter(
            AnalysisJob.created_at < one_week_ago
        ).count()
        
        print(f"ğŸ“… 7ì¼ ì´ìƒ ëœ ì‘ì—…: {old_jobs}ê°œ")
        print(f"ğŸ’¾ ì •ë¦¬ ëŒ€ìƒ (24ì‹œê°„ ì´ìƒ ì¤‘ë‹¨): ", end="")
        
        cutoff = datetime.now() - timedelta(hours=24)
        stale = db.query(AnalysisJob).filter(
            AnalysisJob.status.in_(['pending', 'processing']),
            AnalysisJob.created_at < cutoff
        ).count()
        print(f"{stale}ê°œ\n")
        
    finally:
        db.close()


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ì¤‘ë‹¨ëœ ë¶„ì„ ì‘ì—… ì •ë¦¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # 1. ìƒíƒœë§Œ í™•ì¸ (dry-run)
  python cleanup_stale_jobs.py --check
  
  # 2. í†µê³„ ë³´ê¸°
  python cleanup_stale_jobs.py --stats
  
  # 3. 24ì‹œê°„ ì´ìƒ ì¤‘ë‹¨ëœ ì‘ì—… ì •ë¦¬ (ì‹¤ì œ ì ìš©)
  python cleanup_stale_jobs.py --apply
  
  # 4. 12ì‹œê°„ ì´ìƒ ì¤‘ë‹¨ëœ ì‘ì—… ì •ë¦¬
  python cleanup_stale_jobs.py --hours 12 --apply
        """
    )
    
    parser.add_argument(
        '--check',
        action='store_true',
        help='ì¤‘ë‹¨ëœ ì‘ì—… í™•ì¸ (dry-run, ê¸°ë³¸ê°’)'
    )
    parser.add_argument(
        '--apply',
        action='store_true',
        help='ì‹¤ì œ ì •ë¦¬ ìˆ˜í–‰'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='ì‘ì—… ìƒíƒœ í†µê³„ë§Œ í‘œì‹œ'
    )
    parser.add_argument(
        '--hours',
        type=int,
        default=24,
        help='ê¸°ì¤€ ì‹œê°„ (ê¸°ë³¸: 24ì‹œê°„)'
    )
    
    args = parser.parse_args()
    
    # í†µê³„ ë¨¼ì € í‘œì‹œ
    show_job_stats()
    
    if args.stats:
        sys.exit(0)
    
    # ì •ë¦¬
    cleanup_stale_jobs(hours=args.hours, dry_run=not args.apply)
