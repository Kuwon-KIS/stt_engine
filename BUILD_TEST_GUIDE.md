# STT Engine - ë¹Œë“œ ë° ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë²½ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ìš´ì˜ ì„œë²„ ë°°í¬ ì „ ë¹Œë“œ ì„œë²„ì—ì„œ **Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ + ëª¨ë¸ ë‹¤ìš´ë¡œë“œ + í†µí•© í…ŒìŠ¤íŠ¸**ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì™„ë²½í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

**ì´ ê°€ì´ë“œë¡œ ì™„ì„±ë˜ëŠ” ê²ƒ:**
- âœ… RHEL 8.9 í˜¸í™˜ Docker ì´ë¯¸ì§€ (`stt-engine:cuda129-rhel89-v1.2`)
- âœ… OpenAI Whisper Large-v3-turbo ëª¨ë¸ (ì›ë³¸ + CTranslate2 ë³€í™˜)
- âœ… ì—”ì§„ê³¼ ëª¨ë¸ì˜ í†µí•© í…ŒìŠ¤íŠ¸
- âœ… ìš´ì˜ ì„œë²„ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ

---

## ğŸ”„ Phase 1: ë¹Œë“œ ì„œë²„ í™˜ê²½ ì¤€ë¹„ (5~10ë¶„)

### 1-1. ë¹Œë“œ ì„œë²„ ì ‘ì† ë° ì‚¬ì „ ì²´í¬

```bash
# ë¹Œë“œ ì„œë²„ SSH ì ‘ì†
ssh -i your-key.pem ec2-user@build-server-ip

# Docker ìƒíƒœ í™•ì¸
docker --version
docker ps

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 100GB í•„ìš”: Docker ì´ë¯¸ì§€ 7GB + ëª¨ë¸ 2.5GB)
df -h /

# ì¸í„°ë„· ì—°ê²° í™•ì¸
ping -c 3 8.8.8.8
```

### 1-2. ì†ŒìŠ¤ ì½”ë“œ ì—…ë°ì´íŠ¸

```bash
cd /path/to/stt_engine

# ìµœì‹  ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
git fetch origin main
git reset --hard origin/main

# ë³€ê²½ ì‚¬í•­ í™•ì¸
git log --oneline -5
# ìµœì‹  ì»¤ë°‹ì— "fix: Update CTranslate2 model validation..." ìˆëŠ”ì§€ í™•ì¸
```

### 1-3. ê¸°ì¡´ ì´ë¯¸ì§€ ë° ëª¨ë¸ ì •ë¦¬

```bash
# ê¸°ì¡´ ì´ë¯¸ì§€ ì œê±° (ì„ íƒì‚¬í•­)
docker rmi stt-engine:cuda129-rhel89-v1.2 2>/dev/null || true

# ë¯¸ì‚¬ìš© ì´ë¯¸ì§€ ì •ë¦¬
docker image prune -a --force --filter "until=24h"

# ë””ìŠ¤í¬ ì •ë¦¬
docker system prune -a --force

# ê¸°ì¡´ ëª¨ë¸ ì œê±° (ì˜µì…˜)
rm -rf /path/to/stt_engine/models
```

---

## ğŸ”¨ Phase 2: Docker ì´ë¯¸ì§€ ë¹Œë“œ (20~40ë¶„)

### 2-1. ë¹Œë“œ ì‹¤í–‰

```bash
cd /path/to/stt_engine

# ë°©ë²• 1: RHEL 8.9 ì „ìš© ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)
bash scripts/build-stt-engine-rhel89.sh

# ë˜ëŠ” ë°©ë²• 2: ì§ì ‘ docker build ì‹¤í–‰
docker build \
  --platform linux/amd64 \
  -t stt-engine:cuda129-rhel89-v1.2 \
  -f docker/Dockerfile.engine.rhel89 \
  . 2>&1 | tee /tmp/build.log
```

### 2-2. ë¹Œë“œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```bash
# ë³„ë„ í„°ë¯¸ë„ì—ì„œ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f /tmp/build.log

# Docker ë¹Œë“œ ìƒíƒœ í™•ì¸
docker ps -a
```

### 2-3. ë¹Œë“œ ì™„ë£Œ í™•ì¸

```bash
# ì´ë¯¸ì§€ í™•ì¸
docker images | grep stt-engine

# ì˜ˆìƒ ì¶œë ¥:
# stt-engine   cuda129-rhel89-v1.2   HASH   7.3GB   1 minute ago

# ì´ë¯¸ì§€ ìƒì„¸ ì •ë³´ í™•ì¸
docker inspect stt-engine:cuda129-rhel89-v1.2 | jq '.Config.Env[] | select(startswith("LD_"))'

# ì˜ˆìƒ ì¶œë ¥ì— LD_LIBRARY_PATHê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
```

### 2-4. ë¹Œë“œ ì˜¤ë¥˜ ì²˜ë¦¬

```bash
# ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ë¡œê·¸ í™•ì¸
grep -i "error\|failed\|not found" /tmp/build.log | tail -20
```

---

## ğŸ“¦ Phase 3: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜ (25~45ë¶„)

**ì¤‘ìš”**: ë¹Œë“œ ì„œë²„ì˜ **ë¡œì»¬ í™˜ê²½**ì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

### 3-1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° CTranslate2 ë³€í™˜

```bash
cd /path/to/stt_engine

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš”ì‹œ)
pip install --upgrade huggingface-hub transformers ctranslate2

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜ ì‹¤í–‰ (ìë™ìœ¼ë¡œ ëª¨ë‘ ì²˜ë¦¬)
python3 download_model_hf.py 2>&1 | tee /tmp/model_download.log

# ì˜ˆìƒ ì¶œë ¥:
# ========================================================
# ğŸš€ STT Engine ëª¨ë¸ ì¤€ë¹„
# ========================================================
# 
# ğŸ“Œ Step 1: ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì •ë¦¬
# âœ… ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ
#
# ğŸ“Œ Step 2: Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# â³ openai/whisper-large-v3-turbo ë‹¤ìš´ë¡œë“œ ì¤‘...
# âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ì•½ 3-5ë¶„ ì†Œìš”)
#
# ğŸ“Œ Step 3: ëª¨ë¸ íŒŒì¼ ê²€ì¦
# âœ… config.json ê²€ì¦ ì™„ë£Œ
# âœ… pytorch_model.bin ê²€ì¦ ì™„ë£Œ
# âœ… tokenizer.json ê²€ì¦ ì™„ë£Œ
#
# ğŸ“Œ Step 4: CTranslate2 í¬ë§· ë³€í™˜
# â³ CTranslate2 ë³€í™˜ ì¤‘... (ì•½ 5-10ë¶„ ì†Œìš”)
# âœ… CTranslate2 ë³€í™˜ ì™„ë£Œ
#
# ğŸ“Œ Step 5: ëª¨ë¸ êµ¬ì¡° ê²€ì¦
# âœ… ctranslate2_model êµ¬ì¡° í™•ì¸
#    - config.json âœ“
#    - model.bin âœ“
#    - vocabulary.json âœ“
#
# âœ… ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!
```

### 3-2. ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸

```bash
# ëª¨ë¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
du -sh models/
du -sh models/openai_whisper-large-v3-turbo/
du -sh models/ctranslate2_model/

# ì˜ˆìƒ:
# 2.5G  models/
# 1.6G  models/openai_whisper-large-v3-turbo/
# 0.9G  models/ctranslate2_model/

# íŒŒì¼ í™•ì¸
find models/ -type f -name "*.json" -o -name "*.bin"
```

### 3-3. ëª¨ë¸ íŒŒì¼ ê²€ì¦ (Python)

```bash
python3 << 'PYTHON_TEST'
from pathlib import Path

models_base = Path("models")
print("=" * 70)
print("ğŸ” ëª¨ë¸ êµ¬ì¡° ì„¸ë¶€ ê²€ì¦")
print("=" * 70)

# CTranslate2 ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ CTranslate2 ëª¨ë¸")
ct2_model = models_base / "ctranslate2_model"
required_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "vocabulary.json": "í† í¬ë‚˜ì´ì € ì–´íœ˜"
}

for fname, desc in required_files.items():
    fpath = ct2_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:20} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:20} NOT FOUND")

# OpenAI Whisper ëª¨ë¸ í™•ì¸
print("\nğŸ“‚ OpenAI Whisper ëª¨ë¸")
whisper_model = models_base / "openai_whisper-large-v3-turbo"
required_whisper_files = {
    "config.json": "ì„¤ì • íŒŒì¼",
    "pytorch_model.bin": "ëª¨ë¸ ê°€ì¤‘ì¹˜",
    "tokenizer.json": "í† í¬ë‚˜ì´ì €"
}

for fname, desc in required_whisper_files.items():
    fpath = whisper_model / fname
    if fpath.exists():
        size = fpath.stat().st_size / (1024 * 1024)
        print(f"   âœ… {fname:25} ({size:6.1f} MB) - {desc}")
    else:
        print(f"   âŒ {fname:25} NOT FOUND")

print("\n" + "=" * 70)
PYTHON_TEST
```

---

## ğŸ§ª Phase 4: ê¸°ë³¸ í™˜ê²½ ê²€ì¦ (10~15ë¶„)

### 4-1. ì»¨í…Œì´ë„ˆ ì‹œì‘ (ëª¨ë¸ ë§ˆìš´íŠ¸)

```bash
cd /path/to/stt_engine

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ëª¨ë¸ ë””ë ‰í† ë¦¬ ë§ˆìš´íŠ¸)
docker run -it \
  --name stt-test-engine \
  -v $(pwd)/models:/app/models \
  -e CUDA_VISIBLE_DEVICES=0 \
  stt-engine:cuda129-rhel89-v1.2 \
  /bin/bash
```

### 4-2. ì»¨í…Œì´ë„ˆ ë‚´ ë§ˆìš´íŠ¸ëœ ëª¨ë¸ í™•ì¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# ë§ˆìš´íŠ¸ í™•ì¸
ls -lh /app/models/
du -sh /app/models/*
```

### 4-3. CUDA ë° PyTorch ê²€ì¦

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import torch
import torchaudio
import os

print("=" * 70)
print("ğŸ” CUDA & PyTorch ê²€ì¦")
print("=" * 70)

print(f"\nâœ… PyTorch: {torch.__version__}")
print(f"âœ… torchaudio: {torchaudio.__version__}")
print(f"âœ… CUDA Available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"âœ… CUDA Device: {torch.cuda.get_device_name(0)}")
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.matmul(x, y)
    print(f"âœ… CUDA Matrix Multiplication: Success")

print(f"âœ… LD_LIBRARY_PATH: {bool(os.environ.get('LD_LIBRARY_PATH'))}")
print("=" * 70)

PYTHON_TEST
```

### 4-4. ê¶Œí•œ ë° ìºì‹œ ê²€ì¦

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# í˜„ì¬ ì‚¬ìš©ì
whoami

# ìºì‹œ ê¶Œí•œ í…ŒìŠ¤íŠ¸
touch /opt/app-root/src/.cache/test.txt && rm /opt/app-root/src/.cache/test.txt && echo "âœ… ìºì‹œ ë””ë ‰í† ë¦¬ ì“°ê¸° ê°€ëŠ¥"
```

---

## ğŸ“¦ Phase 5: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (20~30ë¶„)

### 5-1. CTranslate2 ëª¨ë¸ ì§„ë‹¨

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

from stt_engine import diagnose_faster_whisper_model

print("=" * 70)
print("ğŸ” CTranslate2 ëª¨ë¸ ì§„ë‹¨")
print("=" * 70)

result = diagnose_faster_whisper_model("/app/models/ctranslate2_model")

if not result.get('errors'):
    print("\nâœ… ëª¨ë¸ êµ¬ì¡° ì •ìƒ!")
else:
    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {result.get('errors')}")

PYTHON_TEST
```

### 5-2. Faster-Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ Faster-Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from faster_whisper import WhisperModel
    
    print("\nâ³ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = WhisperModel(
        "/app/models/ctranslate2_model",
        device="auto",
        compute_type="float32",
        download_root="/opt/app-root/src/.cache",
        local_files_only=True
    )
    
    print("âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST
```

### 5-3. OpenAI Whisper ë¡œë“œ í…ŒìŠ¤íŠ¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import sys
sys.path.insert(0, '/app')

print("=" * 70)
print("ğŸ¯ OpenAI Whisper ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
    
    print("\nâ³ Processor ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("â³ Model ë¡œë“œ ì¤‘...")
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        "/app/models/openai_whisper-large-v3-turbo",
        local_files_only=True,
        cache_dir="/opt/app-root/src/.cache"
    )
    
    print("âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()

PYTHON_TEST
```

### 5-4. ì˜¤ë””ì˜¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
python3 << 'PYTHON_TEST'
import torch
import torchaudio

print("=" * 70)
print("ğŸµ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
print("=" * 70)

try:
    # í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ìƒì„±
    sample_rate = 16000
    duration_sec = 1
    
    print(f"\nğŸ”Š í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...")
    
    waveform = torch.randn(1, sample_rate * duration_sec)
    if torch.cuda.is_available():
        waveform = waveform.cuda()
    
    print(f"   ìƒ˜í”Œ ë ˆì´íŠ¸: {sample_rate} Hz")
    print(f"   ì§€ì† ì‹œê°„: {duration_sec} ì´ˆ")
    print(f"   ì›¨ì´ë¸Œí¼: {waveform.shape}, ë””ë°”ì´ìŠ¤: {waveform.device}")
    print("âœ… ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {type(e).__name__}: {e}")

PYTHON_TEST
```

---

## âœ… Phase 6: ìµœì¢… ê²€ì¦ (5ë¶„)

ì»¨í…Œì´ë„ˆì—ì„œ `exit` ì‹¤í–‰:

```bash
exit
```

ê·¸ í›„ ìµœì¢… ì •ë¦¬:

```bash
# ì»¨í…Œì´ë„ˆ ì œê±°
docker rm stt-test-engine

# ë¹Œë“œ ê²°ê³¼ ì €ì¥
docker images | grep stt-engine
ls -lh models/

# ë¡œê·¸ ì €ì¥
cp /tmp/build.log build-success-$(date +%Y%m%d).log
cp /tmp/model_download.log model-success-$(date +%Y%m%d).log
```

---

## ğŸ“Š ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë“  í•­ëª©ì´ âœ… ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤:

```
Phase 1: í™˜ê²½ ì¤€ë¹„
  âœ… git fetch & reset ì™„ë£Œ
  âœ… ê¸°ì¡´ ì´ë¯¸ì§€/ëª¨ë¸ ì •ë¦¬

Phase 2: Docker ì´ë¯¸ì§€ ë¹Œë“œ
  âœ… ì´ë¯¸ì§€ ë¹Œë“œ ì„±ê³µ (7.3GB)
  âœ… LD_LIBRARY_PATH ì„¤ì •ë¨

Phase 3: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  âœ… Whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (1.6GB)
  âœ… CTranslate2 ë³€í™˜ (0.9GB)
  âœ… ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì™„ë£Œ

Phase 4: ê¸°ë³¸ í™˜ê²½ ê²€ì¦
  âœ… ëª¨ë¸ ë§ˆìš´íŠ¸ ì„±ê³µ
  âœ… PyTorch ì„¤ì¹˜ í™•ì¸
  âœ… CUDA ì‚¬ìš© ê°€ëŠ¥
  âœ… ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ OK

Phase 5: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
  âœ… CTranslate2 êµ¬ì¡° ì •ìƒ
  âœ… Faster-Whisper ë¡œë“œ ì„±ê³µ
  âœ… OpenAI Whisper ë¡œë“œ ì„±ê³µ
  âœ… ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„±ê³µ

Phase 6: ìµœì¢… ì •ë¦¬
  âœ… ë¹Œë“œ ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ
  âœ… ë¡œê·¸ ì €ì¥ ì™„ë£Œ
```

---

## â±ï¸ ì „ì²´ ì†Œìš” ì‹œê°„

| Phase | ì‹œê°„ | í•©ê³„ |
|-------|------|------|
| 1. í™˜ê²½ ì¤€ë¹„ | 5~10ë¶„ | 5~10ë¶„ |
| 2. Docker ë¹Œë“œ | 20~40ë¶„ | 25~50ë¶„ |
| 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ | 25~45ë¶„ | 50~95ë¶„ |
| 4. ê¸°ë³¸ ê²€ì¦ | 10~15ë¶„ | 60~110ë¶„ |
| 5. ëª¨ë¸ í…ŒìŠ¤íŠ¸ | 20~30ë¶„ | 80~140ë¶„ |
| 6. ìµœì¢… ê²€ì¦ | 5ë¶„ | 85~145ë¶„ |
| **ì´í•©** | - | **90~150ë¶„ (1.5~2.5ì‹œê°„)** |

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

ì´ ëª¨ë“  í•­ëª©ì´ í†µê³¼í•˜ë©´ ìš´ì˜ ì„œë²„ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ:

âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì„±ê³µ (7.3GB)  
âœ… ëª¨ë¸ ì™„ë²½ ë‹¤ìš´ë¡œë“œ (2.5GB)  
âœ… CTranslate2 ë³€í™˜ ì„±ê³µ (model.bin ìƒì„±)  
âœ… ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹œì‘  
âœ… CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë‘ ë¡œë“œ  
âœ… PyTorch/torchaudio ì •ìƒ  
âœ… Faster-Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ  
âœ… OpenAI Whisper ëª¨ë¸ ë¡œë“œ ì„±ê³µ  
âœ… ëª¨ë“  íŒŒì¼ ê¶Œí•œ ì •ìƒ  
âœ… CUDA ê³„ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ  

**ì´ ëª¨ë“  í•­ëª© í†µê³¼ â†’ ìš´ì˜ ë°°í¬ ì•ˆì „!** ğŸš€
